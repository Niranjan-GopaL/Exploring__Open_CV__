{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard dependencies\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow dependencies - Functional API\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load and Preprocess Images\n",
    "\n",
    "## 3.1 Get Image Directories\n",
    "\n",
    "### tf.data.Dataset\n",
    "\n",
    "The `tf.data.Dataset` API supports writing descriptive and efficient input\r",
    " pipelines. `Dataset` usage follows a common pattern:\r\n",
    "\r\n",
    "  1. Create a source dataset from your input data.\r\n",
    "  2. Apply dataset transformations to preprocess the data.\r\n",
    "  3. Iterate over the dataset and process the elements.\n",
    "\n",
    "The simplest way to create a dataset is to create it from a python `list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m        \n",
       "\u001b[1;33m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data.Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[0mDatasetV2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcollections_abc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtracking_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrackable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcomposite_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompositeTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdata_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetaclass\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mABCMeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;34m\"\"\"Represents a potentially large set of elements.\n",
       "\n",
       "  The `tf.data.Dataset` API supports writing descriptive and efficient input\n",
       "  pipelines. `Dataset` usage follows a common pattern:\n",
       "\n",
       "  1. Create a source dataset from your input data.\n",
       "  2. Apply dataset transformations to preprocess the data.\n",
       "  3. Iterate over the dataset and process the elements.\n",
       "\n",
       "  Iteration happens in a streaming fashion, so the full dataset does not need to\n",
       "  fit into memory.\n",
       "\n",
       "  Source Datasets:\n",
       "\n",
       "  The simplest way to create a dataset is to create it from a python `list`:\n",
       "\n",
       "  >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
       "  >>> for element in dataset:\n",
       "  ...   print(element)\n",
       "  tf.Tensor(1, shape=(), dtype=int32)\n",
       "  tf.Tensor(2, shape=(), dtype=int32)\n",
       "  tf.Tensor(3, shape=(), dtype=int32)\n",
       "\n",
       "  To process lines from files, use `tf.data.TextLineDataset`:\n",
       "\n",
       "  >>> dataset = tf.data.TextLineDataset([\"file1.txt\", \"file2.txt\"])\n",
       "\n",
       "  To process records written in the `TFRecord` format, use `TFRecordDataset`:\n",
       "\n",
       "  >>> dataset = tf.data.TFRecordDataset([\"file1.tfrecords\", \"file2.tfrecords\"])\n",
       "\n",
       "  To create a dataset of all files matching a pattern, use\n",
       "  `tf.data.Dataset.list_files`:\n",
       "\n",
       "  ```python\n",
       "  dataset = tf.data.Dataset.list_files(\"/path/*.txt\")\n",
       "  ```\n",
       "\n",
       "  See `tf.data.FixedLengthRecordDataset` and `tf.data.Dataset.from_generator`\n",
       "  for more ways to create datasets.\n",
       "\n",
       "  Transformations:\n",
       "\n",
       "  Once you have a dataset, you can apply transformations to prepare the data for\n",
       "  your model:\n",
       "\n",
       "  >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
       "  >>> dataset = dataset.map(lambda x: x*2)\n",
       "  >>> list(dataset.as_numpy_iterator())\n",
       "  [2, 4, 6]\n",
       "\n",
       "  Common Terms:\n",
       "\n",
       "  **Element**: A single output from calling `next()` on a dataset iterator.\n",
       "    Elements may be nested structures containing multiple components. For\n",
       "    example, the element `(1, (3, \"apple\"))` has one tuple nested in another\n",
       "    tuple. The components are `1`, `3`, and `\"apple\"`.\n",
       "\n",
       "  **Component**: The leaf in the nested structure of an element.\n",
       "\n",
       "  Supported types:\n",
       "\n",
       "  Elements can be nested structures of tuples, named tuples, and dictionaries.\n",
       "  Note that Python lists are *not* treated as nested structures of components.\n",
       "  Instead, lists are converted to tensors and treated as components. For\n",
       "  example, the element `(1, [1, 2, 3])` has only two components; the tensor `1`\n",
       "  and the tensor `[1, 2, 3]`. Element components can be of any type\n",
       "  representable by `tf.TypeSpec`, including `tf.Tensor`, `tf.data.Dataset`,\n",
       "  `tf.sparse.SparseTensor`, `tf.RaggedTensor`, and `tf.TensorArray`.\n",
       "\n",
       "  ```python\n",
       "  a = 1 # Integer element\n",
       "  b = 2.0 # Float element\n",
       "  c = (1, 2) # Tuple element with 2 components\n",
       "  d = {\"a\": (2, 2), \"b\": 3} # Dict element with 3 components\n",
       "  Point = collections.namedtuple(\"Point\", [\"x\", \"y\"])\n",
       "  e = Point(1, 2) # Named tuple\n",
       "  f = tf.data.Dataset.range(10) # Dataset element\n",
       "  ```\n",
       "\n",
       "  For more information,\n",
       "  read [this guide](https://www.tensorflow.org/guide/data).\n",
       "  \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a DatasetV2 object.\n",
       "\n",
       "    This is a difference between DatasetV1 and DatasetV2. DatasetV1 does not\n",
       "    take anything in its constructor whereas in the DatasetV2, we expect\n",
       "    subclasses to create a variant_tensor and pass it in to the super() call.\n",
       "\n",
       "    Args:\n",
       "      variant_tensor: A DT_VARIANT tensor that represents the dataset.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor_attr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_attr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Initialize the options for this dataset and its inputs.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options_attr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0minput_dataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0minput_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# If the V1 dataset does not have the `_dataset` attribute, we assume it\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# is a dataset source and hence does not have options. Otherwise, we\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# grab the options of `_dataset` object\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_dataset\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34mf\"Each input of dataset {type(self)} should be a subclass of \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34mf\"`tf.data.Dataset` but encountered \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;34mf\"{type(input_dataset._dataset)}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0minput_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0minput_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34mf\"Each input of dataset {type(self)} should be a subclass of \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34mf\"`tf.data.Dataset` but encountered {type(input_dataset)}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0minput_options\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options_attr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mutable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_variant_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor_attr\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_variant_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The `_variant_tensor` property cannot be modified.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecated_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Use external_state_policy instead\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                               \u001b[1;34m\"allow_stateful\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_as_serialized_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mallow_stateful\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mstrip_device_assignment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mexternal_state_policy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExternalStatePolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWARN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Produces serialized graph representation of the dataset.\n",
       "\n",
       "    Args:\n",
       "      allow_stateful: If true, we allow stateful ops to be present in the graph\n",
       "        def. In that case, the state in these ops would be thrown away.\n",
       "      strip_device_assignment: If true, non-local (i.e. job and task) device\n",
       "        assignment is stripped from ops in the serialized graph.\n",
       "      external_state_policy: The ExternalStatePolicy enum that determines how we\n",
       "        handle input pipelines that depend on external state. By default, its\n",
       "        set to WARN.\n",
       "\n",
       "    Returns:\n",
       "      A scalar `tf.Tensor` of `tf.string` type, representing this dataset as a\n",
       "      serialized graph.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mexternal_state_policy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexternal_state_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_to_graph_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mexternal_state_policy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mstrip_device_assignment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrip_device_assignment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mstrip_device_assignment\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mallow_stateful\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_stateful\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mstrip_device_assignment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrip_device_assignment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_stateful\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_stateful\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_maybe_track_assets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Finds and tracks nodes in `graph_def` that refer to asset files.\n",
       "\n",
       "    Args:\n",
       "      graph_def: Serialized graph representation of this dataset.\n",
       "\n",
       "    Returns:\n",
       "      A dictionary mapping the node name of an asset constant to a tracked\n",
       "      `asset.Asset` object.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0masset_tracker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FileIdentity\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0masset_tracker\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0masset_tracker\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0masset_tracker\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mtensor_proto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CPU\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mnode_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_parsing_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0masset_tracker\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_track_trackable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAsset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0masset_tracker\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_trackable_children\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                          \u001b[0msave_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtracking_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaveType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCHECKPOINT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                          \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0msave_type\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtracking_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaveType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSAVEDMODEL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# _trace_variant_creation only works when executing eagerly, so we don't\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# want to run it in the object initialization.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mdef_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_creator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mresource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trace_variant_creation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mresource\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_creator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Trigger asset tracking\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mchildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_children\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mchildren\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"_variant_tracker\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_VariantTracker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                                   \u001b[0m_creator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mchildren\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_trace_variant_creation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Traces a function which outputs a variant `tf.Tensor` for this dataset.\n",
       "\n",
       "    Note that creating this function involves evaluating an op, and is currently\n",
       "    only supported when executing eagerly.\n",
       "\n",
       "    Returns:\n",
       "      A zero-argument `ConcreteFunction` which outputs a variant `tf.Tensor`.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvariant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34m\"Constructing a tf.function that reproduces a given dataset is only \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34m\"supported for datasets created eagerly. Please file a feature \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34m\"request if this is important to you.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CPU\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mgraph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_serialized_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexternal_state_policy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions_lib\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                    \u001b[1;33m.\u001b[0m\u001b[0mExternalStatePolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFAIL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutput_node_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"_Retval\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0moutput_node_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_node_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34mf\"Dataset graph is expected to only have one return value but found \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34mf\"{len(output_node_names)} return values: {output_node_names}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutput_node_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_node_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfile_path_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# When building a tf.function, track files as `saved_model.Asset`s.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilding_function\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0masset_tracker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_track_assets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0masset_tracker\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0massets_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masset_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0masset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0masset_tracker\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mfile_path_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massets_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Add functions used in this Dataset to the function's graph, since they\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# need to follow it around (and for example be added to a SavedModel which\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# references the dataset).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvariant_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrap_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_from_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mgraph_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_node_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\":0\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mcaptures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_path_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0mused_function\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mused_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariant_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mvariant_function\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns a list of the input datasets of the dataset.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{type(self)}._inputs()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_attr\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The `_graph` property cannot be modified.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;31m# TODO(jsimsa): Change this to be the transitive closure of functions used\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;31m# by this dataset and its inputs.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mStructuredFunctionWrapper\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns a list of functions associated with this dataset.\n",
       "\n",
       "    Returns:\n",
       "      A list of `StructuredFunctionWrapper` objects.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns the options tensor for this dataset.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_options_tensor_to_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserialized_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Converts options tensor to tf.data.Options object.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized_options\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mpb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_options_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFromString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mserialized_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpb\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns the options for this dataset and its inputs.\n",
       "\n",
       "    Returns:\n",
       "      A `tf.data.Options` object representing the dataset options.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options_tensor_to_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mutable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"To make it possible to preserve tf.data options across \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                  \u001b[1;34m\"serialization boundaries, their implementation has moved to \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                  \u001b[1;34m\"be part of the TensorFlow graph. As a consequence, the \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                  \u001b[1;34m\"options value is in general no longer known at graph \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                  \u001b[1;34m\"construction time. Invoking this method in graph mode \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                  \u001b[1;34m\"retains the legacy behavior of the original implementation, \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                  \u001b[1;34m\"but note that the returned value might not reflect the \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                  \u001b[1;34m\"actual value of the options.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options_attr\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_apply_debug_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mdebug_mode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEBUG_MODE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# Disable autotuning and static optimizations that could introduce\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# parallelism or asynchrony.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautotune\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_optimization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_parallelization\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_optimization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_and_batch_fusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_optimization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_parallelization\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_OptionsDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates an iterator for elements of this dataset.\n",
       "\n",
       "    The returned iterator implements the Python Iterator protocol.\n",
       "\n",
       "    Returns:\n",
       "      An `tf.data.Iterator` for the elements of this dataset.\n",
       "\n",
       "    Raises:\n",
       "      RuntimeError: If not inside of tf.function and not executing eagerly.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`tf.data.Dataset` only supports Python-style \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                         \u001b[1;34m\"iteration in eager mode or within tf.function.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m  \u001b[1;31m# Required as __len__ is defined\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0m__nonzero__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__bool__\u001b[0m  \u001b[1;31m# Python 2 backward compatibility\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns the length of the dataset if it is known and finite.\n",
       "\n",
       "    This method requires that you are running in eager mode, and that the\n",
       "    length of the dataset is known and non-infinite. When the length may be\n",
       "    unknown or infinite, or if you are running in graph mode, use\n",
       "    `tf.data.Dataset.cardinality` instead.\n",
       "\n",
       "    Returns:\n",
       "      An integer representing the length of the dataset.\n",
       "\n",
       "    Raises:\n",
       "      RuntimeError: If the dataset length is unknown or infinite, or if eager\n",
       "        execution is not enabled.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`tf.data.Dataset` only supports `len` in eager mode. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                      \u001b[1;34m\"Use `tf.data.Dataset.cardinality()` instead.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mINFINITE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The dataset is infinite.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mUNKNOWN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The dataset length is unknown.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractproperty\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"The type specification of an element of this dataset.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
       "    >>> dataset.element_spec\n",
       "    TensorSpec(shape=(), dtype=tf.int32, name=None)\n",
       "\n",
       "    For more information,\n",
       "    read [this guide](https://www.tensorflow.org/guide/data#dataset_structure).\n",
       "\n",
       "    Returns:\n",
       "      A (nested) structure of `tf.TypeSpec` objects matching the structure of an\n",
       "      element of this dataset and specifying the type of individual components.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{type(self)}.element_spec()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtype_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;34mf\"<{type_.__name__} element_spec={self.element_spec}>\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m__debug_string__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns a string showing the type of the dataset and its inputs.\n",
       "\n",
       "    This string is intended only for debugging purposes, and may change without\n",
       "    warning.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mto_process\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Stack of (dataset, depth) pairs.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mwhile\u001b[0m \u001b[0mto_process\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mto_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mas_numpy_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns an iterator which converts all elements of the dataset to numpy.\n",
       "\n",
       "    Use `as_numpy_iterator` to inspect the content of your dataset. To see\n",
       "    element shapes and types, print dataset elements directly instead of using\n",
       "    `as_numpy_iterator`.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
       "    >>> for element in dataset:\n",
       "    ...   print(element)\n",
       "    tf.Tensor(1, shape=(), dtype=int32)\n",
       "    tf.Tensor(2, shape=(), dtype=int32)\n",
       "    tf.Tensor(3, shape=(), dtype=int32)\n",
       "\n",
       "    This method requires that you are running in eager mode and the dataset's\n",
       "    element_spec contains only `TensorSpec` components.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
       "    >>> for element in dataset.as_numpy_iterator():\n",
       "    ...   print(element)\n",
       "    1\n",
       "    2\n",
       "    3\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
       "    >>> print(list(dataset.as_numpy_iterator()))\n",
       "    [1, 2, 3]\n",
       "\n",
       "    `as_numpy_iterator()` will preserve the nested structure of dataset\n",
       "    elements.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n",
       "    ...                                               'b': [5, 6]})\n",
       "    >>> list(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n",
       "    ...                                       {'a': (2, 4), 'b': 6}]\n",
       "    True\n",
       "\n",
       "    Returns:\n",
       "      An iterable over the elements of the dataset, with their tensors converted\n",
       "      to numpy arrays.\n",
       "\n",
       "    Raises:\n",
       "      TypeError: if an element contains a non-`Tensor` value.\n",
       "      RuntimeError: if eager execution is not enabled.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"`tf.data.Dataset.as_numpy_iterator()` is only \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                         \u001b[1;34m\"supported in eager mode.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0mcomponent_spec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mcomponent_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;33m(\u001b[0m\u001b[0mtensor_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mragged_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRaggedTensorSpec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m           \u001b[0msparse_tensor_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensorSpec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnone_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoneTensorSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34mf\"`tf.data.Dataset.as_numpy_iterator()` is not supported for \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;34mf\"datasets that produce values of type {component_spec.value_type}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mNumpyIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_flat_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns a list `tf.TensorShapes`s for the element tensor representation.\n",
       "\n",
       "    Returns:\n",
       "      A list `tf.TensorShapes`s for the element tensor representation.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flat_tensor_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_flat_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns a list `tf.DType`s for the element tensor representation.\n",
       "\n",
       "    Returns:\n",
       "      A list `tf.DType`s for the element tensor representation.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flat_tensor_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_flat_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Helper for setting `output_shapes` and `output_types` attrs of an op.\n",
       "\n",
       "    Most dataset op constructors expect `output_shapes` and `output_types`\n",
       "    arguments that represent the flattened structure of an element. This helper\n",
       "    function generates these attrs as a keyword argument dictionary, allowing\n",
       "    `Dataset._variant_tensor` implementations to pass `**self._flat_structure`\n",
       "    to the op constructor.\n",
       "\n",
       "    Returns:\n",
       "      A dictionary of keyword arguments that can be passed to a dataset op\n",
       "      constructor.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Helper for generating dataset metadata.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_and_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_common_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Helper for generating arguments that are common across most dataset ops.\n",
       "\n",
       "    Most dataset op constructors expect `output_shapes` and `output_types`\n",
       "    arguments that represent the flattened structure of an element, as well as a\n",
       "    `metadata` argument for additional metadata such as user-defined dataset\n",
       "    name. This helper function generates common attributes as a keyword argument\n",
       "    dictionary, allowing `Dataset._variant_tensor` implementations to pass\n",
       "    `**self._common_args` to the op constructor.\n",
       "\n",
       "    Returns:\n",
       "      A dictionary of keyword arguments that can be passed to a dataset op\n",
       "      constructor.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0m_type_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mfrom_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` with a single element, comprising the given tensors.\n",
       "\n",
       "    `from_tensors` produces a dataset containing only a single element. To slice\n",
       "    the input tensor into multiple elements, use `from_tensor_slices` instead.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensors([1, 2, 3])\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [array([1, 2, 3], dtype=int32)]\n",
       "    >>> dataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [(array([1, 2, 3], dtype=int32), b'A')]\n",
       "\n",
       "    >>> # You can use `from_tensors` to produce a dataset which repeats\n",
       "    >>> # the same example many times.\n",
       "    >>> example = tf.constant([1,2,3])\n",
       "    >>> dataset = tf.data.Dataset.from_tensors(example).repeat(2)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n",
       "\n",
       "    Note that if `tensors` contains a NumPy array, and eager execution is not\n",
       "    enabled, the values will be embedded in the graph as one or more\n",
       "    `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
       "    memory and run into byte limits of graph serialization. If `tensors`\n",
       "    contains one or more large NumPy arrays, consider the alternative described\n",
       "    in [this\n",
       "    guide](https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
       "\n",
       "    Args:\n",
       "      tensors: A dataset \"element\". Supported values are documented\n",
       "        [here](https://www.tensorflow.org/guide/data#dataset_structure).\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      Dataset: A `Dataset`.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# from_tensors_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\n",
       "\n",
       "    The given tensors are sliced along their first dimension. This operation\n",
       "    preserves the structure of the input tensors, removing the first dimension\n",
       "    of each tensor and using it as the dataset dimension. All input tensors\n",
       "    must have the same size in their first dimensions.\n",
       "\n",
       "    >>> # Slicing a 1D tensor produces scalar tensor elements.\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [1, 2, 3]\n",
       "\n",
       "    >>> # Slicing a 2D tensor produces 1D tensor elements.\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n",
       "\n",
       "    >>> # Slicing a tuple of 1D tensors produces tuple elements containing\n",
       "    >>> # scalar tensors.\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [(1, 3, 5), (2, 4, 6)]\n",
       "\n",
       "    >>> # Dictionary structure is also preserved.\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\n",
       "    >>> list(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n",
       "    ...                                       {'a': 2, 'b': 4}]\n",
       "    True\n",
       "\n",
       "    >>> # Two tensors can be combined into one Dataset object.\n",
       "    >>> features = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\n",
       "    >>> labels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\n",
       "    >>> dataset = Dataset.from_tensor_slices((features, labels))\n",
       "    >>> # Both the features and the labels tensors can be converted\n",
       "    >>> # to a Dataset object separately and combined after.\n",
       "    >>> features_dataset = Dataset.from_tensor_slices(features)\n",
       "    >>> labels_dataset = Dataset.from_tensor_slices(labels)\n",
       "    >>> dataset = Dataset.zip((features_dataset, labels_dataset))\n",
       "    >>> # A batched feature and label set can be converted to a Dataset\n",
       "    >>> # in similar fashion.\n",
       "    >>> batched_features = tf.constant([[[1, 3], [2, 3]],\n",
       "    ...                                 [[2, 1], [1, 2]],\n",
       "    ...                                 [[3, 3], [3, 2]]], shape=(3, 2, 2))\n",
       "    >>> batched_labels = tf.constant([['A', 'A'],\n",
       "    ...                               ['B', 'B'],\n",
       "    ...                               ['A', 'B']], shape=(3, 2, 1))\n",
       "    >>> dataset = Dataset.from_tensor_slices((batched_features, batched_labels))\n",
       "    >>> for element in dataset.as_numpy_iterator():\n",
       "    ...   print(element)\n",
       "    (array([[1, 3],\n",
       "           [2, 3]], dtype=int32), array([[b'A'],\n",
       "           [b'A']], dtype=object))\n",
       "    (array([[2, 1],\n",
       "           [1, 2]], dtype=int32), array([[b'B'],\n",
       "           [b'B']], dtype=object))\n",
       "    (array([[3, 3],\n",
       "           [3, 2]], dtype=int32), array([[b'A'],\n",
       "           [b'B']], dtype=object))\n",
       "\n",
       "    Note that if `tensors` contains a NumPy array, and eager execution is not\n",
       "    enabled, the values will be embedded in the graph as one or more\n",
       "    `tf.constant` operations. For large datasets (> 1 GB), this can waste\n",
       "    memory and run into byte limits of graph serialization. If `tensors`\n",
       "    contains one or more large NumPy arrays, consider the alternative described\n",
       "    in [this guide](\n",
       "    https://tensorflow.org/guide/data#consuming_numpy_arrays).\n",
       "\n",
       "    Args:\n",
       "      tensors: A dataset element, whose components have the same first\n",
       "        dimension. Supported values are documented\n",
       "        [here](https://www.tensorflow.org/guide/data#dataset_structure).\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      Dataset: A `Dataset`.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# from_tensor_slices_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Stores outstanding iterators created from a Python generator.\n",
       "\n",
       "    This class keeps track of potentially multiple iterators that may have\n",
       "    been created from a generator, e.g. in the case that the dataset is\n",
       "    repeated, or nested within a parallel computation.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m  \u001b[1;31m# GUARDED_BY(self._lock)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0m_normalize_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# In debug mode, iterator ids may be eagerly-generated np.arrays instead\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# of Tensors. We convert them to scalars to make them hashable.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0miterator_id\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0miterator_id\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mget_next_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_id\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_id\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# NOTE(mrry): Explicitly create an array of `np.int64` because implicit\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# casting in `py_func()` will create an array of `np.int32` on Windows,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# leading to a runtime error.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mget_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0miterator_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_normalize_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0miterator_completed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_normalize_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecated_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Use output_signature instead\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                               \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mfrom_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moutput_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moutput_signature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` whose elements are generated by `generator`.\n",
       "\n",
       "    Note: The current implementation of `Dataset.from_generator()` uses\n",
       "    `tf.numpy_function` and inherits the same constraints. In particular, it\n",
       "    requires the dataset and iterator related operations to be placed\n",
       "    on a device in the same process as the Python program that called\n",
       "    `Dataset.from_generator()`. In particular, using `from_generator` will\n",
       "    preclude the use of tf.data service for scaling out dataset processing.\n",
       "    The body of `generator` will not be serialized in a `GraphDef`, and you\n",
       "    should not use this method if you need to serialize your model and restore\n",
       "    it in a different environment.\n",
       "\n",
       "    The `generator` argument must be a callable object that returns\n",
       "    an object that supports the `iter()` protocol (e.g. a generator function).\n",
       "\n",
       "    The elements generated by `generator` must be compatible with either the\n",
       "    given `output_signature` argument or with the given `output_types` and\n",
       "    (optionally) `output_shapes` arguments, whichever was specified.\n",
       "\n",
       "    The recommended way to call `from_generator` is to use the\n",
       "    `output_signature` argument. In this case the output will be assumed to\n",
       "    consist of objects with the classes, shapes and types defined by\n",
       "    `tf.TypeSpec` objects from `output_signature` argument:\n",
       "\n",
       "    >>> def gen():\n",
       "    ...   ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n",
       "    ...   yield 42, ragged_tensor\n",
       "    >>>\n",
       "    >>> dataset = tf.data.Dataset.from_generator(\n",
       "    ...      gen,\n",
       "    ...      output_signature=(\n",
       "    ...          tf.TensorSpec(shape=(), dtype=tf.int32),\n",
       "    ...          tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n",
       "    >>>\n",
       "    >>> list(dataset.take(1))\n",
       "    [(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n",
       "    <tf.RaggedTensor [[1, 2], [3]]>)]\n",
       "\n",
       "    There is also a deprecated way to call `from_generator` by either with\n",
       "    `output_types` argument alone or together with `output_shapes` argument.\n",
       "    In this case the output of the function will be assumed to consist of\n",
       "    `tf.Tensor` objects with the types defined by `output_types` and with the\n",
       "    shapes which are either unknown or defined by `output_shapes`.\n",
       "\n",
       "    Note: If `generator` depends on mutable global variables or other external\n",
       "    state, be aware that the runtime may invoke `generator` multiple times\n",
       "    (in order to support repeating the `Dataset`) and at any time\n",
       "    between the call to `Dataset.from_generator()` and the production of the\n",
       "    first element from the generator. Mutating global variables or external\n",
       "    state can cause undefined behavior, and we recommend that you explicitly\n",
       "    cache any external state in `generator` before calling\n",
       "    `Dataset.from_generator()`.\n",
       "\n",
       "    Note: While the `output_signature` parameter makes it possible to yield\n",
       "    `Dataset` elements, the scope of `Dataset.from_generator()` should be\n",
       "    limited to logic that cannot be expressed through tf.data operations. Using\n",
       "    tf.data operations within the generator function is an anti-pattern and may\n",
       "    result in incremental memory growth.\n",
       "\n",
       "    Args:\n",
       "      generator: A callable object that returns an object that supports the\n",
       "        `iter()` protocol. If `args` is not specified, `generator` must take no\n",
       "        arguments; otherwise it must take as many arguments as there are values\n",
       "        in `args`.\n",
       "      output_types: (Optional.) A (nested) structure of `tf.DType` objects\n",
       "        corresponding to each component of an element yielded by `generator`.\n",
       "      output_shapes: (Optional.) A (nested) structure of `tf.TensorShape`\n",
       "        objects corresponding to each component of an element yielded by\n",
       "        `generator`.\n",
       "      args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated\n",
       "        and passed to `generator` as NumPy-array arguments.\n",
       "      output_signature: (Optional.) A (nested) structure of `tf.TypeSpec`\n",
       "        objects corresponding to each component of an element yielded by\n",
       "        `generator`.\n",
       "      name: (Optional.) A name for the tf.data operations used by\n",
       "        `from_generator`.\n",
       "\n",
       "    Returns:\n",
       "      Dataset: A `Dataset`.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# from_generator_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_generator_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mfrom_generator_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                             \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                             \u001b[0moutput_signature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` of a step-separated range of values.\n",
       "\n",
       "    >>> list(Dataset.range(5).as_numpy_iterator())\n",
       "    [0, 1, 2, 3, 4]\n",
       "    >>> list(Dataset.range(2, 5).as_numpy_iterator())\n",
       "    [2, 3, 4]\n",
       "    >>> list(Dataset.range(1, 5, 2).as_numpy_iterator())\n",
       "    [1, 3]\n",
       "    >>> list(Dataset.range(1, 5, -2).as_numpy_iterator())\n",
       "    []\n",
       "    >>> list(Dataset.range(5, 1).as_numpy_iterator())\n",
       "    []\n",
       "    >>> list(Dataset.range(5, 1, -2).as_numpy_iterator())\n",
       "    [5, 3]\n",
       "    >>> list(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n",
       "    [2, 3, 4]\n",
       "    >>> list(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n",
       "    [1.0, 3.0]\n",
       "\n",
       "    Args:\n",
       "      *args: follows the same semantics as python's range.\n",
       "        len(args) == 1 -> start = 0, stop = args[0], step = 1.\n",
       "        len(args) == 2 -> start = args[0], stop = args[1], step = 1.\n",
       "        len(args) == 3 -> start = args[0], stop = args[1], step = args[2].\n",
       "      **kwargs:\n",
       "        - output_type: Its expected dtype. (Optional, default: `tf.int64`).\n",
       "        - name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      Dataset: A `RangeDataset`.\n",
       "\n",
       "    Raises:\n",
       "      ValueError: if len(args) == 0.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> range_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mrange_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` by zipping together the given datasets.\n",
       "\n",
       "    This method has similar semantics to the built-in `zip()` function\n",
       "    in Python, with the main difference being that the `datasets`\n",
       "    argument can be a (nested) structure of `Dataset` objects. The supported\n",
       "    nesting mechanisms are documented\n",
       "    [here] (https://www.tensorflow.org/guide/data#dataset_structure).\n",
       "\n",
       "    >>> # The datasets or nested structure of datasets `*args` argument\n",
       "    >>> # determines the structure of elements in the resulting dataset.\n",
       "    >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
       "    >>> b = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\n",
       "    >>> ds = tf.data.Dataset.zip(a, b)\n",
       "    >>> list(ds.as_numpy_iterator())\n",
       "    [(1, 4), (2, 5), (3, 6)]\n",
       "    >>> ds = tf.data.Dataset.zip(b, a)\n",
       "    >>> list(ds.as_numpy_iterator())\n",
       "    [(4, 1), (5, 2), (6, 3)]\n",
       "    >>>\n",
       "    >>> # The `datasets` argument may contain an arbitrary number of datasets.\n",
       "    >>> c = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n",
       "    ...                                            #       [9, 10],\n",
       "    ...                                            #       [11, 12] ]\n",
       "    >>> ds = tf.data.Dataset.zip(a, b, c)\n",
       "    >>> for element in ds.as_numpy_iterator():\n",
       "    ...   print(element)\n",
       "    (1, 4, array([7, 8]))\n",
       "    (2, 5, array([ 9, 10]))\n",
       "    (3, 6, array([11, 12]))\n",
       "    >>>\n",
       "    >>> # The number of elements in the resulting dataset is the same as\n",
       "    >>> # the size of the smallest dataset in `datasets`.\n",
       "    >>> d = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\n",
       "    >>> ds = tf.data.Dataset.zip(a, d)\n",
       "    >>> list(ds.as_numpy_iterator())\n",
       "    [(1, 13), (2, 14)]\n",
       "\n",
       "    Args:\n",
       "      *args: Datasets or nested structures of datasets to zip together. This\n",
       "        can't be set if `datasets` is set.\n",
       "      datasets: A (nested) structure of datasets. This can't be set if `*args`\n",
       "        is set. Note that this exists only for backwards compatibility and it is\n",
       "        preferred to use *args.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> zip_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mzip_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Must pass at least one dataset to `zip`.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Both `*args` and `datasets` cannot be set.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mzip_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` by concatenating the given dataset with this dataset.\n",
       "\n",
       "    >>> a = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\n",
       "    >>> b = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\n",
       "    >>> ds = a.concatenate(b)\n",
       "    >>> list(ds.as_numpy_iterator())\n",
       "    [1, 2, 3, 4, 5, 6, 7]\n",
       "    >>> # The input dataset and dataset to be concatenated should have\n",
       "    >>> # compatible element specs.\n",
       "    >>> c = tf.data.Dataset.zip((a, b))\n",
       "    >>> a.concatenate(c)\n",
       "    Traceback (most recent call last):\n",
       "    TypeError: Two datasets to concatenate have different types\n",
       "    <dtype: 'int64'> and (tf.int64, tf.int64)\n",
       "    >>> d = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\n",
       "    >>> a.concatenate(d)\n",
       "    Traceback (most recent call last):\n",
       "    TypeError: Two datasets to concatenate have different types\n",
       "    <dtype: 'int64'> and <dtype: 'string'>\n",
       "\n",
       "    Args:\n",
       "      dataset: `Dataset` to be concatenated.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# concatenate_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconcatenate_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mcounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` that counts from `start` in steps of size `step`.\n",
       "\n",
       "    Unlike `tf.data.Dataset.range`, which stops at some ending number,\n",
       "    `tf.data.Dataset.counter` produces elements indefinitely.\n",
       "\n",
       "    >>> dataset = tf.data.experimental.Counter().take(5)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [0, 1, 2, 3, 4]\n",
       "    >>> dataset.element_spec\n",
       "    TensorSpec(shape=(), dtype=tf.int64, name=None)\n",
       "    >>> dataset = tf.data.experimental.Counter(dtype=tf.int32)\n",
       "    >>> dataset.element_spec\n",
       "    TensorSpec(shape=(), dtype=tf.int32, name=None)\n",
       "    >>> dataset = tf.data.experimental.Counter(start=2).take(5)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [2, 3, 4, 5, 6]\n",
       "    >>> dataset = tf.data.experimental.Counter(start=2, step=5).take(5)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [2, 7, 12, 17, 22]\n",
       "    >>> dataset = tf.data.experimental.Counter(start=10, step=-1).take(5)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [10, 9, 8, 7, 6]\n",
       "\n",
       "    Args:\n",
       "      start: (Optional.) The starting value for the counter. Defaults to 0.\n",
       "      step: (Optional.) The step size for the counter. Defaults to 1.\n",
       "      dtype: (Optional.) The data type for counter elements. Defaults to\n",
       "        `tf.int64`.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A `Dataset` of scalar `dtype` elements.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> counter_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcounter_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mcounter_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mfingerprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Computes the fingerprint of this `Dataset`.\n",
       "\n",
       "    If two datasets have the same fingerprint, it is guaranteeed that they\n",
       "    would produce identical elements as long as the content of the upstream\n",
       "    input files does not change and they produce data deterministically.\n",
       "\n",
       "    However, two datasets producing identical values does not always mean they\n",
       "    would have the same fingerprint due to different graph constructs.\n",
       "\n",
       "    In other words, if two datasets have different fingerprints, they could\n",
       "    still produce identical values.\n",
       "\n",
       "    Returns:\n",
       "      A scalar `tf.Tensor` of type `tf.uint64`.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fingerprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mrebatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` that rebatches the elements from this dataset.\n",
       "\n",
       "    `rebatch(N)` is functionally equivalent to `unbatch().batch(N)`, but is\n",
       "    more efficient, performing one copy instead of two.\n",
       "\n",
       "    >>> ds = tf.data.Dataset.range(6)\n",
       "    >>> ds = ds.batch(2)\n",
       "    >>> ds = ds.rebatch(3)\n",
       "    >>> list(ds.as_numpy_iterator())\n",
       "    [array([0, 1, 2]), array([3, 4, 5])]\n",
       "\n",
       "    >>> ds = tf.data.Dataset.range(7)\n",
       "    >>> ds = ds.batch(4)\n",
       "    >>> ds = ds.rebatch(3)\n",
       "    >>> list(ds.as_numpy_iterator())\n",
       "    [array([0, 1, 2]), array([3, 4, 5]), array([6])]\n",
       "\n",
       "    >>> ds = tf.data.Dataset.range(7)\n",
       "    >>> ds = ds.batch(2)\n",
       "    >>> ds = ds.rebatch(3, drop_remainder=True)\n",
       "    >>> list(ds.as_numpy_iterator())\n",
       "    [array([0, 1, 2]), array([3, 4, 5])]\n",
       "\n",
       "    If the `batch_size` argument is a list, `rebatch` cycles through the list\n",
       "    to determine the size of each batch.\n",
       "\n",
       "    >>> ds = tf.data.Dataset.range(8)\n",
       "    >>> ds = ds.batch(4)\n",
       "    >>> ds = ds.rebatch([2, 1, 1])\n",
       "    >>> list(ds.as_numpy_iterator())\n",
       "    [array([0, 1]), array([2]), array([3]), array([4, 5]), array([6]),\n",
       "    array([7])]\n",
       "\n",
       "    Args:\n",
       "      batch_size: A `tf.int64` scalar or vector, representing the size of\n",
       "        batches to produce. If this argument is a vector, these values are\n",
       "        cycled through in round robin fashion.\n",
       "      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
       "        whether the last batch should be dropped in the case it has fewer than\n",
       "        `batch_size[cycle_index]` elements; the default behavior is not to drop\n",
       "        the smaller batch.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A `Dataset` of scalar `dtype` elements.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> rebatch_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# rebatch_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrebatch_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mrebatch_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rebatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` that prefetches elements from this dataset.\n",
       "\n",
       "    Most dataset input pipelines should end with a call to `prefetch`. This\n",
       "    allows later elements to be prepared while the current element is being\n",
       "    processed. This often improves latency and throughput, at the cost of\n",
       "    using additional memory to store prefetched elements.\n",
       "\n",
       "    Note: Like other `Dataset` methods, prefetch operates on the\n",
       "    elements of the input dataset. It has no concept of examples vs. batches.\n",
       "    `examples.prefetch(2)` will prefetch two elements (2 examples),\n",
       "    while `examples.batch(20).prefetch(2)` will prefetch 2 elements\n",
       "    (2 batches, of 20 examples each).\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(3)\n",
       "    >>> dataset = dataset.prefetch(2)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [0, 1, 2]\n",
       "\n",
       "    Args:\n",
       "      buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the maximum\n",
       "        number of elements that will be buffered when prefetching. If the value\n",
       "        `tf.data.AUTOTUNE` is used, then the buffer size is dynamically tuned.\n",
       "      name: Optional. A name for the tf.data transformation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mprefetch_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prefetch\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mlist_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mfile_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"A dataset of all files matching one or more glob patterns.\n",
       "\n",
       "    The `file_pattern` argument should be a small number of glob patterns.\n",
       "    If your filenames have already been globbed, use\n",
       "    `Dataset.from_tensor_slices(filenames)` instead, as re-globbing every\n",
       "    filename with `list_files` may result in poor performance with remote\n",
       "    storage systems.\n",
       "\n",
       "    Note: The default behavior of this method is to return filenames in\n",
       "    a non-deterministic random shuffled order. Pass a `seed` or `shuffle=False`\n",
       "    to get results in a deterministic order.\n",
       "\n",
       "    Example:\n",
       "      If we had the following files on our filesystem:\n",
       "\n",
       "        - /path/to/dir/a.txt\n",
       "        - /path/to/dir/b.py\n",
       "        - /path/to/dir/c.py\n",
       "\n",
       "      If we pass \"/path/to/dir/*.py\" as the directory, the dataset\n",
       "      would produce:\n",
       "\n",
       "        - /path/to/dir/b.py\n",
       "        - /path/to/dir/c.py\n",
       "\n",
       "    Args:\n",
       "      file_pattern: A string, a list of strings, or a `tf.Tensor` of string type\n",
       "        (scalar or vector), representing the filename glob (i.e. shell wildcard)\n",
       "        pattern(s) that will be matched.\n",
       "      shuffle: (Optional.) If `True`, the file names will be shuffled randomly.\n",
       "        Defaults to `True`.\n",
       "      seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
       "        seed that will be used to create the distribution. See\n",
       "        `tf.random.set_seed` for behavior.\n",
       "      name: Optional. A name for the tf.data operations used by `list_files`.\n",
       "\n",
       "    Returns:\n",
       "     Dataset: A `Dataset` of strings corresponding to file names.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"list_files\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mfile_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mfile_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"file_pattern\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mmatching_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_pattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# Raise an exception if `file_pattern` does not match any files.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mcondition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgreater\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                   \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"match_not_empty\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34m\"No files matched pattern: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mstring_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_join\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\", \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"message\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0massert_not_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontrol_flow_assert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAssert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"assert_not_empty\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massert_not_empty\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmatching_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# TODO(b/240947712): Remove lazy import after this method is factored out.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# from_tensor_slices_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrom_tensor_slices_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_TensorSliceDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mmatching_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetV1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# NOTE(mrry): The shuffle buffer size must be greater than zero, but the\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# list of files might be empty.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mbuffer_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Repeats this dataset so each original value is seen `count` times.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
       "    >>> dataset = dataset.repeat(3)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
       "\n",
       "    Note: If the input dataset depends on global state (e.g. a random number\n",
       "    generator) or its output is non-deterministic (e.g. because of upstream\n",
       "    `shuffle`), then different repetitions may produce different elements.\n",
       "\n",
       "    Args:\n",
       "      count: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
       "        number of times the dataset should be repeated. The default behavior (if\n",
       "        `count` is `None` or `-1`) is for the dataset be repeated indefinitely.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> repeat_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access,redefined-outer-name\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrepeat_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mrepeat_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_repeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access,redefined-outer-name\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Enumerates the elements of this dataset.\n",
       "\n",
       "    It is similar to python's `enumerate`.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
       "    >>> dataset = dataset.enumerate(start=5)\n",
       "    >>> for element in dataset.as_numpy_iterator():\n",
       "    ...   print(element)\n",
       "    (5, 1)\n",
       "    (6, 2)\n",
       "    (7, 3)\n",
       "\n",
       "    >>> # The (nested) structure of the input dataset determines the\n",
       "    >>> # structure of elements in the resulting dataset.\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\n",
       "    >>> dataset = dataset.enumerate()\n",
       "    >>> for element in dataset.as_numpy_iterator():\n",
       "    ...   print(element)\n",
       "    (0, array([7, 8], dtype=int32))\n",
       "    (1, array([ 9, 10], dtype=int32))\n",
       "\n",
       "    Args:\n",
       "      start: A `tf.int64` scalar `tf.Tensor`, representing the start value for\n",
       "        enumeration.\n",
       "      name: Optional. A name for the tf.data operations used by `enumerate`.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmax_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrange_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Replicate the range component so that each split is enumerated\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# independently. This avoids the need for prohibitively expensive\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# cross-split coordination.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrange_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_apply_rewrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"replicate_on_split\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreshuffle_each_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Randomly shuffles the elements of this dataset.\n",
       "\n",
       "    This dataset fills a buffer with `buffer_size` elements, then randomly\n",
       "    samples elements from this buffer, replacing the selected elements with new\n",
       "    elements. For perfect shuffling, a buffer size greater than or equal to the\n",
       "    full size of the dataset is required.\n",
       "\n",
       "    For instance, if your dataset contains 10,000 elements but `buffer_size` is\n",
       "    set to 1,000, then `shuffle` will initially select a random element from\n",
       "    only the first 1,000 elements in the buffer. Once an element is selected,\n",
       "    its space in the buffer is replaced by the next (i.e. 1,001-st) element,\n",
       "    maintaining the 1,000 element buffer.\n",
       "\n",
       "    `reshuffle_each_iteration` controls whether the shuffle order should be\n",
       "    different for each epoch. In TF 1.X, the idiomatic way to create epochs\n",
       "    was through the `repeat` transformation:\n",
       "\n",
       "    ```python\n",
       "    dataset = tf.data.Dataset.range(3)\n",
       "    dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
       "    dataset = dataset.repeat(2)\n",
       "    # [1, 0, 2, 1, 2, 0]\n",
       "\n",
       "    dataset = tf.data.Dataset.range(3)\n",
       "    dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
       "    dataset = dataset.repeat(2)\n",
       "    # [1, 0, 2, 1, 0, 2]\n",
       "    ```\n",
       "\n",
       "    In TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\n",
       "    possible to also create epochs through Python iteration:\n",
       "\n",
       "    ```python\n",
       "    dataset = tf.data.Dataset.range(3)\n",
       "    dataset = dataset.shuffle(3, reshuffle_each_iteration=True)\n",
       "    list(dataset.as_numpy_iterator())\n",
       "    # [1, 0, 2]\n",
       "    list(dataset.as_numpy_iterator())\n",
       "    # [1, 2, 0]\n",
       "    ```\n",
       "\n",
       "    ```python\n",
       "    dataset = tf.data.Dataset.range(3)\n",
       "    dataset = dataset.shuffle(3, reshuffle_each_iteration=False)\n",
       "    list(dataset.as_numpy_iterator())\n",
       "    # [1, 0, 2]\n",
       "    list(dataset.as_numpy_iterator())\n",
       "    # [1, 0, 2]\n",
       "    ```\n",
       "\n",
       "    #### Fully shuffling all the data\n",
       "\n",
       "    To shuffle an entire dataset, set `buffer_size=dataset.cardinality()`. This\n",
       "    is equivalent to setting the `buffer_size` equal to the number of elements\n",
       "    in the dataset, resulting in uniform shuffle.\n",
       "\n",
       "    Note: `shuffle(dataset.cardinality())` loads the full dataset into memory so\n",
       "    that it can be shuffled. This will cause a memory overflow (OOM) error if\n",
       "    the dataset is too large, so full-shuffle should only be used for datasets\n",
       "    that are known to fit in the memory, such as datasets of filenames or other\n",
       "    small datasets.\n",
       "\n",
       "    ```python\n",
       "    dataset = tf.data.Dataset.range(20)\n",
       "    dataset = dataset.shuffle(dataset.cardinality())\n",
       "    # [18, 4, 9, 2, 17, 8, 5, 10, 0, 6, 16, 3, 19, 7, 14, 11, 15, 13, 12, 1]\n",
       "    ```\n",
       "\n",
       "    Args:\n",
       "      buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
       "        elements from this dataset from which the new dataset will sample. To\n",
       "        uniformly shuffle the entire dataset, use\n",
       "        `buffer_size=dataset.cardinality()`.\n",
       "      seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
       "        seed that will be used to create the distribution. See\n",
       "        `tf.random.set_seed` for behavior.\n",
       "      reshuffle_each_iteration: (Optional.) A boolean, which if true indicates\n",
       "        that the dataset should be pseudorandomly reshuffled each time it is\n",
       "        iterated over. (Defaults to `True`.)\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mshuffle_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shuffle\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreshuffle_each_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Caches the elements in this dataset.\n",
       "\n",
       "    The first time the dataset is iterated over, its elements will be cached\n",
       "    either in the specified file or in memory. Subsequent iterations will\n",
       "    use the cached data.\n",
       "\n",
       "    Note: To guarantee that the cache gets finalized, the input dataset must be\n",
       "    iterated through in its entirety, until it raises StopIteration. Otherwise,\n",
       "    subsequent iterations may not use cached data.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(5)\n",
       "    >>> dataset = dataset.map(lambda x: x**2)\n",
       "    >>> dataset = dataset.cache()\n",
       "    >>> # The first time reading through the data will generate the data using\n",
       "    >>> # `range` and `map`.\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [0, 1, 4, 9, 16]\n",
       "    >>> # Subsequent iterations read from the cache.\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [0, 1, 4, 9, 16]\n",
       "\n",
       "    When caching to a file, the cached data will persist across runs. Even the\n",
       "    first iteration through the data will read from the cache file. Changing\n",
       "    the input pipeline before the call to `.cache()` will have no effect until\n",
       "    the cache file is removed or the filename is changed.\n",
       "\n",
       "    ```python\n",
       "    dataset = tf.data.Dataset.range(5)\n",
       "    dataset = dataset.cache(\"/path/to/file\")\n",
       "    list(dataset.as_numpy_iterator())\n",
       "    # [0, 1, 2, 3, 4]\n",
       "    dataset = tf.data.Dataset.range(10)\n",
       "    dataset = dataset.cache(\"/path/to/file\")  # Same file!\n",
       "    list(dataset.as_numpy_iterator())\n",
       "    # [0, 1, 2, 3, 4]\n",
       "    ```\n",
       "\n",
       "    Note: `cache` will produce exactly the same elements during each iteration\n",
       "    through the dataset. If you wish to randomize the iteration order, make sure\n",
       "    to call `shuffle` *after* calling `cache`.\n",
       "\n",
       "    Args:\n",
       "      filename: A `tf.string` scalar `tf.Tensor`, representing the name of a\n",
       "        directory on the filesystem to use for caching elements in this Dataset.\n",
       "        If a filename is not provided, the dataset will be cached in memory.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> cache_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcache_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mcache_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` with at most `count` elements from this dataset.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(10)\n",
       "    >>> dataset = dataset.take(3)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [0, 1, 2]\n",
       "\n",
       "    Args:\n",
       "      count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
       "        elements of this dataset that should be taken to form the new dataset.\n",
       "        If `count` is -1, or if `count` is greater than the size of this\n",
       "        dataset, the new dataset will contain all elements of this dataset.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# take_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtake_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mtake_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mskip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` that skips `count` elements from this dataset.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(10)\n",
       "    >>> dataset = dataset.skip(7)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [7, 8, 9]\n",
       "\n",
       "    Args:\n",
       "      count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
       "        elements of this dataset that should be skipped to form the new dataset.\n",
       "        If `count` is greater than the size of this dataset, the new dataset\n",
       "        will contain no elements.  If `count` is -1, skips the entire dataset.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# skip_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskip_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mskip_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mshard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_shards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` that includes only 1/`num_shards` of this dataset.\n",
       "\n",
       "    `shard` is deterministic. The Dataset produced by `A.shard(n, i)` will\n",
       "    contain all elements of A whose index mod n = i.\n",
       "\n",
       "    >>> A = tf.data.Dataset.range(10)\n",
       "    >>> B = A.shard(num_shards=3, index=0)\n",
       "    >>> list(B.as_numpy_iterator())\n",
       "    [0, 3, 6, 9]\n",
       "    >>> C = A.shard(num_shards=3, index=1)\n",
       "    >>> list(C.as_numpy_iterator())\n",
       "    [1, 4, 7]\n",
       "    >>> D = A.shard(num_shards=3, index=2)\n",
       "    >>> list(D.as_numpy_iterator())\n",
       "    [2, 5, 8]\n",
       "\n",
       "    This dataset operator is very useful when running distributed training, as\n",
       "    it allows each worker to read a unique subset.\n",
       "\n",
       "    When reading a single input file, you can shard elements as follows:\n",
       "\n",
       "    ```python\n",
       "    d = tf.data.TFRecordDataset(input_file)\n",
       "    d = d.shard(num_workers, worker_index)\n",
       "    d = d.repeat(num_epochs)\n",
       "    d = d.shuffle(shuffle_buffer_size)\n",
       "    d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
       "    ```\n",
       "\n",
       "    Important caveats:\n",
       "\n",
       "    - Be sure to shard before you use any randomizing operator (such as\n",
       "      shuffle).\n",
       "    - Generally it is best if the shard operator is used early in the dataset\n",
       "      pipeline. For example, when reading from a set of TFRecord files, shard\n",
       "      before converting the dataset to input samples. This avoids reading every\n",
       "      file on every worker. The following is an example of an efficient\n",
       "      sharding strategy within a complete pipeline:\n",
       "\n",
       "    ```python\n",
       "    d = Dataset.list_files(pattern, shuffle=False)\n",
       "    d = d.shard(num_workers, worker_index)\n",
       "    d = d.repeat(num_epochs)\n",
       "    d = d.shuffle(shuffle_buffer_size)\n",
       "    d = d.interleave(tf.data.TFRecordDataset,\n",
       "                     cycle_length=num_readers, block_length=1)\n",
       "    d = d.map(parser_fn, num_parallel_calls=num_map_threads)\n",
       "    ```\n",
       "\n",
       "    Args:\n",
       "      num_shards: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
       "        shards operating in parallel.\n",
       "      index: A `tf.int64` scalar `tf.Tensor`, representing the worker index.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "\n",
       "    Raises:\n",
       "      InvalidArgumentError: if `num_shards` or `index` are illegal values.\n",
       "\n",
       "        Note: error checking is done on a best-effort basis, and errors aren't\n",
       "        guaranteed to be caught upon dataset creation. (e.g. providing in a\n",
       "        placeholder tensor bypasses the early checking, and will instead result\n",
       "        in an error during a session.run call.)\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshard_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mshard_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_shards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m           \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m           \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m           \u001b[0mshard_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m           \u001b[0mcheckpoint_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Saves the content of the given dataset.\n",
       "\n",
       "      Example usage:\n",
       "\n",
       "      >>> import tempfile\n",
       "      >>> path = os.path.join(tempfile.gettempdir(), \"saved_data\")\n",
       "      >>> # Save a dataset\n",
       "      >>> dataset = tf.data.Dataset.range(2)\n",
       "      >>> dataset.save(path)\n",
       "      >>> new_dataset = tf.data.Dataset.load(path)\n",
       "      >>> for elem in new_dataset:\n",
       "      ...   print(elem)\n",
       "      tf.Tensor(0, shape=(), dtype=int64)\n",
       "      tf.Tensor(1, shape=(), dtype=int64)\n",
       "\n",
       "      The saved dataset is saved in multiple file \"shards\". By default, the\n",
       "      dataset output is divided to shards in a round-robin fashion but custom\n",
       "      sharding can be specified via the `shard_func` function. For example, you\n",
       "      can save the dataset to using a single shard as follows:\n",
       "\n",
       "      ```python\n",
       "      dataset = make_dataset()\n",
       "      def custom_shard_func(element):\n",
       "        return np.int64(0)\n",
       "      dataset.save(\n",
       "          path=\"/path/to/data\", ..., shard_func=custom_shard_func)\n",
       "      ```\n",
       "\n",
       "      To enable checkpointing, pass in `checkpoint_args` to the `save` method\n",
       "      as follows:\n",
       "\n",
       "      ```python\n",
       "      dataset = tf.data.Dataset.range(100)\n",
       "      save_dir = \"...\"\n",
       "      checkpoint_prefix = \"...\"\n",
       "      step_counter = tf.Variable(0, trainable=False)\n",
       "      checkpoint_args = {\n",
       "        \"checkpoint_interval\": 50,\n",
       "        \"step_counter\": step_counter,\n",
       "        \"directory\": checkpoint_prefix,\n",
       "        \"max_to_keep\": 20,\n",
       "      }\n",
       "      dataset.save(dataset, save_dir, checkpoint_args=checkpoint_args)\n",
       "      ```\n",
       "\n",
       "      NOTE: The directory layout and file format used for saving the dataset is\n",
       "      considered an implementation detail and may change. For this reason,\n",
       "      datasets saved through `tf.data.Dataset.save` should only be consumed\n",
       "      through `tf.data.Dataset.load`, which is guaranteed to be\n",
       "      backwards compatible.\n",
       "\n",
       "    Args:\n",
       "     path: Required. A directory to use for saving the dataset.\n",
       "     compression: Optional. The algorithm to use to compress data when writing\n",
       "          it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.\n",
       "     shard_func: Optional. A function to control the mapping of dataset\n",
       "          elements to file shards. The function is expected to map elements of\n",
       "          the input dataset to int64 shard IDs. If present, the function will be\n",
       "          traced and executed as graph computation.\n",
       "     checkpoint_args: Optional args for checkpointing which will be passed into\n",
       "          the `tf.train.CheckpointManager`. If `checkpoint_args` are not\n",
       "          specified, then checkpointing will not be performed. The `save()`\n",
       "          implementation creates a `tf.train.Checkpoint` object internally, so\n",
       "          users should not set the `checkpoint` argument in `checkpoint_args`.\n",
       "\n",
       "    Returns:\n",
       "      An operation which when executed performs the save. When writing\n",
       "      checkpoints, returns None. The return value is useful in unit tests.\n",
       "\n",
       "    Raises:\n",
       "      ValueError if `checkpoint` is passed into `checkpoint_args`.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> save_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0msave_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshard_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Loads a previously saved dataset.\n",
       "\n",
       "    Example usage:\n",
       "\n",
       "    >>> import tempfile\n",
       "    >>> path = os.path.join(tempfile.gettempdir(), \"saved_data\")\n",
       "    >>> # Save a dataset\n",
       "    >>> dataset = tf.data.Dataset.range(2)\n",
       "    >>> tf.data.Dataset.save(dataset, path)\n",
       "    >>> new_dataset = tf.data.Dataset.load(path)\n",
       "    >>> for elem in new_dataset:\n",
       "    ...   print(elem)\n",
       "    tf.Tensor(0, shape=(), dtype=int64)\n",
       "    tf.Tensor(1, shape=(), dtype=int64)\n",
       "\n",
       "\n",
       "    If the default option of sharding the saved dataset was used, the element\n",
       "    order of the saved dataset will be preserved when loading it.\n",
       "\n",
       "    The `reader_func` argument can be used to specify a custom order in which\n",
       "    elements should be loaded from the individual shards. The `reader_func` is\n",
       "    expected to take a single argument -- a dataset of datasets, each containing\n",
       "    elements of one of the shards -- and return a dataset of elements. For\n",
       "    example, the order of shards can be shuffled when loading them as follows:\n",
       "\n",
       "    ```python\n",
       "    def custom_reader_func(datasets):\n",
       "      datasets = datasets.shuffle(NUM_SHARDS)\n",
       "      return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\n",
       "\n",
       "    dataset = tf.data.Dataset.load(\n",
       "        path=\"/path/to/data\", ..., reader_func=custom_reader_func)\n",
       "    ```\n",
       "\n",
       "    Args:\n",
       "      path: Required. A path pointing to a previously saved dataset.\n",
       "      element_spec: Optional. A nested structure of `tf.TypeSpec` objects\n",
       "        matching the structure of an element of the saved dataset and specifying\n",
       "        the type of individual element components. If not provided, the nested\n",
       "        structure of `tf.TypeSpec` saved with the saved dataset is used. Note\n",
       "        that this argument is required in graph mode.\n",
       "      compression: Optional. The algorithm to use to decompress the data when\n",
       "        reading it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.\n",
       "      reader_func: Optional. A function to control how to read data from shards.\n",
       "        If present, the function will be traced and executed as graph\n",
       "        computation.\n",
       "\n",
       "    Returns:\n",
       "      A `tf.data.Dataset` instance.\n",
       "\n",
       "    Raises:\n",
       "      FileNotFoundError: If `element_spec` is not specified and the saved nested\n",
       "        structure of `tf.TypeSpec` can not be located with the saved dataset.\n",
       "      ValueError: If `element_spec` is not specified and the method is executed\n",
       "        in graph mode.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> load_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mload_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0melement_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mreader_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreader_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Combines consecutive elements of this dataset into batches.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(8)\n",
       "    >>> dataset = dataset.batch(3)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(8)\n",
       "    >>> dataset = dataset.batch(3, drop_remainder=True)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [array([0, 1, 2]), array([3, 4, 5])]\n",
       "\n",
       "    The components of the resulting element will have an additional outer\n",
       "    dimension, which will be `batch_size` (or `N % batch_size` for the last\n",
       "    element if `batch_size` does not divide the number of input elements `N`\n",
       "    evenly and `drop_remainder` is `False`). If your program depends on the\n",
       "    batches having the same outer dimension, you should set the `drop_remainder`\n",
       "    argument to `True` to prevent the smaller batch from being produced.\n",
       "\n",
       "    Note: If your program requires data to have a statically known shape (e.g.,\n",
       "    when using XLA), you should use `drop_remainder=True`. Without\n",
       "    `drop_remainder=True` the shape of the output dataset will have an unknown\n",
       "    leading dimension due to the possibility of a smaller final batch.\n",
       "\n",
       "    Args:\n",
       "      batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
       "        consecutive elements of this dataset to combine in a single batch.\n",
       "      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
       "        whether the last batch should be dropped in the case it has fewer than\n",
       "        `batch_size` elements; the default behavior is not to drop the smaller\n",
       "        batch.\n",
       "      num_parallel_calls: (Optional.) A `tf.int64` scalar `tf.Tensor`,\n",
       "        representing the number of batches to compute asynchronously in\n",
       "        parallel.\n",
       "        If not specified, batches will be computed sequentially. If the value\n",
       "        `tf.data.AUTOTUNE` is used, then the number of parallel\n",
       "        calls is set dynamically based on available resources.\n",
       "      deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n",
       "        boolean is specified (`True` or `False`), it controls the order in which\n",
       "        the transformation produces elements. If set to `False`, the\n",
       "        transformation is allowed to yield elements out of order to trade\n",
       "        determinism for performance. If not specified, the\n",
       "        `tf.data.Options.deterministic` option (`True` by default) controls the\n",
       "        behavior.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> batch_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access,redefined-outer-name\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbatch_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mbatch_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                           \u001b[0mdeterministic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access,redefined-outer-name\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mpadded_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mpadded_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mpadding_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Combines consecutive elements of this dataset into padded batches.\n",
       "\n",
       "    This transformation combines multiple consecutive elements of the input\n",
       "    dataset into a single element.\n",
       "\n",
       "    Like `tf.data.Dataset.batch`, the components of the resulting element will\n",
       "    have an additional outer dimension, which will be `batch_size` (or\n",
       "    `N % batch_size` for the last element if `batch_size` does not divide the\n",
       "    number of input elements `N` evenly and `drop_remainder` is `False`). If\n",
       "    your program depends on the batches having the same outer dimension, you\n",
       "    should set the `drop_remainder` argument to `True` to prevent the smaller\n",
       "    batch from being produced.\n",
       "\n",
       "    Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\n",
       "    different shapes, and this transformation will pad each component to the\n",
       "    respective shape in `padded_shapes`. The `padded_shapes` argument\n",
       "    determines the resulting shape for each dimension of each component in an\n",
       "    output element:\n",
       "\n",
       "    * If the dimension is a constant, the component will be padded out to that\n",
       "      length in that dimension.\n",
       "    * If the dimension is unknown, the component will be padded out to the\n",
       "      maximum length of all elements in that dimension.\n",
       "\n",
       "    >>> A = (tf.data.Dataset\n",
       "    ...      .range(1, 5, output_type=tf.int32)\n",
       "    ...      .map(lambda x: tf.fill([x], x)))\n",
       "    >>> # Pad to the smallest per-batch size that fits all elements.\n",
       "    >>> B = A.padded_batch(2)\n",
       "    >>> for element in B.as_numpy_iterator():\n",
       "    ...   print(element)\n",
       "    [[1 0]\n",
       "     [2 2]]\n",
       "    [[3 3 3 0]\n",
       "     [4 4 4 4]]\n",
       "    >>> # Pad to a fixed size.\n",
       "    >>> C = A.padded_batch(2, padded_shapes=5)\n",
       "    >>> for element in C.as_numpy_iterator():\n",
       "    ...   print(element)\n",
       "    [[1 0 0 0 0]\n",
       "     [2 2 0 0 0]]\n",
       "    [[3 3 3 0 0]\n",
       "     [4 4 4 4 0]]\n",
       "    >>> # Pad with a custom value.\n",
       "    >>> D = A.padded_batch(2, padded_shapes=5, padding_values=-1)\n",
       "    >>> for element in D.as_numpy_iterator():\n",
       "    ...   print(element)\n",
       "    [[ 1 -1 -1 -1 -1]\n",
       "     [ 2  2 -1 -1 -1]]\n",
       "    [[ 3  3  3 -1 -1]\n",
       "     [ 4  4  4  4 -1]]\n",
       "    >>> # Components of nested elements can be padded independently.\n",
       "    >>> elements = [([1, 2, 3], [10]),\n",
       "    ...             ([4, 5], [11, 12])]\n",
       "    >>> dataset = tf.data.Dataset.from_generator(\n",
       "    ...     lambda: iter(elements), (tf.int32, tf.int32))\n",
       "    >>> # Pad the first component of the tuple to length 4, and the second\n",
       "    >>> # component to the smallest size that fits.\n",
       "    >>> dataset = dataset.padded_batch(2,\n",
       "    ...     padded_shapes=([4], [None]),\n",
       "    ...     padding_values=(-1, 100))\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n",
       "      array([[ 10, 100], [ 11,  12]], dtype=int32))]\n",
       "    >>> # Pad with a single value and multiple components.\n",
       "    >>> E = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\n",
       "    >>> for element in E.as_numpy_iterator():\n",
       "    ...   print(element)\n",
       "    (array([[ 1, -1],\n",
       "           [ 2,  2]], dtype=int32), array([[ 1, -1],\n",
       "           [ 2,  2]], dtype=int32))\n",
       "    (array([[ 3,  3,  3, -1],\n",
       "           [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n",
       "           [ 4,  4,  4,  4]], dtype=int32))\n",
       "\n",
       "    See also `tf.data.experimental.dense_to_sparse_batch`, which combines\n",
       "    elements that may have different shapes into a `tf.sparse.SparseTensor`.\n",
       "\n",
       "    Args:\n",
       "      batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
       "        consecutive elements of this dataset to combine in a single batch.\n",
       "      padded_shapes: (Optional.) A (nested) structure of `tf.TensorShape` or\n",
       "        `tf.int64` vector tensor-like objects representing the shape to which\n",
       "        the respective component of each input element should be padded prior\n",
       "        to batching. Any unknown dimensions will be padded to the maximum size\n",
       "        of that dimension in each batch. If unset, all dimensions of all\n",
       "        components are padded to the maximum size in the batch. `padded_shapes`\n",
       "        must be set if any component has an unknown rank.\n",
       "      padding_values: (Optional.) A (nested) structure of scalar-shaped\n",
       "        `tf.Tensor`, representing the padding values to use for the respective\n",
       "        components. None represents that the (nested) structure should be padded\n",
       "        with default values.  Defaults are `0` for numeric types and the empty\n",
       "        string for string types. The `padding_values` should have the same\n",
       "        (nested) structure as the input dataset. If `padding_values` is a single\n",
       "        element and the input dataset has multiple components, then the same\n",
       "        `padding_values` will be used to pad every component of the dataset.\n",
       "        If `padding_values` is a scalar, then its value will be broadcasted\n",
       "        to match the shape of each component.\n",
       "      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
       "        whether the last batch should be dropped in the case it has fewer than\n",
       "        `batch_size` elements; the default behavior is not to drop the smaller\n",
       "        batch.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "\n",
       "    Raises:\n",
       "      ValueError: If a component has an unknown rank, and the `padded_shapes`\n",
       "        argument is not set.\n",
       "      TypeError: If a component is of an unsupported type. The list of supported\n",
       "        types is documented in\n",
       "        https://www.tensorflow.org/guide/data#dataset_structure.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# padded_batch_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpadded_batch_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mpadded_batch_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_padded_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                         \u001b[0mpadding_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mragged_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mrow_splits_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Combines consecutive elements of this dataset into `tf.RaggedTensor`s.\n",
       "\n",
       "    Like `tf.data.Dataset.batch`, the components of the resulting element will\n",
       "    have an additional outer dimension, which will be `batch_size` (or\n",
       "    `N % batch_size` for the last element if `batch_size` does not divide the\n",
       "    number of input elements `N` evenly and `drop_remainder` is `False`). If\n",
       "    your program depends on the batches having the same outer dimension, you\n",
       "    should set the `drop_remainder` argument to `True` to prevent the smaller\n",
       "    batch from being produced.\n",
       "\n",
       "    Unlike `tf.data.Dataset.batch`, the input elements to be batched may have\n",
       "    different shapes:\n",
       "\n",
       "    *  If an input element is a `tf.Tensor` whose static `tf.TensorShape` is\n",
       "    fully defined, then it is batched as normal.\n",
       "    *  If an input element is a `tf.Tensor` whose static `tf.TensorShape`\n",
       "    contains one or more axes with unknown size (i.e., `shape[i]=None`), then\n",
       "    the output will contain a `tf.RaggedTensor` that is ragged up to any of such\n",
       "    dimensions.\n",
       "    *  If an input element is a `tf.RaggedTensor` or any other type, then it is\n",
       "    batched as normal.\n",
       "\n",
       "    Example:\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(6)\n",
       "    >>> dataset = dataset.map(lambda x: tf.range(x))\n",
       "    >>> dataset.element_spec.shape\n",
       "    TensorShape([None])\n",
       "    >>> dataset = dataset.ragged_batch(2)\n",
       "    >>> for batch in dataset:\n",
       "    ...   print(batch)\n",
       "    <tf.RaggedTensor [[], [0]]>\n",
       "    <tf.RaggedTensor [[0, 1], [0, 1, 2]]>\n",
       "    <tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>\n",
       "\n",
       "    Args:\n",
       "      batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
       "        consecutive elements of this dataset to combine in a single batch.\n",
       "      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
       "        whether the last batch should be dropped in the case it has fewer than\n",
       "        `batch_size` elements; the default behavior is not to drop the smaller\n",
       "        batch.\n",
       "      row_splits_dtype: The dtype that should be used for the `row_splits` of\n",
       "        any new ragged tensors.  Existing `tf.RaggedTensor` elements do not have\n",
       "        their row_splits dtype changed.\n",
       "      name: (Optional.) A string indicating a name for the `tf.data` operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# ragged_batch_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mragged_batch_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mragged_batch_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ragged_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                         \u001b[0mrow_splits_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0msparse_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Combines consecutive elements into `tf.sparse.SparseTensor`s.\n",
       "\n",
       "    Like `Dataset.padded_batch()`, this transformation combines multiple\n",
       "    consecutive elements of the dataset, which might have different\n",
       "    shapes, into a single element. The resulting element has three\n",
       "    components (`indices`, `values`, and `dense_shape`), which\n",
       "    comprise a `tf.sparse.SparseTensor` that represents the same data. The\n",
       "    `row_shape` represents the dense shape of each row in the\n",
       "    resulting `tf.sparse.SparseTensor`, to which the effective batch size is\n",
       "    prepended. For example:\n",
       "\n",
       "    ```python\n",
       "    # NOTE: The following examples use `{ ... }` to represent the\n",
       "    # contents of a dataset.\n",
       "    a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }\n",
       "\n",
       "    a.apply(tf.data.experimental.dense_to_sparse_batch(\n",
       "        batch_size=2, row_shape=[6])) ==\n",
       "    {\n",
       "        ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices\n",
       "         ['a', 'b', 'c', 'a', 'b'],                 # values\n",
       "         [2, 6]),                                   # dense_shape\n",
       "        ([[0, 0], [0, 1], [0, 2], [0, 3]],\n",
       "         ['a', 'b', 'c', 'd'],\n",
       "         [1, 6])\n",
       "    }\n",
       "    ```\n",
       "\n",
       "    Args:\n",
       "      batch_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
       "        consecutive elements of this dataset to combine in a single batch.\n",
       "      row_shape: A `tf.TensorShape` or `tf.int64` vector tensor-like object\n",
       "        representing the equivalent dense shape of a row in the resulting\n",
       "        `tf.sparse.SparseTensor`. Each element of this dataset must have the\n",
       "        same rank as `row_shape`, and must have size less than or equal to\n",
       "        `row_shape` in each dimension.\n",
       "      name: (Optional.) A string indicating a name for the `tf.data` operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# sparse_batch_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse_batch_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0msparse_batch_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Maps `map_func` across the elements of this dataset.\n",
       "\n",
       "    This transformation applies `map_func` to each element of this dataset, and\n",
       "    returns a new dataset containing the transformed elements, in the same\n",
       "    order as they appeared in the input. `map_func` can be used to change both\n",
       "    the values and the structure of a dataset's elements. Supported structure\n",
       "    constructs are documented\n",
       "    [here](https://www.tensorflow.org/guide/data#dataset_structure).\n",
       "\n",
       "    For example, `map` can be used for adding 1 to each element, or projecting a\n",
       "    subset of element components.\n",
       "\n",
       "    >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
       "    >>> dataset = dataset.map(lambda x: x + 1)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [2, 3, 4, 5, 6]\n",
       "\n",
       "    The input signature of `map_func` is determined by the structure of each\n",
       "    element in this dataset.\n",
       "\n",
       "    >>> dataset = Dataset.range(5)\n",
       "    >>> # `map_func` takes a single argument of type `tf.Tensor` with the same\n",
       "    >>> # shape and dtype.\n",
       "    >>> result = dataset.map(lambda x: x + 1)\n",
       "\n",
       "    >>> # Each element is a tuple containing two `tf.Tensor` objects.\n",
       "    >>> elements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\n",
       "    >>> dataset = tf.data.Dataset.from_generator(\n",
       "    ...     lambda: elements, (tf.int32, tf.string))\n",
       "    >>> # `map_func` takes two arguments of type `tf.Tensor`. This function\n",
       "    >>> # projects out just the first component.\n",
       "    >>> result = dataset.map(lambda x_int, y_str: x_int)\n",
       "    >>> list(result.as_numpy_iterator())\n",
       "    [1, 2, 3]\n",
       "\n",
       "    >>> # Each element is a dictionary mapping strings to `tf.Tensor` objects.\n",
       "    >>> elements =  ([{\"a\": 1, \"b\": \"foo\"},\n",
       "    ...               {\"a\": 2, \"b\": \"bar\"},\n",
       "    ...               {\"a\": 3, \"b\": \"baz\"}])\n",
       "    >>> dataset = tf.data.Dataset.from_generator(\n",
       "    ...     lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n",
       "    >>> # `map_func` takes a single argument of type `dict` with the same keys\n",
       "    >>> # as the elements.\n",
       "    >>> result = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n",
       "\n",
       "    The value or values returned by `map_func` determine the structure of each\n",
       "    element in the returned dataset.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(3)\n",
       "    >>> # `map_func` returns two `tf.Tensor` objects.\n",
       "    >>> def g(x):\n",
       "    ...   return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\n",
       "    >>> result = dataset.map(g)\n",
       "    >>> result.element_spec\n",
       "    (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), \\\n",
       "dtype=tf.string, name=None))\n",
       "    >>> # Python primitives, lists, and NumPy arrays are implicitly converted to\n",
       "    >>> # `tf.Tensor`.\n",
       "    >>> def h(x):\n",
       "    ...   return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\n",
       "    >>> result = dataset.map(h)\n",
       "    >>> result.element_spec\n",
       "    (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), \\\n",
       "dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, \\\n",
       "name=None))\n",
       "    >>> # `map_func` can return nested structures.\n",
       "    >>> def i(x):\n",
       "    ...   return (37.0, [42, 16]), \"foo\"\n",
       "    >>> result = dataset.map(i)\n",
       "    >>> result.element_spec\n",
       "    ((TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       "      TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n",
       "     TensorSpec(shape=(), dtype=tf.string, name=None))\n",
       "\n",
       "    `map_func` can accept as arguments and return any type of dataset element.\n",
       "\n",
       "    Note that irrespective of the context in which `map_func` is defined (eager\n",
       "    vs. graph), tf.data traces the function and executes it as a graph. To use\n",
       "    Python code inside of the function you have a few options:\n",
       "\n",
       "    1) Rely on AutoGraph to convert Python code into an equivalent graph\n",
       "    computation. The downside of this approach is that AutoGraph can convert\n",
       "    some but not all Python code.\n",
       "\n",
       "    2) Use `tf.py_function`, which allows you to write arbitrary Python code but\n",
       "    will generally result in worse performance than 1). For example:\n",
       "\n",
       "    >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
       "    >>> # transform a string tensor to upper case string using a Python function\n",
       "    >>> def upper_case_fn(t: tf.Tensor):\n",
       "    ...   return t.numpy().decode('utf-8').upper()\n",
       "    >>> d = d.map(lambda x: tf.py_function(func=upper_case_fn,\n",
       "    ...           inp=[x], Tout=tf.string))\n",
       "    >>> list(d.as_numpy_iterator())\n",
       "    [b'HELLO', b'WORLD']\n",
       "\n",
       "    3) Use `tf.numpy_function`, which also allows you to write arbitrary\n",
       "    Python code. Note that `tf.py_function` accepts `tf.Tensor` whereas\n",
       "    `tf.numpy_function` accepts numpy arrays and returns only numpy arrays.\n",
       "    For example:\n",
       "\n",
       "    >>> d = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n",
       "    >>> def upper_case_fn(t: np.ndarray):\n",
       "    ...   return t.decode('utf-8').upper()\n",
       "    >>> d = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n",
       "    ...           inp=[x], Tout=tf.string))\n",
       "    >>> list(d.as_numpy_iterator())\n",
       "    [b'HELLO', b'WORLD']\n",
       "\n",
       "    Note that the use of `tf.numpy_function` and `tf.py_function`\n",
       "    in general precludes the possibility of executing user-defined\n",
       "    transformations in parallel (because of Python GIL).\n",
       "\n",
       "    Performance can often be improved by setting `num_parallel_calls` so that\n",
       "    `map` will use multiple threads to process elements. If deterministic order\n",
       "    isn't required, it can also improve performance to set\n",
       "    `deterministic=False`.\n",
       "\n",
       "    >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
       "    >>> dataset = dataset.map(lambda x: x + 1,\n",
       "    ...     num_parallel_calls=tf.data.AUTOTUNE,\n",
       "    ...     deterministic=False)\n",
       "\n",
       "    The order of elements yielded by this transformation is deterministic if\n",
       "    `deterministic=True`. If `map_func` contains stateful operations and\n",
       "    `num_parallel_calls > 1`, the order in which that state is accessed is\n",
       "    undefined, so the values of output elements may not be deterministic\n",
       "    regardless of the `deterministic` flag value.\n",
       "\n",
       "    Args:\n",
       "      map_func: A function mapping a dataset element to another dataset element.\n",
       "      num_parallel_calls: (Optional.) A `tf.int64` scalar `tf.Tensor`,\n",
       "        representing the number elements to process asynchronously in parallel.\n",
       "        If not specified, elements will be processed sequentially. If the value\n",
       "        `tf.data.AUTOTUNE` is used, then the number of parallel\n",
       "        calls is set dynamically based on available CPU.\n",
       "      deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n",
       "        boolean is specified (`True` or `False`), it controls the order in which\n",
       "        the transformation produces elements. If set to `False`, the\n",
       "        transformation is allowed to yield elements out of order to trade\n",
       "        determinism for performance. If not specified, the\n",
       "        `tf.data.Options.deterministic` option (`True` by default) controls the\n",
       "        behavior.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Maps `map_func` across this dataset and flattens the result.\n",
       "\n",
       "    The type signature is:\n",
       "\n",
       "    ```\n",
       "    def flat_map(\n",
       "      self: Dataset[T],\n",
       "      map_func: Callable[[T], Dataset[S]]\n",
       "    ) -> Dataset[S]\n",
       "    ```\n",
       "\n",
       "    Use `flat_map` if you want to make sure that the order of your dataset\n",
       "    stays the same. For example, to flatten a dataset of batches into a\n",
       "    dataset of their elements:\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices(\n",
       "    ...     [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
       "    >>> dataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
       "\n",
       "    `tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n",
       "    `flat_map` produces the same output as\n",
       "    `tf.data.Dataset.interleave(cycle_length=1)`\n",
       "\n",
       "    Args:\n",
       "      map_func: A function mapping a dataset element to a dataset.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> flat_map_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mignore_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_warning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Drops elements that cause errors.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])\n",
       "    >>> dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, \"\"))\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    Traceback (most recent call last):\n",
       "    ...\n",
       "    InvalidArgumentError: ... Tensor had Inf values\n",
       "    >>> dataset = dataset.ignore_errors()\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [1.0, 0.5, 0.25]\n",
       "\n",
       "    Args:\n",
       "      log_warning: (Optional.) A bool indicating whether or not ignored errors\n",
       "        should be logged to stderr. Defaults to `False`.\n",
       "      name: (Optional.) A string indicating a name for the `tf.data` operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# ignore_errors_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mignore_errors_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mignore_errors_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ignore_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_warning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0minterleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mcycle_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mblock_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Maps `map_func` across this dataset, and interleaves the results.\n",
       "\n",
       "    The type signature is:\n",
       "\n",
       "    ```\n",
       "    def interleave(\n",
       "      self: Dataset[T],\n",
       "      map_func: Callable[[T], Dataset[S]]\n",
       "    ) -> Dataset[S]\n",
       "    ```\n",
       "\n",
       "    For example, you can use `Dataset.interleave()` to process many input files\n",
       "    concurrently:\n",
       "\n",
       "    >>> # Preprocess 4 files concurrently, and interleave blocks of 16 records\n",
       "    >>> # from each file.\n",
       "    >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
       "    ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
       "    >>> def parse_fn(filename):\n",
       "    ...   return tf.data.Dataset.range(10)\n",
       "    >>> dataset = dataset.interleave(lambda x:\n",
       "    ...     tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n",
       "    ...     cycle_length=4, block_length=16)\n",
       "\n",
       "    The `cycle_length` and `block_length` arguments control the order in which\n",
       "    elements are produced. `cycle_length` controls the number of input elements\n",
       "    that are processed concurrently. If you set `cycle_length` to 1, this\n",
       "    transformation will handle one input element at a time, and will produce\n",
       "    identical results to `tf.data.Dataset.flat_map`. In general,\n",
       "    this transformation will apply `map_func` to `cycle_length` input elements,\n",
       "    open iterators on the returned `Dataset` objects, and cycle through them\n",
       "    producing `block_length` consecutive elements from each iterator, and\n",
       "    consuming the next input element each time it reaches the end of an\n",
       "    iterator.\n",
       "\n",
       "    For example:\n",
       "\n",
       "    >>> dataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
       "    >>> # NOTE: New lines indicate \"block\" boundaries.\n",
       "    >>> dataset = dataset.interleave(\n",
       "    ...     lambda x: Dataset.from_tensors(x).repeat(6),\n",
       "    ...     cycle_length=2, block_length=4)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [1, 1, 1, 1,\n",
       "     2, 2, 2, 2,\n",
       "     1, 1,\n",
       "     2, 2,\n",
       "     3, 3, 3, 3,\n",
       "     4, 4, 4, 4,\n",
       "     3, 3,\n",
       "     4, 4,\n",
       "     5, 5, 5, 5,\n",
       "     5, 5]\n",
       "\n",
       "    Note: The order of elements yielded by this transformation is\n",
       "    deterministic, as long as `map_func` is a pure function and\n",
       "    `deterministic=True`. If `map_func` contains any stateful operations, the\n",
       "    order in which that state is accessed is undefined.\n",
       "\n",
       "    Performance can often be improved by setting `num_parallel_calls` so that\n",
       "    `interleave` will use multiple threads to fetch elements. If determinism\n",
       "    isn't required, it can also improve performance to set\n",
       "    `deterministic=False`.\n",
       "\n",
       "    >>> filenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n",
       "    ...              \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
       "    >>> dataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n",
       "    ...     cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n",
       "    ...     deterministic=False)\n",
       "\n",
       "    Args:\n",
       "      map_func: A function that takes a dataset element and returns a\n",
       "        `tf.data.Dataset`.\n",
       "      cycle_length: (Optional.) The number of input elements that will be\n",
       "        processed concurrently. If not set, the tf.data runtime decides what it\n",
       "        should be based on available CPU. If `num_parallel_calls` is set to\n",
       "        `tf.data.AUTOTUNE`, the `cycle_length` argument identifies\n",
       "        the maximum degree of parallelism.\n",
       "      block_length: (Optional.) The number of consecutive elements to produce\n",
       "        from each input element before cycling to another input element. If not\n",
       "        set, defaults to 1.\n",
       "      num_parallel_calls: (Optional.) If specified, the implementation creates a\n",
       "        threadpool, which is used to fetch inputs from cycle elements\n",
       "        asynchronously and in parallel. The default behavior is to fetch inputs\n",
       "        from cycle elements synchronously with no parallelism. If the value\n",
       "        `tf.data.AUTOTUNE` is used, then the number of parallel\n",
       "        calls is set dynamically based on available CPU.\n",
       "      deterministic: (Optional.) When `num_parallel_calls` is specified, if this\n",
       "        boolean is specified (`True` or `False`), it controls the order in which\n",
       "        the transformation produces elements. If set to `False`, the\n",
       "        transformation is allowed to yield elements out of order to trade\n",
       "        determinism for performance. If not specified, the\n",
       "        `tf.data.Options.deterministic` option (`True` by default) controls the\n",
       "        behavior.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops -> interleave_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterleave_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0minterleave_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycle_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                     \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Filters this dataset according to `predicate`.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
       "    >>> dataset = dataset.filter(lambda x: x < 3)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [1, 2]\n",
       "    >>> # `tf.math.equal(x, y)` is required for equality comparison\n",
       "    >>> def filter_fn(x):\n",
       "    ...   return tf.math.equal(x, 1)\n",
       "    >>> dataset = dataset.filter(filter_fn)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [1]\n",
       "\n",
       "    Args:\n",
       "      predicate: A function mapping a dataset element to a boolean.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> filter_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfilter_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mfilter_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformation_func\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Applies a transformation function to this dataset.\n",
       "\n",
       "    `apply` enables chaining of custom `Dataset` transformations, which are\n",
       "    represented as functions that take one `Dataset` argument and return a\n",
       "    transformed `Dataset`.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(100)\n",
       "    >>> def dataset_fn(ds):\n",
       "    ...   return ds.filter(lambda x: x < 5)\n",
       "    >>> dataset = dataset.apply(dataset_fn)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [0, 1, 2, 3, 4]\n",
       "\n",
       "    Args:\n",
       "      transformation_func: A function that takes one `Dataset` argument and\n",
       "        returns a `Dataset`.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformation_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_types\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34mf\"`transformation_func` must return a `tf.data.Dataset` object. \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34mf\"Got {type(dataset)}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_datasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns a dataset of \"windows\".\n",
       "\n",
       "    Each \"window\" is a dataset that contains a subset of elements of the\n",
       "    input dataset. These are finite datasets of size `size` (or possibly fewer\n",
       "    if there are not enough input elements to fill the window and\n",
       "    `drop_remainder` evaluates to `False`).\n",
       "\n",
       "    For example:\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(7).window(3)\n",
       "    >>> for window in dataset:\n",
       "    ...   print(window)\n",
       "    <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
       "    <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
       "    <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n",
       "\n",
       "    Since windows are datasets, they can be iterated over:\n",
       "\n",
       "    >>> for window in dataset:\n",
       "    ...   print(list(window.as_numpy_iterator()))\n",
       "    [0, 1, 2]\n",
       "    [3, 4, 5]\n",
       "    [6]\n",
       "\n",
       "    #### Shift\n",
       "\n",
       "    The `shift` argument determines the number of input elements to shift\n",
       "    between the start of each window. If windows and elements are both numbered\n",
       "    starting at 0, the first element in window `k` will be element `k * shift`\n",
       "    of the input dataset. In particular, the first element of the first window\n",
       "    will always be the first element of the input dataset.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(7).window(3, shift=1,\n",
       "    ...                                           drop_remainder=True)\n",
       "    >>> for window in dataset:\n",
       "    ...   print(list(window.as_numpy_iterator()))\n",
       "    [0, 1, 2]\n",
       "    [1, 2, 3]\n",
       "    [2, 3, 4]\n",
       "    [3, 4, 5]\n",
       "    [4, 5, 6]\n",
       "\n",
       "    #### Stride\n",
       "\n",
       "    The `stride` argument determines the stride between input elements within a\n",
       "    window.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(7).window(3, shift=1, stride=2,\n",
       "    ...                                           drop_remainder=True)\n",
       "    >>> for window in dataset:\n",
       "    ...   print(list(window.as_numpy_iterator()))\n",
       "    [0, 2, 4]\n",
       "    [1, 3, 5]\n",
       "    [2, 4, 6]\n",
       "\n",
       "    #### Nested elements\n",
       "\n",
       "    When the `window` transformation is applied to a dataset whos elements are\n",
       "    nested structures, it produces a dataset where the elements have the same\n",
       "    nested structure but each leaf is replaced by a window. In other words,\n",
       "    the nesting is applied outside of the windows as opposed inside of them.\n",
       "\n",
       "    The type signature is:\n",
       "\n",
       "    ```\n",
       "    def window(\n",
       "        self: Dataset[Nest[T]], ...\n",
       "    ) -> Dataset[Nest[Dataset[T]]]\n",
       "    ```\n",
       "\n",
       "    Applying `window` to a `Dataset` of tuples gives a tuple of windows:\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3, 4, 5],\n",
       "    ...                                               [6, 7, 8, 9, 10]))\n",
       "    >>> dataset = dataset.window(2)\n",
       "    >>> windows = next(iter(dataset))\n",
       "    >>> windows\n",
       "    (<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>,\n",
       "     <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>)\n",
       "\n",
       "    >>> def to_numpy(ds):\n",
       "    ...   return list(ds.as_numpy_iterator())\n",
       "    >>>\n",
       "    >>> for windows in dataset:\n",
       "    ...   print(to_numpy(windows[0]), to_numpy(windows[1]))\n",
       "    [1, 2] [6, 7]\n",
       "    [3, 4] [8, 9]\n",
       "    [5] [10]\n",
       "\n",
       "    Applying `window` to a `Dataset` of dictionaries gives a dictionary of\n",
       "    `Datasets`:\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3],\n",
       "    ...                                               'b': [4, 5, 6],\n",
       "    ...                                               'c': [7, 8, 9]})\n",
       "    >>> dataset = dataset.window(2)\n",
       "    >>> def to_numpy(ds):\n",
       "    ...   return list(ds.as_numpy_iterator())\n",
       "    >>>\n",
       "    >>> for windows in dataset:\n",
       "    ...   print(tf.nest.map_structure(to_numpy, windows))\n",
       "    {'a': [1, 2], 'b': [4, 5], 'c': [7, 8]}\n",
       "    {'a': [3], 'b': [6], 'c': [9]}\n",
       "\n",
       "    #### Flatten a dataset of windows\n",
       "\n",
       "    The `Dataset.flat_map` and `Dataset.interleave` methods can be used to\n",
       "    flatten a dataset of windows into a single dataset.\n",
       "\n",
       "    The argument to `flat_map` is a function that takes an element from the\n",
       "    dataset and returns a `Dataset`. `flat_map` chains together the resulting\n",
       "    datasets sequentially.\n",
       "\n",
       "    For example, to turn each window into a dense tensor:\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(7).window(3, shift=1,\n",
       "    ...                                           drop_remainder=True)\n",
       "    >>> batched = dataset.flat_map(lambda x:x.batch(3))\n",
       "    >>> for batch in batched:\n",
       "    ...   print(batch.numpy())\n",
       "    [0 1 2]\n",
       "    [1 2 3]\n",
       "    [2 3 4]\n",
       "    [3 4 5]\n",
       "    [4 5 6]\n",
       "\n",
       "    Args:\n",
       "      size: A `tf.int64` scalar `tf.Tensor`, representing the number of elements\n",
       "        of the input dataset to combine into a window. Must be positive.\n",
       "      shift: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
       "        number of input elements by which the window moves in each iteration.\n",
       "        Defaults to `size`. Must be positive.\n",
       "      stride: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the\n",
       "        stride of the input elements in the sliding window. Must be positive.\n",
       "        The default value of 1 means \"retain every input element\".\n",
       "      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
       "        whether the last windows should be dropped if their size is smaller than\n",
       "        `size`.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> window_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwindow_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mwindow_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Reduces the input dataset to a single element.\n",
       "\n",
       "    The transformation calls `reduce_func` successively on every element of\n",
       "    the input dataset until the dataset is exhausted, aggregating information in\n",
       "    its internal state. The `initial_state` argument is used for the initial\n",
       "    state and the final state is returned as the result.\n",
       "\n",
       "    >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n",
       "    5\n",
       "    >>> tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n",
       "    10\n",
       "\n",
       "    Args:\n",
       "      initial_state: An element representing the initial state of the\n",
       "        transformation.\n",
       "      reduce_func: A function that maps `(old_state, input_element)` to\n",
       "        `new_state`. It must take two arguments and return a new element\n",
       "        The structure of `new_state` must match the structure of\n",
       "        `initial_state`.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A dataset element corresponding to the final state of the transformation.\n",
       "\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"initial_state\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstate_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Iteratively rerun the reduce function until reaching a fixed point on\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# `state_structure`.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mneed_to_rerun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mwhile\u001b[0m \u001b[0mneed_to_rerun\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mwrapped_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructured_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructuredFunctionWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mreduce_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34m\"reduce()\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0minput_structure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0madd_to_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# Extract and validate class information from the returned values.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moutput_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_classes\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mstate_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mlambda\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_legacy_output_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mstate_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mfor\u001b[0m \u001b[0mnew_state_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_class\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_state_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[1;34mf\"The element classes for the new state must match the initial \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[1;34mf\"state. Expected {state_classes} but got \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[1;34mf\"{wrapped_func.output_classes}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# Extract and validate type information from the returned values.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moutput_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mstate_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mlambda\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_legacy_output_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mstate_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mfor\u001b[0m \u001b[0mnew_state_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mnew_state_type\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mstate_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[1;34mf\"The element types for the new state must match the initial \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[1;34mf\"state. Expected {state_types} but got \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[1;34mf\"{wrapped_func.output_types}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# Extract shape information from the returned values.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0moutput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mstate_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mlambda\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcomponent_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_legacy_output_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mstate_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mflat_state_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mflat_new_state_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mweakened_state_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0moriginal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mfor\u001b[0m \u001b[0moriginal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_state_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_new_state_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mneed_to_rerun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mfor\u001b[0m \u001b[0moriginal_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweakened_shape\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_state_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                                \u001b[0mweakened_state_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0moriginal_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mweakened_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0moriginal_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mweakened_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mneed_to_rerun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mbreak\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0mneed_to_rerun\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# TODO(b/110122868): Support a \"most specific compatible structure\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# method for combining structures, to avoid using legacy structures\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# here.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mstate_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_legacy_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mstate_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweakened_state_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mstate_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreduce_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreduce_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_debug_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_and_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mstate_structure\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mreduce_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0moutput_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flat_tensor_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flat_tensor_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mget_single_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns the single element of the `dataset`.\n",
       "\n",
       "    The function enables you to use a `tf.data.Dataset` in a stateless\n",
       "    \"tensor-in tensor-out\" expression, without creating an iterator.\n",
       "    This facilitates the ease of data transformation on tensors using the\n",
       "    optimized `tf.data.Dataset` abstraction on top of them.\n",
       "\n",
       "    For example, lets consider a `preprocessing_fn` which would take as an\n",
       "    input the raw features and returns the processed feature along with\n",
       "    it's label.\n",
       "\n",
       "    ```python\n",
       "    def preprocessing_fn(raw_feature):\n",
       "      # ... the raw_feature is preprocessed as per the use-case\n",
       "      return feature\n",
       "\n",
       "    raw_features = ...  # input batch of BATCH_SIZE elements.\n",
       "    dataset = (tf.data.Dataset.from_tensor_slices(raw_features)\n",
       "              .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n",
       "              .batch(BATCH_SIZE))\n",
       "\n",
       "    processed_features = dataset.get_single_element()\n",
       "    ```\n",
       "\n",
       "    In the above example, the `raw_features` tensor of length=BATCH_SIZE\n",
       "    was converted to a `tf.data.Dataset`. Next, each of the `raw_feature` was\n",
       "    mapped using the `preprocessing_fn` and the processed features were\n",
       "    grouped into a single batch. The final `dataset` contains only one element\n",
       "    which is a batch of all the processed features.\n",
       "\n",
       "    NOTE: The `dataset` should contain only one element.\n",
       "\n",
       "    Now, instead of creating an iterator for the `dataset` and retrieving the\n",
       "    batch of features, the `tf.data.get_single_element()` function is used\n",
       "    to skip the iterator creation process and directly output the batch of\n",
       "    features.\n",
       "\n",
       "    This can be particularly useful when your tensor transformations are\n",
       "    expressed as `tf.data.Dataset` operations, and you want to use those\n",
       "    transformations while serving your model.\n",
       "\n",
       "    #### Keras\n",
       "\n",
       "    ```python\n",
       "\n",
       "    model = ... # A pre-built or custom model\n",
       "\n",
       "    class PreprocessingModel(tf.keras.Model):\n",
       "      def __init__(self, model):\n",
       "        super().__init__(self)\n",
       "        self.model = model\n",
       "\n",
       "      @tf.function(input_signature=[...])\n",
       "      def serving_fn(self, data):\n",
       "        ds = tf.data.Dataset.from_tensor_slices(data)\n",
       "        ds = ds.map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n",
       "        ds = ds.batch(batch_size=BATCH_SIZE)\n",
       "        return tf.argmax(self.model(ds.get_single_element()), axis=-1)\n",
       "\n",
       "    preprocessing_model = PreprocessingModel(model)\n",
       "    your_exported_model_dir = ... # save the model to this path.\n",
       "    tf.saved_model.save(preprocessing_model, your_exported_model_dir,\n",
       "                  signatures={'serving_default': preprocessing_model.serving_fn}\n",
       "                  )\n",
       "    ```\n",
       "\n",
       "    Args:\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A nested structure of `tf.Tensor` objects, corresponding to the single\n",
       "      element of `dataset`.\n",
       "\n",
       "    Raises:\n",
       "      InvalidArgumentError: (at runtime) if `dataset` does not contain exactly\n",
       "        one element.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_and_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_to_single_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0munbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Splits elements of a dataset into multiple elements.\n",
       "\n",
       "    For example, if elements of the dataset are shaped `[B, a0, a1, ...]`,\n",
       "    where `B` may vary for each input element, then for each element in the\n",
       "    dataset, the unbatched dataset will contain `B` consecutive elements\n",
       "    of shape `[a0, a1, ...]`.\n",
       "\n",
       "    >>> elements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\n",
       "    >>> dataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\n",
       "    >>> dataset = dataset.unbatch()\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [1, 2, 3, 1, 2, 1, 2, 3, 4]\n",
       "\n",
       "    Note: `unbatch` requires a data copy to slice up the batched tensor into\n",
       "    smaller, unbatched tensors. When optimizing performance, try to avoid\n",
       "    unnecessary usage of `unbatch`.\n",
       "\n",
       "    Args:\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops -> unbatch_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munbatch_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0munbatch_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mwith_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns a new `tf.data.Dataset` with the given options set.\n",
       "\n",
       "    The options are \"global\" in the sense they apply to the entire dataset.\n",
       "    If options are set multiple times, they are merged as long as different\n",
       "    options do not use different non-default values.\n",
       "\n",
       "    >>> ds = tf.data.Dataset.range(5)\n",
       "    >>> ds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n",
       "    ...                    cycle_length=3,\n",
       "    ...                    num_parallel_calls=3)\n",
       "    >>> options = tf.data.Options()\n",
       "    >>> # This will make the interleave order non-deterministic.\n",
       "    >>> options.deterministic = False\n",
       "    >>> ds = ds.with_options(options)\n",
       "\n",
       "    Args:\n",
       "      options: A `tf.data.Options` that identifies the options the use.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "\n",
       "    Raises:\n",
       "      ValueError: when an option is set more than once to a non-default value\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0m_OptionsDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mcardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Returns the cardinality of the dataset, if known.\n",
       "\n",
       "    `cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\n",
       "    contains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\n",
       "    the analysis fails to determine the number of elements in the dataset\n",
       "    (e.g. when the dataset source is a file).\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(42)\n",
       "    >>> print(dataset.cardinality().numpy())\n",
       "    42\n",
       "    >>> dataset = dataset.repeat()\n",
       "    >>> cardinality = dataset.cardinality()\n",
       "    >>> print((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\n",
       "    True\n",
       "    >>> dataset = dataset.filter(lambda x: True)\n",
       "    >>> cardinality = dataset.cardinality()\n",
       "    >>> print((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\n",
       "    True\n",
       "\n",
       "    Returns:\n",
       "      A scalar `tf.int64` `Tensor` representing the cardinality of the dataset.\n",
       "      If the cardinality is infinite or unknown, `cardinality` returns the\n",
       "      named constants `tf.data.INFINITE_CARDINALITY` and\n",
       "      `tf.data.UNKNOWN_CARDINALITY` respectively.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_cardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mgroup_by_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mkey_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mreduce_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mwindow_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mwindow_size_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Groups windows of elements by key and reduces them.\n",
       "\n",
       "    This transformation maps each consecutive element in a dataset to a key\n",
       "    using `key_func` and groups the elements by key. It then applies\n",
       "    `reduce_func` to at most `window_size_func(key)` elements matching the same\n",
       "    key. All except the final window for each key will contain\n",
       "    `window_size_func(key)` elements; the final window may be smaller.\n",
       "\n",
       "    You may provide either a constant `window_size` or a window size determined\n",
       "    by the key through `window_size_func`.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(10)\n",
       "    >>> window_size = 5\n",
       "    >>> key_func = lambda x: x%2\n",
       "    >>> reduce_func = lambda key, dataset: dataset.batch(window_size)\n",
       "    >>> dataset = dataset.group_by_window(\n",
       "    ...           key_func=key_func,\n",
       "    ...           reduce_func=reduce_func,\n",
       "    ...           window_size=window_size)\n",
       "    >>> for elem in dataset.as_numpy_iterator():\n",
       "    ...   print(elem)\n",
       "    [0 2 4 6 8]\n",
       "    [1 3 5 7 9]\n",
       "\n",
       "    Args:\n",
       "      key_func: A function mapping a nested structure of tensors (having shapes\n",
       "        and types defined by `self.output_shapes` and `self.output_types`) to a\n",
       "        scalar `tf.int64` tensor.\n",
       "      reduce_func: A function mapping a key and a dataset of up to `window_size`\n",
       "        consecutive elements matching that key to another dataset.\n",
       "      window_size: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
       "        consecutive elements matching the same key to combine in a single batch,\n",
       "        which will be passed to `reduce_func`. Mutually exclusive with\n",
       "        `window_size_func`.\n",
       "      window_size_func: A function mapping a key to a `tf.int64` scalar\n",
       "        `tf.Tensor`, representing the number of consecutive elements matching\n",
       "        the same key to combine in a single batch, which will be passed to\n",
       "        `reduce_func`. Mutually exclusive with `window_size`.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "\n",
       "    Raises:\n",
       "      ValueError: if neither or both of {`window_size`, `window_size_func`} are\n",
       "        passed.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops -> group_by_window_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgroup_by_window_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mgroup_by_window_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_group_by_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mbucket_by_sequence_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0melement_length_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mbucket_boundaries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mbucket_batch_sizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mpadded_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mpadding_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mpad_to_bucket_boundary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mno_padding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"A transformation that buckets elements in a `Dataset` by length.\n",
       "\n",
       "    Elements of the `Dataset` are grouped together by length and then are padded\n",
       "    and batched.\n",
       "\n",
       "    This is useful for sequence tasks in which the elements have variable\n",
       "    length. Grouping together elements that have similar lengths reduces the\n",
       "    total fraction of padding in a batch which increases training step\n",
       "    efficiency.\n",
       "\n",
       "    Below is an example to bucketize the input data to the 3 buckets\n",
       "    \"[0, 3), [3, 5), [5, inf)\" based on sequence length, with batch size 2.\n",
       "\n",
       "    >>> elements = [\n",
       "    ...   [0], [1, 2, 3, 4], [5, 6, 7],\n",
       "    ...   [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]\n",
       "    >>> dataset = tf.data.Dataset.from_generator(\n",
       "    ...     lambda: elements, tf.int64, output_shapes=[None])\n",
       "    >>> dataset = dataset.bucket_by_sequence_length(\n",
       "    ...         element_length_func=lambda elem: tf.shape(elem)[0],\n",
       "    ...         bucket_boundaries=[3, 5],\n",
       "    ...         bucket_batch_sizes=[2, 2, 2])\n",
       "    >>> for elem in dataset.as_numpy_iterator():\n",
       "    ...   print(elem)\n",
       "    [[1 2 3 4]\n",
       "    [5 6 7 0]]\n",
       "    [[ 7  8  9 10 11  0]\n",
       "    [13 14 15 16 19 20]]\n",
       "    [[ 0  0]\n",
       "    [21 22]]\n",
       "\n",
       "    Args:\n",
       "      element_length_func: function from element in `Dataset` to `tf.int32`,\n",
       "        determines the length of the element, which will determine the bucket it\n",
       "        goes into.\n",
       "      bucket_boundaries: `list<int>`, upper length boundaries of the buckets.\n",
       "      bucket_batch_sizes: `list<int>`, batch size per bucket. Length should be\n",
       "        `len(bucket_boundaries) + 1`.\n",
       "      padded_shapes: Nested structure of `tf.TensorShape` to pass to\n",
       "        `tf.data.Dataset.padded_batch`. If not provided, will use\n",
       "        `dataset.output_shapes`, which will result in variable length dimensions\n",
       "        being padded out to the maximum length in each batch.\n",
       "      padding_values: Values to pad with, passed to\n",
       "        `tf.data.Dataset.padded_batch`. Defaults to padding with 0.\n",
       "      pad_to_bucket_boundary: bool, if `False`, will pad dimensions with unknown\n",
       "        size to maximum length in batch. If `True`, will pad dimensions with\n",
       "        unknown size to bucket boundary minus 1 (i.e., the maximum length in\n",
       "        each bucket), and caller must ensure that the source `Dataset` does not\n",
       "        contain any elements with length longer than `max(bucket_boundaries)`.\n",
       "      no_padding: `bool`, indicates whether to pad the batch features (features\n",
       "        need to be either of type `tf.sparse.SparseTensor` or of same shape).\n",
       "      drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing\n",
       "        whether the last batch should be dropped in the case it has fewer than\n",
       "        `batch_size` elements; the default behavior is not to drop the smaller\n",
       "        batch.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "\n",
       "    Raises:\n",
       "      ValueError: if `len(bucket_batch_sizes) != len(bucket_boundaries) + 1`.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_batch_sizes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_boundaries\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34mf\"`len(bucket_batch_sizes)` must equal `len(bucket_boundaries) + 1` \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34mf\"but `len(bucket_batch_sizes)={len(bucket_batch_sizes)}` and \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;34mf\"`len(bucket_boundaries)={len(bucket_boundaries)}`.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mbatch_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_batch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0melement_to_bucket_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;34m\"\"\"Return int64 id of the length bucket for this element.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mseq_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melement_length_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mboundaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mbuckets_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mboundaries\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mbuckets_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboundaries\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mconditions_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mless_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuckets_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mless\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuckets_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mbucket_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_min\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconditions_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mbucket_id\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mwindow_size_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;31m# The window size is set to the batch size for this bucket\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mwindow_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mmake_padded_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnone_filler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mpadded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mfor\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mnone_filler\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mpadded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mbatching_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrouped_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;34m\"\"\"Batch elements in dataset.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindow_size_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0mno_padding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mgrouped_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_remainder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mnone_filler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0mpad_to_bucket_boundary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"When pad_to_bucket_boundary=True, elements must have \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                   \u001b[1;34m\"length < max(bucket_boundaries).\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_less\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mbucket_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbucket_batch_sizes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mboundaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[0mbucket_boundaries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mbucket_boundary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboundaries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mnone_filler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbucket_boundary\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_legacy_output_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrouped_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_padded_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mpadded_shapes\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnone_filler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnone_filler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mgrouped_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadded_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mshapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mpadding_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mdrop_remainder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_remainder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup_by_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mkey_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0melement_to_bucket_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mreduce_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatching_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mwindow_size_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow_size_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrerandomize_each_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a `Dataset` of pseudorandom values.\n",
       "\n",
       "    The dataset generates a sequence of uniformly distributed integer values.\n",
       "\n",
       "    `rerandomize_each_iteration` controls whether the sequence of random number\n",
       "    generated should be re-randomized for each epoch. The default value is False\n",
       "    where the dataset generates the same sequence of random numbers for each\n",
       "    epoch.\n",
       "\n",
       "    >>> ds1 = tf.data.Dataset.random(seed=4).take(10)\n",
       "    >>> ds2 = tf.data.Dataset.random(seed=4).take(10)\n",
       "    >>> print(list(ds1.as_numpy_iterator())==list(ds2.as_numpy_iterator()))\n",
       "    True\n",
       "\n",
       "    >>> ds3 = tf.data.Dataset.random(seed=4).take(10)\n",
       "    >>> ds3_first_epoch = list(ds3.as_numpy_iterator())\n",
       "    >>> ds3_second_epoch = list(ds3.as_numpy_iterator())\n",
       "    >>> print(ds3_first_epoch == ds3_second_epoch)\n",
       "    True\n",
       "\n",
       "    >>> ds4 = tf.data.Dataset.random(\n",
       "    ...     seed=4, rerandomize_each_iteration=True).take(10)\n",
       "    >>> ds4_first_epoch = list(ds4.as_numpy_iterator())\n",
       "    >>> ds4_second_epoch = list(ds4.as_numpy_iterator())\n",
       "    >>> print(ds4_first_epoch == ds4_second_epoch)\n",
       "    False\n",
       "\n",
       "    Args:\n",
       "      seed: (Optional) If specified, the dataset produces a deterministic\n",
       "        sequence of values.\n",
       "      rerandomize_each_iteration: (Optional) If set to False, the dataset\n",
       "      generates the same sequence of random numbers for each epoch. If set to\n",
       "      True, it generates a different deterministic sequence of random numbers\n",
       "      for each epoch. It is defaulted to False if left unspecified.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      Dataset: A `Dataset`.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops -> random_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mrandom_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_random\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mrerandomize_each_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrerandomize_each_iteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0msnapshot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"AUTO\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mreader_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mshard_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"API to persist the output of the input dataset.\n",
       "\n",
       "    The snapshot API allows users to transparently persist the output of their\n",
       "    preprocessing pipeline to disk, and materialize the pre-processed data on a\n",
       "    different training run.\n",
       "\n",
       "    This API enables repeated preprocessing steps to be consolidated, and allows\n",
       "    re-use of already processed data, trading off disk storage and network\n",
       "    bandwidth for freeing up more valuable CPU resources and accelerator compute\n",
       "    time.\n",
       "\n",
       "    https://github.com/tensorflow/community/blob/master/rfcs/20200107-tf-data-snapshot.md\n",
       "    has detailed design documentation of this feature.\n",
       "\n",
       "    Users can specify various options to control the behavior of snapshot,\n",
       "    including how snapshots are read from and written to by passing in\n",
       "    user-defined functions to the `reader_func` and `shard_func` parameters.\n",
       "\n",
       "    `shard_func` is a user specified function that maps input elements to\n",
       "    snapshot shards.\n",
       "\n",
       "    Users may want to specify this function to control how snapshot files should\n",
       "    be written to disk. Below is an example of how a potential `shard_func`\n",
       "    could be written.\n",
       "\n",
       "    ```python\n",
       "    dataset = ...\n",
       "    dataset = dataset.enumerate()\n",
       "    dataset = dataset.snapshot(\"/path/to/snapshot/dir\",\n",
       "        shard_func=lambda x, y: x % NUM_SHARDS, ...)\n",
       "    dataset = dataset.map(lambda x, y: y)\n",
       "    ```\n",
       "\n",
       "    `reader_func` is a user specified function that accepts a single argument:\n",
       "    (1) a Dataset of Datasets, each representing a \"split\" of elements of the\n",
       "    original dataset. The cardinality of the input dataset matches the\n",
       "    number of the shards specified in the `shard_func` (see above). The function\n",
       "    should return a Dataset of elements of the original dataset.\n",
       "\n",
       "    Users may want specify this function to control how snapshot files should be\n",
       "    read from disk, including the amount of shuffling and parallelism.\n",
       "\n",
       "    Here is an example of a standard reader function a user can define. This\n",
       "    function enables both dataset shuffling and parallel reading of datasets:\n",
       "\n",
       "    ```python\n",
       "    def user_reader_func(datasets):\n",
       "      # shuffle the datasets splits\n",
       "      datasets = datasets.shuffle(NUM_CORES)\n",
       "      # read datasets in parallel and interleave their elements\n",
       "      return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\n",
       "\n",
       "    dataset = dataset.snapshot(\"/path/to/snapshot/dir\",\n",
       "        reader_func=user_reader_func)\n",
       "    ```\n",
       "\n",
       "    By default, snapshot parallelizes reads by the number of cores available on\n",
       "    the system, but will not attempt to shuffle the data.\n",
       "\n",
       "    Args:\n",
       "      path: Required. A directory to use for storing / loading the snapshot to /\n",
       "        from.\n",
       "      compression: Optional. The type of compression to apply to the snapshot\n",
       "        written to disk. Supported options are `GZIP`, `SNAPPY`, `AUTO` or None.\n",
       "        Defaults to `AUTO`, which attempts to pick an appropriate compression\n",
       "        algorithm for the dataset.\n",
       "      reader_func: Optional. A function to control how to read data from\n",
       "        snapshot shards.\n",
       "      shard_func: Optional. A function to control how to shard data when writing\n",
       "        a snapshot.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops -> snapshot_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msnapshot_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0msnapshot_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_snapshot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshard_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mscan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscan_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"A transformation that scans a function across an input dataset.\n",
       "\n",
       "    This transformation is a stateful relative of `tf.data.Dataset.map`.\n",
       "    In addition to mapping `scan_func` across the elements of the input dataset,\n",
       "    `scan()` accumulates one or more state tensors, whose initial values are\n",
       "    `initial_state`.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(10)\n",
       "    >>> initial_state = tf.constant(0, dtype=tf.int64)\n",
       "    >>> scan_func = lambda state, i: (state + i, state + i)\n",
       "    >>> dataset = dataset.scan(initial_state=initial_state, scan_func=scan_func)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [0, 1, 3, 6, 10, 15, 21, 28, 36, 45]\n",
       "\n",
       "    Args:\n",
       "      initial_state: A nested structure of tensors, representing the initial\n",
       "        state of the accumulator.\n",
       "      scan_func: A function that maps `(old_state, input_element)` to\n",
       "        `(new_state, output_element)`. It must take two arguments and return a\n",
       "        pair of nested structures of tensors. The `new_state` must match the\n",
       "        structure of `initial_state`.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# scan_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscan_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mscan_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscan_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mtake_while\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"A transformation that stops dataset iteration based on a `predicate`.\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.range(10)\n",
       "    >>> dataset = dataset.take_while(lambda x: x < 5)\n",
       "    >>> list(dataset.as_numpy_iterator())\n",
       "    [0, 1, 2, 3, 4]\n",
       "\n",
       "    Args:\n",
       "      predicate: A function that maps a nested structure of tensors (having\n",
       "        shapes and types defined by `self.output_shapes` and\n",
       "        `self.output_types`) to a scalar `tf.bool` tensor.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops -> take_while_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtake_while_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mtake_while_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_while\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"A transformation that discards duplicate elements of a `Dataset`.\n",
       "\n",
       "    Use this transformation to produce a dataset that contains one instance of\n",
       "    each unique element in the input. For example:\n",
       "\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices([1, 37, 2, 37, 2, 1])\n",
       "    >>> dataset = dataset.unique()\n",
       "    >>> sorted(list(dataset.as_numpy_iterator()))\n",
       "    [1, 2, 37]\n",
       "\n",
       "    Note: This transformation only supports datasets which fit into memory\n",
       "    and have elements of either `tf.int32`, `tf.int64` or `tf.string` type.\n",
       "\n",
       "    Args:\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency (dataset_ops -> unique_op ->\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munique_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0munique_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mrejection_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Resamples elements to reach a target distribution.\n",
       "\n",
       "    Note: This implementation can reject **or repeat** elements in order to\n",
       "    reach the `target_dist`. So, in some cases, the output `Dataset` may be\n",
       "    larger than the input `Dataset`.\n",
       "\n",
       "    >>> initial_dist = [0.6, 0.4]\n",
       "    >>> n = 1000\n",
       "    >>> elems = np.random.choice(len(initial_dist), size=n, p=initial_dist)\n",
       "    >>> dataset = tf.data.Dataset.from_tensor_slices(elems)\n",
       "    >>> zero, one = np.bincount(list(dataset.as_numpy_iterator())) / n\n",
       "\n",
       "    Following from `initial_dist`, `zero` is ~0.6 and `one` is ~0.4.\n",
       "\n",
       "    >>> target_dist = [0.5, 0.5]\n",
       "    >>> dataset = dataset.rejection_resample(\n",
       "    ...    class_func=lambda x: x,\n",
       "    ...    target_dist=target_dist,\n",
       "    ...    initial_dist=initial_dist)\n",
       "    >>> dataset = dataset.map(lambda class_func_result, data: data)\n",
       "    >>> zero, one = np.bincount(list(dataset.as_numpy_iterator())) / n\n",
       "\n",
       "    Following from `target_dist`, `zero` is ~0.5 and `one` is ~0.5.\n",
       "\n",
       "    Args:\n",
       "      class_func: A function mapping an element of the input dataset to a scalar\n",
       "        `tf.int32` tensor. Values should be in `[0, num_classes)`.\n",
       "      target_dist: A floating point type tensor, shaped `[num_classes]`.\n",
       "      initial_dist: (Optional.)  A floating point type tensor, shaped\n",
       "        `[num_classes]`.  If not provided, the true class distribution is\n",
       "        estimated live in a streaming fashion.\n",
       "      seed: (Optional.) Python integer seed for the resampler.\n",
       "      name: (Optional.) A name for the tf.data operation.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# TODO(b/245793127): Consider switching back to the 'v1' implementation.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtarget_dist_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"target_dist\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtarget_dist_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_dist_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Get initial distribution.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0minitial_dist\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0minitial_dist_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"initial_dist\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0minitial_dist_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_dist_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0macceptance_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_of_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0m_calculate_acceptance_probs_with_mixing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_dist_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                                                  \u001b[0mtarget_dist_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0minitial_dist_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0minitial_dist_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0macceptance_dist_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0macceptance_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mprob_of_original_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mprob_of_original\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0minitial_dist_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_estimate_initial_dist_ds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mtarget_dist_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0macceptance_and_original_prob_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_dist_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mlambda\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_calculate_acceptance_probs_with_mixing\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[1;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_dist_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0macceptance_dist_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macceptance_and_original_prob_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mlambda\u001b[0m \u001b[0maccept_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maccept_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mprob_of_original_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macceptance_and_original_prob_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_original\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprob_original\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfiltered_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_filter_ds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macceptance_dist_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_dist_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                             \u001b[0mclass_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Prefetch filtered dataset for speed.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfiltered_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mprob_original_static\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_prob_original_static\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0minitial_dist_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_dist_t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minitial_dist\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0madd_class_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mclass_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mclass_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mprob_original_static\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_class_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32melif\u001b[0m \u001b[0mprob_original_static\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mfiltered_ds\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_from_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_class_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_ds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprob_of_original_ds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mstop_on_empty_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0msample_from_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mstop_on_empty_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mrerandomize_each_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Samples elements at random from the datasets in `datasets`.\n",
       "\n",
       "    Creates a dataset by interleaving elements of `datasets` with `weight[i]`\n",
       "    probability of picking an element from dataset `i`. Sampling is done without\n",
       "    replacement. For example, suppose we have 2 datasets:\n",
       "\n",
       "    ```python\n",
       "    dataset1 = tf.data.Dataset.range(0, 3)\n",
       "    dataset2 = tf.data.Dataset.range(100, 103)\n",
       "    ```\n",
       "\n",
       "    Suppose that we sample from these 2 datasets with the following weights:\n",
       "\n",
       "    ```python\n",
       "    sample_dataset = tf.data.Dataset.sample_from_datasets(\n",
       "        [dataset1, dataset2], weights=[0.5, 0.5])\n",
       "    ```\n",
       "\n",
       "    One possible outcome of elements in sample_dataset is:\n",
       "\n",
       "    ```\n",
       "    print(list(sample_dataset.as_numpy_iterator()))\n",
       "    # [100, 0, 1, 101, 2, 102]\n",
       "    ```\n",
       "\n",
       "    Args:\n",
       "      datasets: A non-empty list of `tf.data.Dataset` objects with compatible\n",
       "        structure.\n",
       "      weights: (Optional.) A list or Tensor of `len(datasets)` floating-point\n",
       "        values where `weights[i]` represents the probability to sample from\n",
       "        `datasets[i]`, or a `tf.data.Dataset` object where each element is such\n",
       "        a list. Defaults to a uniform distribution across `datasets`.\n",
       "      seed: (Optional.) A `tf.int64` scalar `tf.Tensor`, representing the random\n",
       "        seed that will be used to create the distribution. See\n",
       "        `tf.random.set_seed` for behavior.\n",
       "      stop_on_empty_dataset: If `True`, sampling stops if it encounters an empty\n",
       "        dataset. If `False`, it continues sampling and skips any empty datasets.\n",
       "        It is recommended to set it to `True`. Otherwise, the distribution of\n",
       "        samples starts off as the user intends, but may change as input datasets\n",
       "        become empty. This can be difficult to detect since the dataset starts\n",
       "        off looking correct. Default to `False` for backward compatibility.\n",
       "      rerandomize_each_iteration: An optional `bool`. The boolean argument\n",
       "      controls whether the sequence of random numbers used to determine which\n",
       "      dataset to sample from will be rerandomized each epoch. That is, it\n",
       "      determinies whether datasets will be sampled in the same order across\n",
       "      different epochs (the default behavior) or not.\n",
       "\n",
       "    Returns:\n",
       "      A dataset that interleaves elements from `datasets` at random, according\n",
       "      to `weights` if provided, otherwise with uniform probability.\n",
       "\n",
       "    Raises:\n",
       "      TypeError: If the `datasets` or `weights` arguments have the wrong type.\n",
       "      ValueError:\n",
       "        - If `datasets` is empty, or\n",
       "        - If `weights` is specified and does not match the length of `datasets`.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# (dataset_ops -> sample_from_datasets_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msample_from_datasets_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0msample_from_datasets_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sample_from_datasets\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mstop_on_empty_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mrerandomize_each_iteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mdef\u001b[0m \u001b[0mchoose_from_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_on_empty_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DatasetV2\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Creates a dataset that deterministically chooses elements from `datasets`.\n",
       "\n",
       "    For example, given the following datasets:\n",
       "\n",
       "    ```python\n",
       "    datasets = [tf.data.Dataset.from_tensors(\"foo\").repeat(),\n",
       "                tf.data.Dataset.from_tensors(\"bar\").repeat(),\n",
       "                tf.data.Dataset.from_tensors(\"baz\").repeat()]\n",
       "\n",
       "    # Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`.\n",
       "    choice_dataset = tf.data.Dataset.range(3).repeat(3)\n",
       "\n",
       "    result = tf.data.Dataset.choose_from_datasets(datasets, choice_dataset)\n",
       "    ```\n",
       "\n",
       "    The elements of `result` will be:\n",
       "\n",
       "    ```\n",
       "    \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\"\n",
       "    ```\n",
       "\n",
       "    Args:\n",
       "      datasets: A non-empty list of `tf.data.Dataset` objects with compatible\n",
       "        structure.\n",
       "      choice_dataset: A `tf.data.Dataset` of scalar `tf.int64` tensors between\n",
       "        `0` and `len(datasets) - 1`.\n",
       "      stop_on_empty_dataset: If `True`, selection stops if it encounters an\n",
       "        empty dataset. If `False`, it skips empty datasets. It is recommended to\n",
       "        set it to `True`. Otherwise, the selected elements start off as the user\n",
       "        intends, but may change as input datasets become empty. This can be\n",
       "        difficult to detect since the dataset starts off looking correct.\n",
       "        Defaults to `True`.\n",
       "\n",
       "    Returns:\n",
       "      A new `Dataset` with the transformation applied as described above.\n",
       "\n",
       "    Raises:\n",
       "      TypeError: If `datasets` or `choice_dataset` has the wrong type.\n",
       "      ValueError: If `datasets` is empty.\n",
       "    \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# Loaded lazily due to a circular dependency\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# (dataset_ops -> choose_from_datasets_op -> dataset_ops).\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchoose_from_datasets_op\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mchoose_from_datasets_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_choose_from_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_on_empty_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     DatasetV1, DatasetSource, UnaryDataset, _VariantDataset, TFRecordDatasetV2, _PerDeviceGenerator, _ReincarnatedPerDeviceGenerator"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.data.Dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents a potentially large set of elements.\n",
    "\n",
    "anchor = tf.data.Dataset.list_files(ANC_PATH+'\\*.jpg').take(300)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH+'\\*.jpg').take(300)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'\\*.jpg').take(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'data\\\\anchor\\\\334093f9-209f-11ef-94fe-f21805e91111.jpg'\n"
     ]
    }
   ],
   "source": [
    "# Now we have a DATA PIPELINE \n",
    "# A way to iterate over all the data points in a dir\n",
    "\n",
    "dir_test = anchor.as_numpy_iterator()\n",
    "\n",
    "print(dir_test.next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Preprocessing - Scale and Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    # Load in the image \n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing steps - resizing the image to be 100 x 100 x 3 <--- size of 100 x 100 and ALL 3 channel\n",
    "    img = tf.image.resize(img, (100,100))\n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "    \n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- def pre_process()\n",
    "- dataset.map( pre_process )\n",
    "> applies pre_process fn to each data_label of dataset\n",
    "\n",
    "\n",
    "- [ ] tf.io.decode_jpeg() docmentation is interesting ; there is a way to downscale WHILE DECODING , but only  by 2,4,8 ; Try Downscaling by 255 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_jpeg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcontents\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m<\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[1;34m'tensorflow.security.fuzzing.py.annotation_types.String'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mchannels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mratio\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfancy_upscaling\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0macceptable_fraction\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdct_method\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m<\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[1;34m'tensorflow.security.fuzzing.py.annotation_types.UInt8'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "\u001b[1;32mdef\u001b[0m \u001b[0mdecode_jpeg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_atypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mString\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfancy_upscaling\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macceptable_fraction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdct_method\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_atypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUInt8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;34mr\"\"\"Decode a JPEG-encoded image to a uint8 tensor.\n",
       "\n",
       "  The attr `channels` indicates the desired number of color channels for the\n",
       "  decoded image.\n",
       "  \n",
       "  Accepted values are:\n",
       "  \n",
       "  *   0: Use the number of channels in the JPEG-encoded image.\n",
       "  *   1: output a grayscale image.\n",
       "  *   3: output an RGB image.\n",
       "  \n",
       "  If needed, the JPEG-encoded image is transformed to match the requested number\n",
       "  of color channels.\n",
       "  \n",
       "  The attr `ratio` allows downscaling the image by an integer factor during\n",
       "  decoding.  Allowed values are: 1, 2, 4, and 8.  This is much faster than\n",
       "  downscaling the image later.\n",
       "  \n",
       "  \n",
       "  This op also supports decoding PNGs and non-animated GIFs since the interface is\n",
       "  the same, though it is cleaner to use `tf.io.decode_image`.\n",
       "\n",
       "  Args:\n",
       "    contents: A `Tensor` of type `string`. 0-D.  The JPEG-encoded image.\n",
       "    channels: An optional `int`. Defaults to `0`.\n",
       "      Number of color channels for the decoded image.\n",
       "    ratio: An optional `int`. Defaults to `1`. Downscaling ratio.\n",
       "    fancy_upscaling: An optional `bool`. Defaults to `True`.\n",
       "      If true use a slower but nicer upscaling of the\n",
       "      chroma planes (yuv420/422 only).\n",
       "    try_recover_truncated: An optional `bool`. Defaults to `False`.\n",
       "      If true try to recover an image from truncated input.\n",
       "    acceptable_fraction: An optional `float`. Defaults to `1`.\n",
       "      The minimum required fraction of lines before a truncated\n",
       "      input is accepted.\n",
       "    dct_method: An optional `string`. Defaults to `\"\"`.\n",
       "      string specifying a hint about the algorithm used for\n",
       "      decompression.  Defaults to \"\" which maps to a system-specific\n",
       "      default.  Currently valid values are [\"INTEGER_FAST\",\n",
       "      \"INTEGER_ACCURATE\"].  The hint may be ignored (e.g., the internal\n",
       "      jpeg library changes to a version that does not have that specific\n",
       "      option.)\n",
       "    name: A name for the operation (optional).\n",
       "\n",
       "  Returns:\n",
       "    A `Tensor` of type `uint8`.\n",
       "  \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0m_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0mtld\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFE_Py_FastPathExecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DecodeJpeg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"channels\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ratio\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fancy_upscaling\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfancy_upscaling\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"try_recover_truncated\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"acceptable_fraction\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macceptable_fraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"dct_method\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdct_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mpass\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mreturn\u001b[0m \u001b[0mdecode_jpeg_eager_fallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mcontents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mfancy_upscaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfancy_upscaling\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0macceptable_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0macceptable_fraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdct_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdct_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m          \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m      \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0mchannels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mchannels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0mchannels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"channels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0mratio\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0mratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ratio\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0mfancy_upscaling\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mfancy_upscaling\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0mfancy_upscaling\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfancy_upscaling\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fancy_upscaling\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0mtry_recover_truncated\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtry_recover_truncated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0mtry_recover_truncated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"try_recover_truncated\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0macceptable_fraction\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0macceptable_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0macceptable_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macceptable_fraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"acceptable_fraction\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0mdct_method\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdct_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0mdct_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdct_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dct_method\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op_def_library\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"DecodeJpeg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                      \u001b[0mfancy_upscaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfancy_upscaling\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                      \u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtry_recover_truncated\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                      \u001b[0macceptable_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0macceptable_fraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                      \u001b[0mdct_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdct_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"channels\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_attr_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"channels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ratio\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_attr_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ratio\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fancy_upscaling\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_attr_bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fancy_upscaling\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"try_recover_truncated\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_attr_bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"try_recover_truncated\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[1;34m\"acceptable_fraction\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"acceptable_fraction\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m              \u001b[1;34m\"dct_method\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dct_method\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"DecodeJpeg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_inputs_flat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[0m_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\n",
       "\u001b[0m  \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\hp\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages\\tensorflow\\python\\ops\\gen_image_ops.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.io.decode_jpeg??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_img = tf.io.read_file('data\\\\anchor\\\\334093f9-209f-11ef-94fe-f21805e91111.jpg')\n",
    "img_decoded = tf.io.decode_jpeg(byte_img)\n",
    "img_resized = tf.image.resize(img_decoded, (100,100) )\n",
    "img_scaled_down = img_resized / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e2ee53ed90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(img_resized) does not work as the img_resized in a tensor ( not a numpy library )\n",
    "# plt.imshow(img_resized.numpy()) \n",
    "plt.imshow(img_resized.numpy().astype(np.uint8) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e2ee22d0d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAACpQ0lEQVR4nO39e7RtZ1Ulio/5nnO993uffR7JSQgkBBBMeIRQpUJ+8lOsgiJNi9/Fqvi4RamJEmhNJZZQrVAIWvenKWyIpc1CbAWiNAtEvOLFoFSh4RUNEAIhkNd57fde7zXf8/6Rw/rG6DsbcyDU2iSjt3Za2/PMtdf65je/ueaevY/Ru1VVVUUKhUKhUPxvhj3rASgUCoXiyQm9ASkUCoViJtAbkEKhUChmAr0BKRQKhWIm0BuQQqFQKGYCvQEpFAqFYibQG5BCoVAoZgK9ASkUCoViJtAbkEKhUChmAr0BKRQKhWIm+LbdgN7xjnfQxRdfTGEY0vOf/3z69Kc//e36KIVCoVB8B8L6dnjB/fEf/zH923/7b+l3fud36PnPfz7ddttt9P73v5/uvfdeWl5e/oa/W5YlnT17lprNJlmW9XgPTaFQKBTfZlRVRYPBgNbW1si2v8FzTvVtwPOe97zqxhtvnG4XRVGtra1Vt9566z/5u6dOnaqISP/pP/2n//Tfd/i/U6dOfcPve5ceZ6RpSnfeeSfdcsst0/+zbZuuu+46uuOOO/a9PkkSSpJkul2dfyB7SmeJHOvrd06zv1nzxe8XcSa2PTs0nwtPUHZViu3tvZ7YDsI583OtI/ZVlpyqyWQ0/dmxKrHPsgqxnWX8+OS+MJDHk2Zyv+N405+LOJef48i/LEZjM6YgDMW+pcUFsT3u705/XlicE/v29sZi2w9qYtt2zFwUZSr2ZbYcYxGYMfZGQ7mvlMc+GptzWYPxR5F8bejJ8zEemWOfJHJNhE4gttutxvTnvSQW+2zPE9t+ZtZQBOupEcgxFIFcXwPbzA0sU6rG8r06hRnjfK0l37dIxPZOYtbtyIf1UpPzlqTy/BxZNAzEDlsDRETjVM6F75vz7nlyDeTkiO1GZOatzOVn1nx57gbDvthOEzNv7da82FevtcW2FZnjq7eaYt/Zh07Jz23Upz8vH5HMS42tASKiqC7XSJP91T7K5PwPMnkyXcsc38njF4l9l198idj+yue/OP35gY0zYl/VisR2MpHnox2Z/Y16Xey74rueIbYLdq1tnn5Y7MPvq6I0aygK5HmtSjkGD667qjDnrqzMtR/HMf3Sm95KzaY8R4jH/Qa0vb1NRVHQysqK+P+VlRX68pe/vO/1t956K/2n//Sf9v2/Y9nkfH0RVOZidfFxDrb5/v03IPwMC7bN77o2nAjLgdc67Ge8Acntkr1vRXKfA+PH9+JjIku+1oJtfryOhfOC4zf7PdiHc4y/a7NtC76IKpxkfj7gfSvC8fPXwngvYNuGPwDwtfx4cN/+z2VrD9YLzovlwHrinyPvTVTZ8F7sb0HPlpelXR18PPA3CDnON54n13UPfO2+Y2f7HQe+eOC8O+x9LfhDj3/mo72X45jjc2Gf68o/CCzPfNl7nrxpOPBa/rueL1/rByFsy/0BW6s5nKsErw/L/G4Yyht1rSZvFGFobiI+jKmCMVWlvJYCtj8I5c0KP6dg5yCE1+6/AZkbRxTCeYZ1+1hvQF/HPyWjPO43oAvFLbfcQq9//eun2/1+n44fP05k+9Mvr5w9IeWJnLwMngqabXOScvhLZTSUd/NGHf/aMk8J5zb3xD5c7CU7My78de7B9vzi4vTnrc0Nsa/fl08bcy35NDIZmzE3YJEl8BfrReyvryyXK2d3V/61G7Ink5Lka/GvIM+HC44twrSUX44j9mRIRGSzi7NM5V+Si3Ny/o8smuMbDOTTkm3hzUsu3XrTvFdeymM9cfyofC/2hTLYXhf7LFdeMDX2V2cIN9d4PBHbxVjOxe7IjMOBL6aGJ/+yH43NOvY9+T7JRK7jlH0BOvDljn8Q4B8XOTsH+Fc0/OlAIXvyTRM5pn3HPjKf027IY2115BNdnsr1FgRmzuNcHqsNf4E3QzPmjb1Nsc+BL8+obf763h4OxL6dsVynz3jmFWK7ctn1MZDHno7lNZsV5tzd/9X7xL7Ln/ZUsf3A1rnpz+NKXg8uEFKlA9dWbK4JF67R/o6cCysxY2yF8FTvymspjs1rq0LOf4U3qxxYGPZHpOeaaz13cTU9Oh73G9Di4iI5jkMbG/KLdmNjg1ZXV/e9PggCCuCvD4VCoVA88fG4l2H7vk9XXXUV3X777dP/K8uSbr/9drrmmmse749TKBQKxXcovi0U3Otf/3q64YYb6Oqrr6bnPe95dNttt9FoNKIf//Eff8zvsdcdTjUNzzKPfa1A8pmgd1K/ax4nLagwzyr52NqJQOAszP24E0rxrIL3snzzXgVoDohB3zw6l4W854ce0CC5pIDazc70596epJYWV2RhQZGacVSlpDlaDXk8HqN5skzSHAVwuSXQnuOxOZ64kL9bOqB/xWYcDRCji4mk2QLGF9fgfeJczvGwJ+mLWs0cX+SG8FpJp+ascGJ+TtJDtg2aAxtGBEUHtXm5ftbX5VN/JzT7e0BZ+XPyvYaV2b81kMUxfgRagc8KUxxJmVgVFFUAhTLhwjZoGaEDhR+OudaaTdRI5OdkjO72SZ7ncR+O3Zb77dC8tw96BcH445EpYADJjWoBaEBM27BAx0Eq6S5WHEBE9LRnXWneB4pw/LFce62muYZPgPbd35FrbzI01F9Yl99HY6AJ/Uiej4xdAw9CYcHzrvousd20TJHFufu+KvZZINX4jIK3gJ5LCzlvZSG/V3ymyZU5O1f75aBHxbflBvSv//W/pq2tLXrTm95E6+vr9OxnP5s+8pGP7CtMUCgUCsWTF9+2IoSbbrqJbrrppm/X2ysUCoXiOxzqBadQKBSKmWDmZdgHoTO3MO1hCJkeEE8kP15Z0MjJuMdaJPWVCJomQFagQc+8d+BIPtkGDjmODQdeAOHZgea5JDH7GzAmrAAsigK2ze960GSIpdW8ZDhPpTbAeWoiopKVu7o1OYa0BHI9k9vjkdHZAmgM9oCY58cegn43GUsNaG9kOHAHGh+TAvtO5P4+ayp+1uWXwXh3xHbMjm8ylPrE3t45sT03b8ri47Hkv4tI6kfjgXyvjOkMqCn2dqU2QMR1EJjThjzvvMI0aMl5OXHxmhxjIkXS4YCtJ9DK9rblerJbZhy2DyXOnvzdNmsKzaE9IAJdZwil+jEra+735PXtQUPyhLU/tNuymdSF6zDtd6c/zy8tin0JXPylLT9nfdOMox3JtXZkSc4xk4OpyTRbIqLhbh9ey75X4PvHhxJ6yqEEmunHPjRXPwhNuFesHWG/Bx8Ex+6wA8DvOR9Lw6EnzSXz+pzpzvZjdHjTJyCFQqFQzAR6A1IoFArFTKA3IIVCoVDMBIdWA3KJpoxonBmOeAFMNYd96EcYGR44J8ln1iPJRY8G0lIjYJYVoSs51gwsQjxWO49+XAVw3IFrPnc8AvsN9FKTb0UO+x8bvKHabdAgWI+Ej0alI8lFtzqGs8+AawZ/Tjq+Ijlvyo0GkVdyXtDniyozjhr0PIGMQwvLhqcH9xk6tyV7JDLoM+Cc88Ngh1KvS02ly6ydhuAS2gbbmHRiNJQ69OOcPSftTxLsD2GWNLxfgoiohPOeZmbeKhd0zVTy7suLpp1hMJK6TdKTOhSa27q2OYY62AN5S/Jz66z/pbcn188SxKp8/p7PT3++5JKL5Zigr8wGzZFYH1qzJnWdoAb9eKm5Zl3QpXBR+KGZ440dablUg744fo0SEaV7Zr2NwIZoG5oPv+uKp01/XliUWtPnNqX/5WZqvhvqoDsFLqyJRGqOObv+qwp9/OR71RtGhw5Bd87g+4l/TWZgX0YFmhiCLlWZuXC5l+a+b7JHhz4BKRQKhWIm0BuQQqFQKGaCQ0vBhb47LcNOS0ONjSGbwwL7DZuVDRZgIzEeS25pNJKPotwiJKnkY7Ztg4Mvc961wWKmKsDKhpU9WlDiXCTytWNw2nUs87nzc2CnAzYlFz3NZI8MIXunhL81Nntd/iliXzOSNMjWGUk1ceqjgEftzS1Z8jw/b1yqE6AfJ2Dj84+f/9z05/bCEbGvsiRF0u7IUvciMu+1Upev5W6/RETzrHy6AdZHcSJfm7Na2UFX0lsEpfphS15OGaMvRn0oL4bMn4RRmS64IGeFXIuC/s3kvgZQMRMHfpfRzFkCxwPVuhWzLOq0JV0Xj+XxfPd3Pct8JtDVCVT1lz7QR8zZGVhmasHn1m2zjbS4Hcv1RczaKfQhrgDWbQjXezVkVlXAQj3lWU8T25ddZq67Bx98SL4YzqXPnPKjGtgOAdXqwHcOeeZ4KqDNH/ia/NyjrO0C3oXcANepmTcb2igmqVwjHriv25ZZTxWzPuI/fyPoE5BCoVAoZgK9ASkUCoViJtAbkEKhUChmgkOrATXrHrnnI4EHY8MRY5TxYCQ5SpdZmnhQ1phAKXIbLE6skn0ORMlmUHoZM240H8t9lgNltCzV8ciiTOe0INE1qEFEN+Ps2w0sDZca1umHDA88tyTTRjOw46jXjQZRQEREPoZyVl/Ok8M4e8eXPPYqWPOMhkP2Wsnnh1CHfelFxvZmpyc1rMqWY+qDpsJL6B/cOCv2YSlpxpZ9aaGljNS/Wi0zT6cevF/s67Q7YntffHTEkktB3Njak+OP6mYu7AATOKVWxlNNPShFxtYC15NjWuiYOR5BQu0I9MeUlUcXYN/iQ5vCbs9cW1EL4gswTgJ0280NUyI938DWAjmmbZb8ubAg17jvQUsD14BL+ZnDiTz2JqT+NttmXTsNuSYciM7+/Je+NP25u3Fa7Ot15Xk+sWTaSNbXt8W+CCK6IZyXKlbOjlpNmcnrZdw35fl2Droy6tksgmGfrgbPKD4kQ/OvYx5Jb+eP7daiT0AKhUKhmAn0BqRQKBSKmUBvQAqFQqGYCQ6tBpRMxlSc7wOqmMVG4Es+NnYkn8ktdErgeT3QjyroQ+EtRjn0CThgkR+yev4O9BikKWgqtuGmt7uS921CDLIHVjYF6wN6YPOM2HfRMZkwG7G52Yb47vactAixCnN8BWgkhSW59HpTzvkwMdpTtwe6GljkVxbv8chhn9gUfVoY/RtA7wK0TFDIYp0dV+oINbBg2t4xfHkthH4c6KsZnjN6Elql1DyMAgE9r2E+NwG9ZWlVWtnY7L2HQ8nnBxDDQUyrjECLScaSw9/ckT1czXmjQQwgJhw/J2JWPKhDgSu/EANcOLF5Io+9gL4mn9m7uKCVJTDGtVVjC1XAIIJQzkXMrn8Pet3mGh2xjekBGbF5hAzrT3z6U2L7BS+4avrz4tFVsW9vb0ts726Z89GE6yoFWyi4PMguzfGWpTx2D/THBtPdBjvyu8CC70x+ulyw+AlrUs9DnbDWMt99Oet/LLGp7ADoE5BCoVAoZgK9ASkUCoViJtAbkEKhUChmgkOrAeVJQWQ/Qsy6TBfZPic51Xp7TmwXjIf3PIiTLeX9drHTEdsZi7Hm2gsRUa0tvcc2tg2XC4nVlAH/mbLa+sUVGSeR9GQvjxvKMY9zw2PPrUnNpwBPrXhkPge1pBKs6pstczwORE13wQ9tuwccssds16Gng/eOEBE1mQ4yD3O43ZWx1H3mzYcxzhnw402w6d/ZMR50Bazqh87KvqBa06wZC87zaCxjHxZYr09QydcmMKbGnNSTtrpm3vrgqTW/JM/liOkVBXxObyjHVK8bfj8upL4Sgt61XJOeesMR/xzwJYR+sCGLK29BfME+vYLpewn07tSbUkcYQnR5h/WSBaB8tOD67rPY9hrojQXoOJOYxTw05LxU8DkFbAe+WdcYJ+FDvMe5XfOd5JGclw7EVpw7a6JMdrtdOWA0bQONK2K6joUxCaBTbW+ZMdWhV6ys5Jopc/Nevg36NfQ/cu2biGg0NnqlzzwYK2xiOgD6BKRQKBSKmUBvQAqFQqGYCQ4tBZcmGRXnE/Y89jgcQYpjvyvpCZ7+h6WvXgnpnfDYze1d8BEyBVrNYxEMWM6aY30xK5FM0Fo/khRWhnGkrLQRy3675zbEtsXs9WsNWRoeJ/Jzt8fG/sSy5Pv6QH8VIzkmzvy1oDzdtsCyhf2Js7croxoGE4gDsM1yzBJ5rnxYqr0NWV4cLXTMazuQqtmU1FjGqJq+iKUgSmBMNVb+vQfl0SWkjQ6AfrTZ77pg7YTj91l6KpYEe2Axwy3xA6BaI1gjTiW3Jyzd04Nk0hhK0PnlUQGFSDAmh5XbY8mzDcfeBJqw0Tb0Xq8nrWtKoIC4A00cyzFZjpw4Tg+ncN1hFMLcwpLY3hkY+nTYl+e9Bamn46EZ8xyULY+hFYSvPQtKp/HEuza2HjB5AVpK2i1JkZYsDiHbZ0Ul5yJgUkUKlK5jg70R0IJj/n3FqL4Uk1UPgD4BKRQKhWIm0BuQQqFQKGYCvQEpFAqFYiY4tBqQ61jknOeOW4zfPLsuy7BzsIkvSsN3NjuS+/eAT94Gi4piYvhMB8qLS4g2biyYcuocrGtsR27nvAQa9oWB5OgxymHYMxpXkEqO+FgTSrqJvRbKZgfAj+esXH1nV9rpPOsZV4jts6dOyTEyHjiHGOQUYp651XsHuHMnkr87ZGWzLtiFtAN5PIEll+5DG2ZdLIXSUsaHcvytc+emP7fqUi9qzMtScc7Lhw2Ik/Dk5wzHUivImAaJYyjRy4ZpFHXQJwLQPfkc89YBIqJzG9Lq6SkXy/jolMV/VMDTN+cgdoDpe6Ur/1ZNSnl+LKZFVWBJ5LoQWQ/7JxNWtg3yaZ5DmTmL+yigZHgUy5aGFiv7T0BTxPL17sY5se0wjWtxAdYt/NkeMI3Fg1pwbn1ERNRnlkYok9TguyCGloBxbNZIe1mOyYXyaJvpMbYL5wq+B1M6OOahAF0K7Y6ajpnjkq0JC3XwA6BPQAqFQqGYCfQGpFAoFIqZQG9ACoVCoZgJDq0GNN9ukXu+Bj1ldu6LHWnNMU4lnzxgOk6vK3neS49KW5IxePofWTk2/bkEX4w94Iy3Wf9RAT1DIUT2Vpl5r05Hxgi7Odi7gA1Og0VnVzBeB7SnRmR0kkkCFjOZ5MdTRra3Aqk5fI1FDBMRdUBPmozNXLgQUWDBkhqyXpnBWdm3FDRlr9K4b7SooJBzmoG/jgda2lLLrAs3l7z1YCS1maW6OR4LbG9ssOapGAeO/DjIIlSvQyxHxbl2ee4WFqU2MJqY85OBrjmG2PlzZ0wsR6smP/PpT5H6Ha6ZiOkMkwL64iAK4czDRvsLwbbKA3udgkW+z0VyTNjXNJzItbh8hEUYjOS+Es5Hj/Xq7fWlRrK4KHt5Rqx/rQS9KIT1MxlCn1Ni9ifQQzffkcdXYwfYhfHbcI2uMGuhbbDi8VzoA2qiFmh0aQ/6vdpz8tgD9v1VJHKeMFbbi3j/DvRdwTxZsOYT1gfE+72KQvuAFAqFQnGIoTcghUKhUMwEh5aCiwdjcs8/NvOKvggSLDNbPqKXqXmEnIOS2o11aX/S9uQjbspsStCBeHMkLUJclgTYCCExtC9ptIDRHkNIEO1udeWYavK9+jtmfxNSQeehPLfJyo9HYFPigHVKm702h9c2gErKYP+Czz4HKMM2WgDZLO3SlrTBdl+WwXfa5txakOxJlqQn9ln18FJfoJ1s2HYZd+Z68m8w7qBMRFSxxWeBLckklsdeOViqzMpmF+Ra7A7kOsi4TRRYtLjwuatHDFUcgl3Lww+fFtuXXXSJ2A4YFVVKVnnf+ZlvmjFHLTn+c9vSVqnGynOH4JRduHKMGdQf8wRYpHQdH+ynhobiqgjpIfk5g6E5PzVw1cY59l1I67XNfh+uHbTIcdjx2eCcbYNd02RkzrsPdjoe0ISdjpzzJDVrc3NLltv/4+fk+Xj593zP9Oe903Kd5qWk2fjy4lQqEZEN112eye/bBnPAHvbMeUT3/YOgT0AKhUKhmAn0BqRQKBSKmUBvQAqFQqGYCQ6tBmSVGVnVI/dH2zb8cjKRHH0EtbBLrDy0BJ0gHYM1el3ywH2mdXhg5zIP3LTbMPsx9uEo2m8wLjqw5GeuzElLDUgzoPrK2vTnYixLPPtD+bm9vhl/2JLjt4CbdrkVDNTJFmPQsEAbCJjWUYI1RwY8dsZsVxJLcs8LYM1z5rQp+/U8sKoHy5kQLIs4r12PJHduF/Lc8frpFISQPJXrq8ZscWIosa1BbAX+PRcnZl63NrvypaA9LR4xc5HkkDYKJekFKwvGtM6nPOVise2UckHtdI12YIG9URfKgtsLRo8cQWzFJauypeHMGXPuHIh5COFYHYiQsJhmFHakrplAOe/iikkYtUKpo+VQUu8xnafXla8dQmk1VMmT55tjWGzKEme0EuJl5e1IXnc5piPzEnRHrvG99a7YbkKMyOqRo2y48ljPxNJKqN7omI+pSe17O5b6djI2Y8ygFLyCz8Enlr0dk2rsO+Z6sKrHdmvRJyCFQqFQzAR6A1IoFArFTKA3IIVCoVDMBIdWA2rMdcg7X6Be5IYDTxLJqY5GUq/g2oEN/QerR6XmgBHRE6blHFmSrx1tSRuZUdf0BTlg0RKDvU7ArC9isIWpgW1JMoY+FFZPvwTaUhVJfjxjfSmVLXnqEWgbVsh6FyzULkBvaUi9Jc0Nh2zBsTdq0mpoh8UklPDnzh7Yzc/PGe0mg/j0RkPy4Y4NvTJktk89IHthykzy2itHjfVLBX0a7bbURQoWbVxrS36/gN8NIUbhyGJn+vPuQB7rblf2QD1474PTn5e4NQ0RNSKpi7gNc+5ATqGkkNcDt/AnIiqYfodR37yHjojIZg0itbocwxDiyX0WX1IDXZBAh/IgorvBIjEy0FfIk68dpub4CkJrKuwJNGu+5cgxufDd0AcdtMG05CyTx1pvgJUNiz7A+a5D/HjMdFwX+saihnztqfUzYrvOeslc6BeMQcP63LrRhOZAAm3CopmwYVSQhxH68rX7IuvZ91fBeoSsx5bGoE9ACoVCoZgN9AakUCgUipng0FJw4yyeWpBYhXm8xyf0dkNSPh6zvUkSSD3sy1JMpMos9kg8hnJvfKIMWMKlDSXaPjjVliwlMQYH32YoKbh2Cygg9khfYIQilJ3yYQS+pKjqdUlh8crkpJRH16nJMXhoj+IaKmoCNj2b67IctBOZz63VJI1TB7ffjS3zu4OJpJIm4NIbwfGMGAeB1FKE1kIsERKteNDyxyVDt4AxMI3Apbo7kHZNIUuaxORbFxJdF1lJOrih0ATWTI2vvZo8z2M4Hy7Y1aSMUnSg5r/WkrROzhIu5Uoj8uF4vKhjfg8sigjcvT2gT4uCpbTCubOw/YF9F1hQ1m8DjdZktKEP7Q9be5ICDeaXxfaIrbdxBWtvXs5Two6vDc7yJVyz/HtjDHZf+H1VQUvD1mlDycVg4+Mvy1LxP73zk9Of3/P//1Wx7+O/9X+J7ZDZ6ViOHH+eyvVkgy2Uzb47AnZ9W2ibfQD0CUihUCgUM4HegBQKhUIxE+gNSKFQKBQzwaHVgLx6jdzzpHs2YWXYQ6nN7OzI8ug5lpga1MESBOzNy0S+l8uI7gxKUiNfluCmluGtMVlyD6z2g9LwpouQXBi68n0HwPdzxnUSQwl3II+vxkrQPTizDSjxPLdnyqOjptR8kL+1Xfk5586tm9+tgRYAHLHPqGq3lErCfV+G5NU5FnEBNiXcGoWIyAZ+3GJRFbVI6kMpxAPYLMG2Qscf4OEdVk+aw/u4leThj65IHWGTxU1MwEYJEzmX2mZt7gy6Yl9RgAU+S+Sc5JhgKc8z6mG1wLxXWJNrLwEL/UHPaFo2aIqVA5EKsVnzdbTHqsP6quSYeIwFyF8UQXpnh1lXbZC0mLHhmrXYuUTrmna7I7Zj+K7oMaueVhNaC/bkdbjA1m1Syu8UH0qtx6yMmSeREhG5UHLugH7X3TTfdZdecaV83zNyLi5trkx/riDa5PLvvlpsP/iFu8xrE7lOqSXH78N1WTJLrzg28x3DdXQQ9AlIoVAoFDOB3oAUCoVCMRPoDUihUCgUM8Gh1YAKKsk6333jeYarXpiXfHKVy3toa74z/bkPMdoNsFmZIKfPun3KSr5vAJqDF5gxwdtQD2K3W4yX98CaowQrm7SSLLjPxuS5EOELQo/LIgpq0KfBow6IiDzGeTdrsk9mc68rtmOSB9ieMzqbDbqOA/Nmsx6oCcQZXHnlFWJ7xGzibdnSQUUh/2OUSL2iLM1+C/j+0VCej85CZ/pzDhqKBw0vPrP0z8B6JwL9zofYhIhNRQfWbQk6SML0PQsikzsteX5SpiNETdmT4sOaiMcQt870oy7TAYmIihJiOViPh01yn1eX4+fXVgg9KnGGawT2D8y6WDm+JvadPSNtlZaWzHlfgP6uzZ683j02/gnEVhBoiutgyzVmMe5HF6Q1kgfxHtxaKGiAVgwR7znTeZJCjgla0CiCcxmwePKdbXnuAniU4NZID371rNj3khf/f8T2xj1fnP7casOcVrKnq4R1wMG1Y+0DUigUCsWhht6AFAqFQjET6A1IoVAoFDPBodWAyqygr8sLwz3D7VYTWYM/Bj+urU3DNy+uyZ6bSSL5TOyD6LL+naUjx8W+Pdb7QkSUxYa3Rt2mCXHeTWY3n7iS983hd8OO5GCH50zt/zzoRw70GHBPqnRP6h7NekdsF46Zp96u5M5ziBjvQEwyMR54bVH2voy7UucZ9cy2l8n37W9L3t1jNvcO+LknECcxhn4w8pnOBnrd4pyMseAeehb44FW55LjbLHY7h16YBHSoADSgi5ZML8bYlrrObg/6LZguEgTyc5aX5fgTpiugZ976htQGLn/q08T2kPUjOaCVLS/Lc7nbN9daAf1dexDR7UVmzBPQBV2YU9+S+ssym6dJX87LieNHxfb2rokUH4/k+9oQ+xBPzJyXEJ+O/XY5HF+dRWsUhRxTCyIKAqbjxj35vjZcoxbTSCcQM59MQI+cl9ddwPRIkNmoFck5LViD29/f+Vmx78XXPl1sX/uDPzD9+WMf/ah8Y/ggjFmwWT+bxdawC/N5EPQJSKFQKBQzgd6AFAqFQjETHF4KznWoPP94FwTmca5hS3orbM+J7YxZ4I+hHHqzJ+mJNlAbK8um3LIPlvJhR5a7tphNRplKKiYu5KM0L10cxfJxfg/or0uPnRDbARujm0qKIQjk6bNZ5fViS9qHDHt7YnvEyn4XF6RFUTUAaimQj9Px0MzN1hlphVT35TzZLKm0hPLiGpQxW4xCrCw5BqQ1rVLSVDa3JYJS6griAGosbmI8kefDhyiKEaNqKkjjDaCUdwT2+j6LEhhAqubiAqTbsnL7HGiO/lhSlRmz+Leg3L7tyvlP4fjifnf6cwiXPx6fzxbUCBNq65LS8lmJsA/WTS746+QQdZKyWIgU4iQalTyeiLU/bGxKWnwZEoOTwLxvCZY+BRwPrpGV1Y4ZQx3Sd4GWShll3QBLn+FE0s4xkwxciKVoz8trlqBMu2SfOxyCPVNDjilncRLrm/L63Unl7y495bLpzyusJJuI6P77ZSprEcnfrVjpvm+zFFkbTZUeHfoEpFAoFIqZQG9ACoVCoZgJLugGdOutt9Jzn/tcajabtLy8TK94xSvo3nvvFa+J45huvPFGWlhYoEajQddffz1tbGwc8I4KhUKheLLigjSgj3/843TjjTfSc5/7XMrznH7pl36Jvv/7v5/uueceqp+3xXjd615Hf/EXf0Hvf//7qd1u00033USvfOUr6e/+7u8ubGC2NeVabWZb7jiS885Af3FZiWRky8ObsyR/GUMZpJUZftZx5O+un9sW29yaByOr/VCOsWBlqQFw0QutjtjOx5Ifj1gZZ6sjYwbGY6krNCLDj/eGUjewSc6Tz2x9MC66UcjyzxRiK4TlBpSZov2R65r3bnQg6hvsXLLS8OU58N95Lrn0EGIg+jtG45prSE0LufaElfq2oRR/ALY9LVaOG4CdzpktOccd0HX6qRmTA7YqWSG1jiAM2D7Jn4MDPuXM2gZjNnLUCbEMmPH/a0dWxL7TG1JTabCY8Fog2wPGEJE+Scy15Dbk+gnxAEA7O8Oipn2IPXHwd1n7QAGWPljaziM8KkfOS1Ggnic/xnXM+rMs+T2Rol0Ts9fJKvlaL5JjnHPN3CRj+O6C+G4bY7dZybPjyQGnEKVBldlvw3fmn3zo78X2j13/wunP//wHv1/si//kz8X2LnyveCwChtuiYRvFQbigG9BHPvIRsf0Hf/AHtLy8THfeeSf983/+z6nX69Hv//7v03vf+1568YtfTERE73rXu+iKK66gT37yk/SCF7xg33smSSKy0Pv9/r7XKBQKheKJh29JA+qdN/+bP1+9ceedd1KWZXTddddNX3P55ZfTiRMn6I477njU97j11lup3W5P/x0/fvxRX6dQKBSKJxa+6RtQWZZ0880307XXXkvPeMYziIhofX2dfN+nTqcjXruyskLr6+uP8i5Et9xyC/V6vem/U6dOPerrFAqFQvHEwjfdB3TjjTfS3XffTZ/4xCe+pQEEQUABREsTEY0GPXLP6yxeZXjuap8bONj/M8v8ooR+FtBmvBC0AYtRgQPZ09GBGv1kZPSXRlNy3qhXRJHhz0c9yf23I6lXFNAjYYdMa8K+n0wee8k0Ls+XHGzoSq0gi81+1HgCF2O25ecUbNo8iJauIF6CW8j7yFsDX16wfqkUog9CPHcwFw3f6BVWCosErGC4+04Jceo1sNOvuEc+/Lm2L9I6h16ZObNmSldqJkkh+8z4Wi1h3Tpgbe8znTNNIJIb4jCKsdy/xqIFbOh9mYfo7JTpq3jdhZac/4LpjzsQje1BxoUHERhNFqFSQSQEJJ2QxfrvLOgj60LcxzzrR3JIrss62Cp1liDmhb08h96kCiaDX4Y1T46pgDVhl2beVufkd8q4LzXdIcS4D5lG5MD6dz3ZL8XcpqiE6/fOex4W2y/+/xpp5LKG1Jn/2Uv+mdj+67/6S7E9GbK1yvTGErMlDsA39QR000030Yc//GH6m7/5Gzp27Nj0/1dXVylNU+p2u+L1GxsbtLq6SgqFQqFQfB0XdAOqqopuuukm+sAHPkAf+9jH6OTJk2L/VVddRZ7n0e233z79v3vvvZcefvhhuuaaax6fESsUCoXiCYELouBuvPFGeu9730t/9md/Rs1mc6rrtNttiqKI2u02/eRP/iS9/vWvp/n5eWq1WvSzP/uzdM011zxqBdw3glNV5Jx/1OUV0Vg+WQK94lmsVBEePdNUPuJ6ETi22uZxclKBXchEvhdPdeztSpubuidf2902dN5cc1HsK0eSjmgArROwqMMKYkKjpqTVClZWPonl+6KtBy8d3+dwC6mHI7SrYc67CdBFJcnPzRil4kClqAV0HT93UUtSkxk4aZfgju0yOgbnaQy2StzBN4Iy5jE4ptdYSXE8kuun3pCXTw5l/TkrjU2hxJYg3dZh5axIgVrgsi22HElfl5A+6kIJLieeSriWfKBT+yxN1a9LaqYCWrBg62tUyXNV2GD9Aq0INqO0cqDvBpDO67jmcwOg0SKw5bLZGLElw4aWjGp08JgTKDm3LGgfYDSuD/XcE6jZdguzPbcm3cedSs6LA+Xq48Ks+VEsx2un8ny4bG5SWP89uA7f9xefmf5886uuFfvmjh4R282OvC4pM99tA0blo9XRQbigG9A73/lOIiL63u/9XvH/73rXu+jHfuzHiIjoN3/zN8m2bbr++uspSRJ66UtfSr/92799IR+jUCgUiicBLugGhOLboyEMQ3rHO95B73jHO77pQSkUCoXiiQ/1glMoFArFTHBo4xgaUXta4puzclEHuNwRaChFYbjRlXnJV+6NumJ7uLsrth1m+VODMtMSSqvtgtkDlZITDgmsbepGR6iD1QiER1IIaYulsAFBK3S5zalpPwQ+GZ5ebXbqU9AuCDj7JuhSBfucFDh7bstPRBQ6fJ7kGCxITSzYnI9HEG0AupoDekXGtBsH/q5qYmk10waLXGpJZS5LbvPYzAXqBgVoigSl41HNaBIOXGqWC+e5MHOD5zXDGIiaKbkdQSm1B+vWBa0mZnEgLmgxGZwfn+lSEcRhjMAyiktaoYXnGbQyKPvnUqAHabDNmiwvLtnxBp58LQyfAnatgdMOjaDkucrlGH225oNAjiHLUTsz40iG8nvCi2R5N9fktvbkGn/60y8X21978Ktie3udlU//E8ees3TYyoNr35bX+/q6Gcenvii/E7/vyo7Yfu5zXyi2P/Y/Pjj9uc6uK+sxWvHoE5BCoVAoZgK9ASkUCoViJtAbkEKhUChmgkOrAU2GE8rsr0dym16NIpdF7DZahLiGG+3tSuvwFCzw0TbGYbXz9ZrsD0HbFWIa0BDsddD+32fbdajth5YbKkF/4ZWHqENlYGniMCIee2zyBPtDzPiRri0q7CECnYHNU+nIAwiB7w+YPjbOYJ5c0FQqc265xf0j22D5U8nfjULzuTlw9K4L+hE7HzH0F0VgZeMRs3aC92lDH0rblpZMJZvjDPp+rFAez4RZMCWxPLYjy7JfZJKb/S7oUhH0BdXgeBKmNeUF9NCBJpRaZh2kqewF4z1zj4zDzMUc6EU59Iq5EE3hMb2SR24TEeFS5Mu6AOGj2ZbWNiPWx5SBPueBZU49kucuapr9cSrXCNYCu9zSy4f1A2smZtpZvSO1pa+eflBsf+m+L4vtGrP8SlLZ2+O5cvwx6xni6+WRMcrPzcdG9/mfn7xL7FtdkAYCl0bYD8Y1UnNysJfwIOgTkEKhUChmAr0BKRQKhWImOLQU3CP3xkfuj6uLhoKockll7GSybNBmVhfoAhs2ZGJlBVQALy+uoDS5N5FJn43QPMb68D7jiXxkr3cYnQe3fKSSCKmlyJRtJmCpEUSSAgpqZtsCSxYPKCy3Mtv5SB6rD+7kLtBHPEDQI3xf+dqYWZzYkA7pBmCVFJtxWJDgmidyjCFQNTmbtxLK1TOggMQWpE664NgdMWsVNNm2SdI6GdDDFqNBauA0vdmT67bdNtSGC/QvurpPGPVaA5dwF/yOSrkUyWXluw4svQzpbO6BBWXwHtBfE369TOQYQrg+Eri2uJUNGGdTEsv3qkWcPpJjGg4lTViyMQY+loJDcik443PLrxwcrVtgE1VU5rUWzBOW+fNWiQcelmXWo1i+toBy9o2z56Y/H1mT5s687JqIiNiawZJoD2y5Blunpz/3dzfEvveMpM3Ya6//PrE9YVRb6HG5QCk4hUKhUBxi6A1IoVAoFDOB3oAUCoVCMRMcWg3Id3xyz5dh15km4UJ5burjPdRsJ4Xkx/sJ2ofIw58MDdcbQxrh0oK0eq/VDU+PVjYO2GT0xt3pz2Ul+X0fXttqSq0gYNqNBfpQZYHmwLZz0COyRHKybWZxEtWl5jMY9+XnQCkvMZ3NhbLfBBI6602TBhuDPoFl2VzDyoZyTo+uHRXbe7tSk+NeJK4j9aEG2Ll0u+b4PFfOfxjI87PLSvn9egivhTiAUK6n/pBZrWTytXUH9EmmDZSgLXGNgYjIZ+JNBmuaQBvLQA/jpbIeRhSA/uIyDTVO5bnbgRYHNzLz5kB5ugdr3IYS6JhFh0zA4qcGMRC9njl3FZTb12p1sb28sjT9OQct5sU/dJ3YxgiPlJXn33vvvWLf7va22ObX8NxyR+zrwnfDHrMA6iysyM9EbWwir4/lDvsOgriVEHTbImfWVKnUaW0oDY8a5vpoL8hzM9mQmtDmDqw335S+T9KuGR4KigdAn4AUCoVCMRPoDUihUCgUM4HegBQKhUIxExxeDYicaY/JXMPoIpEv+czN9bNiu9E0Nfr9Lclf+j7EL49Br+B8M1CYqG34zG7EBuudypLcrce0gVZb8tTIY6PdTmUbbrdWB0t/6K/odU3NPtrC11zJ7e6yKApIRaAaWKlUDoyRccgOaBlpBn0PzG7HBz3CRUt5Yn1A8L5nzq6L7ac97Wli+96vPTD9OcAoY+ivkJYhYFkE/VO87wS1mBpEa4wn0uKf63kV9BstdKTWF1vmvaMQ+oCgv6XVMmsoKzFPXeqeOdiwlKzXKh/JdeqDNhMwixyrlGttaWlJbAv1Aq7RFDNHYLtZM3MRg2VONoFzx3TQtePHxb42zKkfmjX0ou+Rms/mxhmxXXPldVlvmO150JacNkSvs1MQgd44v7ImthvsuhvCumxChArG3dvs+sEerjroeRmLRbegn6gB8SQ8p72CxrEc4uDXN6X2t3bs0unPD331LvZ78KVyAPQJSKFQKBQzgd6AFAqFQjET6A1IoVAoFDPBodWA4iyn/Hxtfsxitheash9nabUjtvsDw6sCfUm7e9J/q4T+Fm5ftDAn/Z58eC/ugj8YgE8ccKxN5h3V60tvJQ8FGODhuadTkUlO2EFbe8v8bnNO9k8kGKHM/N1qqDlAzEMGkQU8rhi97FzwwhqNTN9GGEh9wobIiJJpM03o/0jBr25nS57LZp3x9KC3OJbUmjpHjpjxjaW/HrYvcD+xDGz5Y/AII+ivyNj5EuMjorSA92L2+vPzUsvIIJIgCM3x7Q3k+zjQ11RBc4/Heq3qNak5oH+Xx/rDUCurYojdYNHfKfiSBeAluA19NDabx1ZDaiiuC76EHvNg9CECPYCeJ8+878bGA2KfF8jrbmVRfq/UWT9YOZLa3mc+dUq+lvXnnDt9WuzL4fqOMzOmCnROFGNt8HCzmNY8HkldsBbKc5cxv0YbohomEHdvMx+8nb7UzbOxvGbPnNsS2y974ZXTn7/6lTvN76E33QHQJyCFQqFQzAR6A1IoFArFTHB4KTinmDIaqyePTf+/CREEtW1JH/VHhmrKwdqiDbQa0hU+s+KvgIaqgIIYDcwjcAPSFAms3oeJKXd14dG/0+mI7aW5RbFdMKv6va4sgSwgH6C7Z6jAOlAZvitPteuYcaRgs5JAeagDY+alyiWkpVoQx1BvmLnxHLAoiiW14TMLFzuXn1kHu/wK0jyTiTkfUQOtXiRNxe1Eckt+jgUl3CWzaClKeV6RqkRKi1u0YMxDbyip2Gbd7C+BcyuA+ksY3RXAebUwZRaoEE57uj6eV7kOCs9sl1DuHdbAxoed2wDK1ctCHs/JozIWxWNrMYQWgHEq1yKP2igh4bjVkLQtp6FbLfm+c3Cd1X1JkTYYLf3sZ14p9n32U/9TbFcZo3FT4Euh1SBgFHXlQIwLfF8lcD4ytuYxvXYHSvXDpjn2FL4nMK5kzGzH6nPy+9Suy+1zW5ti2wmfN/2Zf/fG8B1yEPQJSKFQKBQzgd6AFAqFQjET6A1IoVAoFDPBodWAWstt8s5zpKeZbcYXwXonHUmu8expU+JZhwhuG8qWc7D9mExYDAHE1s4vS+uRpG94U3C6oDboLxHj97k9CJHUnYiI1s/KEs8W01Bs0D08V/7uGisvLmx5bDmU/SYjw1sHEKkQgp0+2rePWekyDElEYxMRZUxDsWw5Bg/iDFymZ9QhViAGjtsF+/kGm4sMYx6Ah0+Y/VEAHPf2oCu2LaaT+KBhBTBG2z64DDsHLQM1h4qXew8hhtqT64lrAbb3jS1PmhDZnQnNDiz96/L40orFOsdyPeH4+aGj3tUfyFL3NJHnh7u22PJ0kA1rhsd51yJZrn72zINiO2cV0U8BTXF5+ali++il0topZdfHpC/LxnOwEirGRsvMIEI8h9L2zryJL7BB1wxqco53WPQEEZHFyrQx0iIHSy9iLQ/coouIaHlRxkBUTHN04MvMC+TxrG+dE9tcPp5bMLraBKIkDoI+ASkUCoViJtAbkEKhUChmAr0BKRQKhWImOLQa0NkzZ8g5Tyx/+QvG4uHEgqzfbweSB+bxy9hTsHlW2kxYaHXBthfBmmMylpxmo8N6ikAIKS3JEcep0S+Qt84TyY/XfHlKRl3T2zM3LzWt/kDqIj7r1yksye+74N+eT8x+D+IkbAu1MvleVmWOt9aU+kR7CcaYsF4feF8PrHhcNo1jsAvxQOsoSGoDYcQiIsCaPh5LHrvKzWst8Gua78jznrN47PEEtIwUPPFhHj32910AfRse2NNkLP/DquSxYv8Up+nR8MS25N+UeA0U7FzmGP0NfU71wOiPC82O2Bf3QcdhGlcyludmsSPXSJLKMQ5YD1dvIM/7pJDaWcx60HZ68rXz0NszZmt8c0Oeuy98XlrmRG0Z+c5jXzDy4tLLLxfb93/ta9OfA4g98S2Iu98110MBMSeFh88D8D3C+qkSOFdBIDW5rDL7YflQksK5Y72GRSavnRRiaAIP+87Mz1c+0+hoI7AKOgj6BKRQKBSKmUBvQAqFQqGYCQ4tBRcPBuScp8RW5k15ca0hKaz+UD5a5wlL4KxLOqUD5YdlIqmCXpeVWwKVMSmQVjOP0jw9kYio9CQlV2dP8EkBDspgt9EGCmiHUQ6jIThaYwkueyx3SL7vzs42vNaMEVgDWluVdEQX3L5rkTmgQmZh0vaOLNMM2sYSpILy27IAKxJW6tuCElUbjodcmEceYQvO5WFD0iAWf6tQvm8KKZTEaNmgIcuLcQg+/KrHxmRDeeswhlJx5v5dILMHHApnahpQyp5BabUNJekVW38VHKsfod2RoVFyoDELKC/mZJLrgk3SWK6fcQoO5AkrV8/BdRsonwm3uQIbosFAHis/9OFQzsuDGzJht/WQXLfPfoahkxywXLroImnNc/aUcWZHu6x6uyO2PT5T0BrhOZBmm6CdjVkYYUvWq/eGco5PnTLjOLK4Kt+lkp8bMXd7pGEroB+HI1kavsfoUytgVCtcCwdBn4AUCoVCMRPoDUihUCgUM4HegBQKhUIxExxaDWhhbpnc89YTdWaN7kCp62AgtY21tePTn3sDaffvQElkLZQxCvW6Ka1OJpIL3dyVCZxOzfC1UUvqUg6kUPL41Anw3yFYavSHsrTUZ5Yzk4ksbaxH8ncTVu5dQQnnfFuWR7sWs//P5Wu7QzlvJVjM5MzOpQTrnfn5jtjmWkdVyTlFW/iA6RkOlGjbttQV3FAu3YppLGUmP6cGMQm8rHkQy/PhgKzm+WaeilKOIYA4j0lX8uMuszQqYPwOxD44zDInwDL4RBLq8cRoAxHY9NRceQDr56S20Vk0a9yBKI26A8mZTC9KIfoAS5OL3IzRAw10G/RH+FVyWGLnYCL1iWKM5etmu0JrpBDsjVgsBOpOVSb1ldNg8ZXl5nifdlxqx2kuz0edpd16vlwDY9CPeIoxansliH8+JKbaTKy1YBJrtvwumF+7ePpzvyuv51YAURqsjSTwpQ61Bxq758vPWd8xNj/zrGzfzuA78ADoE5BCoVAoZgK9ASkUCoViJtAbkEKhUChmgkOrAZWWReX5HoxRbHjU0pf88urxY2J7ODTcbQlWOy70FAzAMtxj3HsCtuod6M9xWcRCOpEc6+q8jP62WUw12s+groDgegbGefeh9t9xeN+J/JwYYsIjFrGQQBRwAH0zfk3yvpya9kGvKMaSW2+wF6c2xGzXpOaQMeuUPsQiNCAqIBtjvw6LKw7A0x90nYr1LgVgfxJA/1fKdBDLkmNAnarZlnpMyOxcBmBNEkWgV7AxZrnUQTBO3WE6TzqCWHPoC+q05FqMWZRGG+LgB3tyPU1YM0cC0eVwedCAjaOE3rZmXc4L9pYkKdPKINK9gF4xy2PnFtZTFyI7akyv8KExpQZWTqNtqZWdTbrTny85IqO+V9ekJnTXXWzNN+SYXNAJhz0zTxHoKUOwIarV5f7dHROH3V6StkM2zJvD1l5Ul9dZAhHpCYu0aXfkuvTh+oBlQLs9o41bkTlXo5HUjg6CPgEpFAqFYibQG5BCoVAoZoJDS8EVnj2NWYxY8qRt4yO5fCZsz5vH5TSW1FIG1IDjy8fWyDWP9y5QMTFa6DDKJ2pAOSXYn9Rr5n3LSpa+JjCmspKUnM1oHseWJZINsKtJmcttivQiuDEXrAQ3AiqsAjfv0VhSGzGz1PGglLoF7st8Gh0o/yzBHqVMzXbgwGuBqowiSbPlmaFUHKAjKiibtZlFTgkO12ENKIjQzFsfrJsw9bEO9FfJ/r6r1eTxYGqrXZnfbUDS6j7L64i5nsO+MZQXO0DbpsyqanNbthZgmm3KSsf3JlBiXpe0VMLoSRfWQGZheqccM78E8lSOdwIvXlg05yeD1N99Ts2Mvm5Ecv20A/m7liPPx4lVY18zGUp7HRdK3aO6WSPJjlxPNtCNDqO+HVjjR5Zk6vLXHrxPbC+vsP02rHF4lGjWzPkYD7tiX2dOWvOES4amHYILfS2U3xsE6cJbjLp0W4ZqHcN3xkHQJyCFQqFQzAR6A1IoFArFTKA3IIVCoVDMBIdWAyKHzO2RlRRj0XJ/ILlpn3GsnSakWwJf3tuVv7vJ4hisUt6bW1BiS0xzqKEVTwXWKczaxvOkjlMDbWYMCYQy4VJy9AlYtBQ8mRVTNIFvHjOrdx/TUsFqH6aCfGaNVIwl541aTZIyixbQZnKSvHsU1dg+ebJc4J4zKB0vWDppDbSYESTHZiy9k5dKExFZYJHvsnkMQU8pKvScl79bsgVXwHgdS66DlNncN9dkHMZWV1rZVCxvwgG9KIVy42QiNaFE2C5BaXUKCZ1Mf0xyuQi2zsoxuU1zDbigT5w9e0Zst2qyNHx1yZQ11+ECn4DuVrDWA68m57As5fj90Bxfkss1kJZy3o4sQJuFbdZmpy5LnhP4znnKSdMKcu+X7xb7tkZyntpz5lhLH8+dvA5XjiyL7SFLTw4hQ8Uq5fdGNTbrbQ70rgLsy4rcfLe5cP1WoH27gdyfs5YB2zO6oO0+tmcbfQJSKBQKxUygNyCFQqFQzAR6A1IoFArFTHBoNaAyKaY9JDwNwAN+PwXOsmCW5mc3pL3GwrzkVF1f3n8bLcNhOjA1SSK1jsgzPHcOPLvfAQt5xrWPBvJ91rsbYntp7YjYLpgW4oA9uw89UHlm9hfQINIHmxI+j24kufQsk7x7BnENZWb4Zgv6lrp9aQ3TrB/cW+XA9pjZudgQoVBCP4jny/PDrW1isPRHi/mCWSc1wMJ/D+IZSqb7YA+a7cl5cUq5DmzGn6NtTwD2OgtNo4tsnH5Qvg98bsi0MsuV75vC35R+0BHbO2dM7EAGWt8klTpCUDe6TuBLnTOI5Zh622beKrCBmpuT1jVoEzVikR0j0FeWwAKLX9/YK+Zi7AbTiGpNeZ7nF2U8SQi9PXMsqiWDXjHHR1HUjGluTvanTSDiOmH9X04ldeVuX/bgZDn2qJn3tkGPtFB7TYxWVoANVxDKHi6faTfCE4qI+nA9eKAtj1lEzEMPmdfG0CN3EPQJSKFQKBQzgd6AFAqFQjET6A1IoVAoFDPBodWAQq82jeROR4ZPtMAHvgJ/8DMbpsb9xHHZTzEcSV7SJ8kZu0xTycBTazKGOnv22hL0oaCUPPBcw/D7ni0/c/mI5IF5jw2R9GEbwedgL8yZM+tmDBD5XAdL/JI19ySZ1IdQG0N+OY2NxpIM5Zyi71qvb8a8MtcR+2qBHKPDen0K6EnBWPAUuPUR060s8Nurh1IDikIzF6OhPM859NF4rLcEI8TTVP6ul8vtMjbzivHpXYhJDgK+vsC/bSKPJ8+MVjDKZU/HsJDn6uGNPbHdWDA66BB6zhzoS0mYdmBlqLVCPLlvdAQPerZc8DAk0IBsloN+7OKLxL4ORoEwHTQAP0DbgvPOIjwyWJcN6AlsQtRGk2l0qKeOE6nVlKW5BmrQLthKQFMpWMSFDT1noGGNd+XnuKmZ824iozNadflV3mRrPIP+tGQiz/vIZT6K8D2RwnVYb8nPOX364enPl7FonGqfgeGjQ5+AFAqFQjET6A1IoVAoFDPBoaXgXMcm9/zjtu+aEsoK4gsIHp0XlkwZ8yQBu3aCEsKxfDRtsEf6NJaPqR6UzSYs+bPWAEqhkq/lCZA+lP0S0EODGB/vDe3T25P7bEiEXFww5a5YBolxDLyyGhxMaBeSMYd70rZ/vmVKYxc60qak1+3KN2Nl2kmKNh/yXFosqVSSXUS1BSg3BhucilFnjaY81gEkjEbsXNaAxmkFwKEw6q8Ces6HpEl7Ahb5LN4Ag2/n56X1Pg947W3JUmTPljRUPDD0XWrLMQ0LaC2Az9kcmHXhN+T4CyixHbJyexfKvQsodZ+fMxRcvSnnsLPQEdtI0dWZ5X+BUSYRlFqzcmnXk2PysCybrb1aJI+1CZRoBTTb7tbW9OeFFVlG3oxkSfqQWSWtLsjrYX1dUqQh+9wilOMd9iTFjrR5xqjvEqIzBntg9cTSVJs1ucZTS85xztoFSrA+qsEaj8cgYzDaMI3Nd2IK73MQ9AlIoVAoFDOB3oAUCoVCMRN8Szegt73tbWRZFt18883T/4vjmG688UZaWFigRqNB119/PW1sbBz8JgqFQqF4UuKb1oA+85nP0H/9r/+VnvWsZ4n/f93rXkd/8Rd/Qe9///up3W7TTTfdRK985Svp7/7u7y7o/bNJTNXXNQ5Wtol6RQLcrcXiDvoQC7vYknqL50sumoc9OFD2O47ley0w+/lWA+KjoQRxwMq/HeB9PVeKA0Ow/fAZr43m/xmUU4a8NBwsNQqSfLnFyrT7Xan5LC9BZK8LJd1MQ8HS12ZdvjZldi+TseSpx1C27DJNq4LgjTiTx2qF8vh8dg6GKVgJga5QsLmZQJxEDnY09bbRNtwI/l6DiPHBbldsL813pj+nULZse1KDKNiYF5al5jAeyXnLWdR0ChEXHugGu1157A7TIPNC/u7CnLS9sdjcVGC5tHBUWtn4LFLcg2jsAKz5axFEhbAIjEYAOhqcu5SVUweg13kO2DexiI5JKq/fPJdzaoMmZLtMNwHLKBe+N+qOmdPVzpocb/9Osd0bm+vFtaQ2U2tKbWkCemrUMK9vwmutTF6HDWa1VYLW2lmQOvQwNnMRw/ovYY00YH21mH0Zj+FOYnltH4Rv6gloOBzSq1/9avq93/s9mmOLttfr0e///u/Tb/zGb9CLX/xiuuqqq+hd73oX/f3f/z198pOffNT3SpKE+v2++KdQKBSKJz6+qRvQjTfeSC972cvouuuuE/9/5513UpZl4v8vv/xyOnHiBN1xxx2P+l633nortdvt6b/jx49/M0NSKBQKxXcYLvgG9L73vY/+4R/+gW699dZ9+9bX18n3fep0OuL/V1ZWaH19fd/riYhuueUW6vV603+nTp260CEpFAqF4jsQF6QBnTp1il772tfSRz/6UQrD8J/+hceAIAgoAEsWokdifb8e7VuvGb5zEoNFC0QFk2t0nhR0G3dBfk4beN+c6S8l9P2UkOftsihnL5S18nkmdYUO65FwQ9CdfMmxNi1Z358xvr/RkBx9BaeA9yaNgfP2HckZJ8xGxg3lvKxvyD8WAogD2OkbzagOfP4y6Bdpz/Qu2Q5a4oAVErNVAtqd0grsRMAGxy/NODCq3IXYinhi9ttg7dSpyXnycvO7feDkm0zjISKywWap32N9G9CjUoAGMRmZ9TUY7sjXQlNUxaIdwI2JQK6jpQWp1cQ5W29giePCtRWxZrEAenfQyiZgse4RXDs11B9RZ2BWMRlES5e2/KDVVdNns7Iq41VOPSDXbcI0R4yWLqFfarQnP2dhxWg5kSd1nQC0ppx9N9ge2HDNyb6glaMnpj/ftyfX8BC+Y7ojuIZZn5kN35keaOE2E8vxWur2oUeQrflGU0Y1yCuHyIHeQ94PtnLMzNPk2xHHcOedd9Lm5iZ993d/N7muS67r0sc//nF6+9vfTq7r0srKCqVpSl24UDc2Nmh1dfXR31ShUCgUT0pc0BPQS17yEvrCF74g/u/Hf/zH6fLLL6df/MVfpOPHj5PneXT77bfT9ddfT0RE9957Lz388MN0zTXXPH6jVigUCsV3PC7oBtRsNukZz3iG+L96vU4LCwvT///Jn/xJev3rX0/z8/PUarXoZ3/2Z+maa66hF7zgBRc0MMfxp497OSs1tZAaA7fWcddU0bWgZNB15OPjOAbLE/6+mXxkb3bke53b2Zz+PIolBbe8AHSXKJuFhMQJ2NFACSt3o80zycVsr0uqJme2PWELaEHJgtCI0Y31QL62MyepDUILHVZCzJ2AiYgGUH7p103JZ2CBI/ROV34OoyAw/TW35Lz54P7Lh9iDVFYHbGQ6zH6n2pcKKodUMEq0AEKi25UUSQRWNiNOaYFtDDosRyylMgjA6gXaCRxG49bBkmgAFlL1OpR7Mxq0D/PkAVUWsXlaATuayV5XbFe2mceqAIf0VJ7Lhx46I7aXT5j3tmryfLjQpnDqnEl03dyU699zJP3F3WoioGFtcEwPPDlGboF17uxZse+itRNimyez9kaSfl9elfTd58+Ynsh+IdeLF8ny6LAh6TBi3yMTsCQrS6SdzfH4LpSYQ6m7E7CkVUe+1oJE1BTSkhtN893BjwZbRg7C4+4F95u/+Ztk2zZdf/31lCQJvfSlL6Xf/u3ffrw/RqFQKBTf4fiWb0B/+7d/K7bDMKR3vOMd9I53vONbfWuFQqFQPIGhXnAKhUKhmAkObRzDufVtcs7XDx5ZM8mmPtiqO2DREniGMx4NZXzBOJTcZwmpgtyiPQILcx9iE4KamboxWJQ3Om2xXbAxllA+bAWSBy5yqfOMWOJoDnrRRRddIrY9Vtac2FIz2R3JSIW5OaMzhLYcQ6vREdu9PbSJN7+bpHIO223JY08YJw60Oy0dlVEBBUvgtEEDGidSr8ghhqDN0largTzPcSLnPEvNufNceewZRHakqfmcHMqWsbw1CiR/HrB5LUF/RCsYJzBv1mxLDSjM5ZxazKZoArb3TiDfN8NJL9hahHmZgJ6aMXuXPmg+NqSp8lL+CucF4hmiZke+wOY2PnKelhdk68HXvvTV6c/t5Xn5NpCOnBM7VpgH25JffTlovgMmBi7U5LUfg/5SYy0ZCZzXpYUjYnvyVaN/WWDlFNigwdXkd10lbHHk9wSu45CVg6dQ3m3BWrSYzdKk3xX7XNBImxDhMd8233X9vknfjb+dVjwKhUKhUHyr0BuQQqFQKGYCvQEpFAqFYiY4tBrQ/NLy1J5/zPhm5I+HQ6ltHF02dfeXXnxM7Ish7tqxIZKY1bwXwIdvQyw1p5AtsGc/dU72OTyFWVQE4JWyA5xrDL0+La63VJJ3TzLk4Y0GkUNuhe/LU11nFvlt6ANCK5ggwl4Sw3MvHJF6l+fJ8XP7/NCRfQ2BLbn1hEVglJi7AcJCq90R2xlbIz68b1SXel7Bo9hTiDYG3ZBbAPkNee4WweZmNJa8t8dEIt+TY+rMgc7D+ppKB/QhW47RZXZTMfRlpODN48O5626ZiOgcrHfSkVxPUWi0G8uVukEG/WwFs9dpzEltxg/lsXa7Us87fsL01dgk3/f0Vx4S207O9CKS6zYDu52K9TUVECfRH0rdNujItRkybaOC3sOgAf06gZmboA9rbVf2Gi6w+AKM9k4hUqEAHSVlFlOo6+R4fmyzLlyIgCl8eS3ZgXmvEL4najW5flL4zuERNtweK0k1kluhUCgUhxh6A1IoFArFTHBoKbjKyaf2HkVpHvMeOg2P5JDYN2aU3KgLqYdwtAWU8qZDQ8GFYKvSh6RSYhRWPZLUTNCQj62DsSlVTm1JBZRAOSwtS9PW8cA88tp1+Zid5Phe5lG6BimUlnwp2YyGynblo34bnM4TV1JAhWXGNOlui30x0ISRZ6gM18ETIN+XG+16QB154DRdkPycilnm1H1Jd6UxOGmzktsm2J/YMFEh8c+Va627AZQu0CBpaSx02i1Jt2xtSefmS1uGLq7AVgVtoUI2UbuwLhuhpID29iQFxF2JUijlnV+SlCI/PT0wGHbBuiZgjvAptBJkAznGNpSZtxil5YLlUtKV6yBcNK9tzcF1NoHPTc3BemCjZIEze5pISs5lSb/1pvwuGBdyTnO2vJodSQv24dxF7LocD6XFUlCTJef9XO5PKk4pil0UfqM5hXNl18AdfmzWsQUWURMo1XfcgyndOvscK4MBHgB9AlIoFArFTKA3IIVCoVDMBHoDUigUCsVMcGg1oFpok3vejqUoDcm6tiajAhqBtPk4/cCD059PHJE2GBmkhI5SybV7zKqn3pTaQORBCTG30AH7jWIitYGiZNwt8LGYRjrJ5BhrDWapAfpECWmeg6GxwvDAcj0E3tdix76ztSn2HVmT5esVWP6zQM59lhvzbckRV6xcNBnviX1hIPWKgGk3aGlieahpwfGx0mvk1j3QnoaMl9/sbYl9DV+OP2PaQAN4dgsuHx/0Fz6vk5acp7k6lK8znTMCDW5nLMuWK5ZOiumjFpR7V1BezMvi2y25ngoofeca0N6ePB95LnURh2kmBKmmtabURVaPyutynBiNKIWIFB8ShC1m9D8ayvWUg36RsfetIFnVB9sbHwTi4bZZF52O1GYIjp2XqDtgr5Pm8txZlhl/ANZNvZE89izH4zHfg02ImimgBH04YknE8L2R9OX54TZQtVB+7yUjiIsBTWth3hxDo2m+y1xI0D0I+gSkUCgUiplAb0AKhUKhmAn0BqRQKBSKmeDQakCha5N3XgMaMw6zuyv7TvIA+GYWI9zvSS3GcySfGfiSBw5YfDRZYAVjyR4DXh9fDiXPWwf+M6obDrwowSolkBpQCPEMFbOYcRw5BpAriJiVfZbLvoZhX2oQbmzea5VZBRERWTD+JJN9HBWzxXFg/N2B/NyIvVc6ktoMRorXmP1JrQP9OdDvZUMPy3Bgji/y5Zhy4NJtxvcHYFNiwd9kEesTyqCfCC1ZSrBSmZs3fTX7+rDkJp1bN7HPS8tSc+A6IJG0nPHrwLVD/9fKnOzteeBrD05/Ho/hvMKoCtZs0gL9awi2QxNmY1UHfSJL5HkvCnm9TMZm3nzoOSsL+Tm1yLx3Vsrruyrl7xLvSStBB4E10ahLrWxpydgJOaAt5bFc4/Wa+V0P+6MIwHScFKyQhn25vlosYoRIaq8W2IrVQjn+yDLrwoH4BW9Orq8h08qGAzmGmiPX+BCiZ1ZXjJ432DVrONE4BoVCoVAcZugNSKFQKBQzgd6AFAqFQjETHFoNyHMc8s5zl23GKa9vdcXr/IbkwHnUbjaRPGQT4qLHmeRRuaU/IW8K2kxvx3jOBeBp5rWgF4N5Y1W2vOdj/0FvS+oiDvMFK+HvhaQEvpnFlRfAW4/A06nhGHZ6MJbzMEhlb0wOMclR0+gB+zyfcqnVOMxzK6yD/XwhtQGf2f970B/luPJ4lhek5f94z3DT3R3ZHzLoS83BZvHFbih7VFJYMw7rF/Hh3Hk+WOCTHGPFdCrbA8EONEXuj+aCp1xRSJ2TJ4OnmeTkHXjfDOI9bOZ/2G5I3cCCaPbBxOgBGDlig22/w7wTfYiHdmD9bJ99WGw3mmZe7YZ8re/J9ZSx/jDUrDCyI/DMGFGDc0G7TAupfRTsuoMkdpqDtReyXqXQBa0ykbpzozLHl1uozcjeMGh1Iys0x9dpSJ1tOJJrvGTfDSFoiA/ff05sZ0xcaoJHXrMlx7i4sCh/l/fr8UnGCT8A+gSkUCgUiplAb0AKhUKhmAkOLQW3u7E3TUS1fEOThC7a9MtHxLI0FIptycffyUA+pmJSoMseRQd9SQ+VgaQyOnVTytgGeg4fh22WYIm2Njk8okdA1ViMCnChxBbLvYmVXtYcSXd5lRx/PGQxDwRl4758ZB+Ukp4ImVWMXclH9nwsX7u1bqjKDKjKVl1SGf3trvmMWB7r0WOynLiEklub26EAfeoCdTZg5ceNeUllVIE8ngmzHnHgMytII8W0iVrNUL6YSFvB+ShS1mrA4juIiJoteT5SRnuEQJuNYY0PM5nkO89iIaxK0lBxAmuEzWnlge2QK8fEU1pd+LvWB0oR0zL5dUdAN9qwxm2WbFoCzWNhjAWbY1wTaF1TQYpuxm26LHk8w4kcvxf67KVAQUP5d8V+twlrLcCWBrBgCpjNkkXyffn8E8nE1HKfFCGvu4KNuQbnbm9b0vFtiOzg5fd8TZeVUnAKhUKhOMTQG5BCoVAoZgK9ASkUCoViJji0GlCRVWTZj3CKR48uTf//gYdPiddlmeQ3eRmkBdHAVEp+9swDshxxbc1Y0iy0l8S+B8/Izy0rwyF3Hfm+K6syMiJOOd+MtvCSuy2gbJbHGYRYngv88oRxxgFoYx6UqGZML5qbl8faHcmy7E4kdRJhYwIW/mEkj8djx1OlyLvLMZUsYtyCcu7AgYiCja58L/Y5Y7BGigdSz2swy59xLO1o0KLFY9pB3ZUcfQnaGM75hNnrl6A51Gqy/Ltgpi31jizHLQkspJjmEGNkMmgdAZjBuEw3sR2w+88gdoNZV3lgVVOHKPMxs6exQTNxoCy7Ca0TnZb5GipTufYiKJNPWKtEsE+vk3NRFmb8jYZ8nwzaFBxHalo2+2osLbx25NrMmYYyAguaBlhKtVhk93ZfltDDkqc0lXrYeGK2J+DD5aHuyWIhwkAee68n2xTm5sy5hYQOql0kbbrWd6WmmLE2AIetrUrLsBUKhUJxmKE3IIVCoVDMBHoDUigUCsVMcGg1ICuMyDrPqd/z5fum/5+DJf5SW/LleWU4WE/uIgv6aBbmVsW2Exi+NgLu/PgJiKlmbxXvySjdMoc+iNSQu8eOHxf7Tq+fkWMAvtxn3G5B2GMgtxusZwKd6RsNacGe5WbMG3uSE26CJcgQbEp8pkGMurJnBa2FWgvmc4cTyXmXqRxkM+D9XvLYRl2pDTgENjhMY2mB3fwaRECfXl+f/mxBrIMHkdY2+xzsZ3EK6OlKQY9kvT829H/t0y7JfG6aS+4/qEEPERuyH0iNIQVrpFoD+rSYRjEZYL+R7B3rLLI4iUDu296V56MlTpfUV1xYE3WIseCxCUFDrr0YotltZtGUQy9Moy21ypj1e3loowQ2OBRAZD2zfqpBDxT2BfG15zhyTAOIoe/3TV9N4Mp+nLKCMUEEScXmtajkGol8eey8V2yyJyNsllry+mixaPDN01Lr7iXye7ABdmY5i2rhESlVJsd3EPQJSKFQKBQzgd6AFAqFQjETHFoKbvXIUfK+7rBrm6S9CiiTfixpBLLNo58FdidRXZZajqBMe7NrHpfbpSxzjMA52LF4maacxhLKNG1Go507e1bsy+BR1QFrlYzTMRDSOr8obTH4I3oCJbU5nOqW3Zn+HMaS5sjBRiMI5FwkiaHSQrBKQYuQklmr2ODU7ILDssfsfz0oEY6AIuHJt0REA8aCWsA/7u7J0lGHuRfXgFbDc8nL4ht1KJmHkttkBNQZG2OK5epgA1Ux26gMLGU8grXI1nF3V65/D87HuA+pwOz0LALlVgBNaLEy83Qs10how3lmc+5BIm1ZQTpmCq7hLOnXgTL+GNJ4XUad2VD2HoOtVbtlrtmNs7LlYg5oWssBJ21GB0NwMhVwIXKqNQT3bh/mgv/mqCfPXQ/c+Zvz0nk6Zd99DlDUXSit9iPmru7IMQx7UjIomHVYFMjragJrvLksv3NsZlXFWU4b1vtB0CcghUKhUMwEegNSKBQKxUygNyCFQqFQzASHVgPq7u5O4xh48mQfrFMcV/L9CbMmyRPJQ84Bv+lDQqfHeNOyJ0uPCfSKLotrsArk7CVqLO1yMuiKffUaJGWCRUjG+XIo8RwC75swrt2HUtc4lyXQBSv59CF91IYoxhzs8yOWfmlBabgPliCcGK435edMBmijZH72wNa+SKWdziAdw36mH4H1yD57GvFnl1w/o6Hk4bmc18skdz7XlucuhLr/0ciMsd6Waw21P17WXwtBa4KIguHErLe5xY7Yl06kVtOGdTDcNCW5ZSZfC05D5HpmbhKw/KnVwHKJW0qBhmhBTkUCMRYWK2vOJvK82mBDlLMxo7bhwgEkTNtcPXJUfib86Z1CVEjBPmcI5erzHdlKwcvvMWpie6crtkc8CmROtgc0PblGHjy7KbZtnvAKB9CoyzJsPjcORLz0wWqr0TK/i3ExHpxLedZliTqPvwEZ80DoE5BCoVAoZgK9ASkUCoViJtAbkEKhUChmgkOrARVZMrXiiUvW2wP1+uNE8tiOY/j/k8dOiH2DsdRB+kPJa48YN1q3gRCH3gUeeRuCPlSCzcc9X/vK9OeLjsioBgf+BnBB+6g3O9OfPYgYd0P5Oa2m6XvYHHbFPi8E+xOmfWSJ1Bia0POUxXLebBYDUYMeoXgitZqKRSxAegQ1Ivm7XHsqIRbBBjuXPEW7F8NOZ2CJ43hyjmOmIzrAcTdAE+IaV3teWqeMh2DBBJ/rM60sS+R5zXKpFXSYTjLZkZpDe1Gej4qNqYKeM7RRGkOf0NJRYz816O3I93VhjTM9oN6U56qAfi+brSFov6OiBDsd6N/JeVQFXDsY7cDXkwV2TB7aWPlmTTgYCw7zTxAt7zlm23fkvnFfrvHWqpnzTktqMVvbG2K7OWd0nnEmr6ventRmqn2tNPwakPNfkJxjr2b0bgvWCCxxGjF7s2ZdnudWU/ZL4XddxfoULcuMwQL7roOgT0AKhUKhmAn0BqRQKBSKmeDQUnBZPqbq/KP6mJVmxkCZLC6tiO2YlV5unJVlymELqAxw2uXu2G4JNh99WR7Ky6MXOvJ9H7r/q2L7aZddNP25QMPbRD6qtiDpkD8v4xN5mUtKLhmb7QoSUFMoFW+wufAt+Vg92JO0TT2Q+5lzCkVAo7U7stQ9YbRUWknqokixVtMsR6RTUkgUDcCKh1ePej7Y3ABdYbF5wqTYFMpoc8YnWaWkt7AE3fXkmBzP0LglrNuwLt/LZu+F68mGdMmC0bQ2HBumXaI1T8zWjA30aUaQ0Mm5NODVXFfOsVcz8zhA+x9flsUjfVQVLEkTyqEtoF6F1RNEiLpob8QumBjSawvkt+D81FgKMFJ9USjPM6e0urtd+VqgtHoTM47AB1ocknxhiikKzXu5DpzXGMr6mTVSFEg5IVqR35mDkbneA2wLsZFKA+qbbUf8ewJ52AOgT0AKhUKhmAn0BqRQKBSKmUBvQAqFQqGYCQ6tBtTqdMg7b6FS6xi+fDiRnGSZQbku43Jt9I0oJU8KjuZkl5xbBw1iIjnNmmO4UizLXDoiLcu9muGQh7HkxxdrsmyzAm7aYiWr3MaDiCiwwWJj15QFBy3Ju3suTAaz0y9t+Zl1+N0yBpt+btUDtvwZWKfYkXltZMv3tcHYw2J6TJ7LkmbkvCspCVHKrPgtSLBMJvK8c9sSD2yHbLDMSRmXHUOKqR9Jbh3LXbl+UYGOY8HiSxiXvrUny6PbEAPhsnmzCiyblbqCB/EeI2bFz2MQiIhaYE11bsskxxapnMNaKM9lynQp1AWTGHU1mCfn4K8hF2yUeBSC50Bq7kCWxfOPiWBNE2hLKXyP8HiJCvSMHCyMMhZVEXhy/Y9h7ZXsK3cI8xLUwCoMNC6LxZn4dUiZhTTYomD2RmDDhdZUPo8+gde2mnJMNqwDboU26ZnvwTjRRFSFQqFQHGLoDUihUCgUM4HegBQKhUIxExxaDag36E/jGJrtzvT/kU+egCYUcv65lPfX3d0tsZ1CH43D+jjaDVkrPxkg98k+c15y9Oc2ZfzvZGQs8C86Jt83LUFX8KSukLJ4hmanAfvkmLzKDCrw5ZhSsIkpWRxAowbxBbactxB6GRLG73qgg8Q56FSsd6Hflz0qDuhHNouISFPJRftgpVJB7HbGeHkeGU4kbVWIiPLMfC7GRyfwvgXTbobQpzG/0BHbKfSVuSye2YbepBIsl5o1FgUyBnHSQW3JzMUE7KVq0DyS4DyyvqBA5lLQcCj7v46umAiDUV8ee4mx2mwac7iuXOhFKnPs9TE/x9CH1WHXPpG0erLA878B65RYvHpZghUS9Bu1WvJzeO+YF2E8Bmqi5gCGYM+0Bz11JbMKy1M5hh70T9VqUs+b7xhbnI0tafFjwzqOPPM5nZa0kAo8eS0df8pl058L0IBstJcC7dJnETEJ08pyuLYPgj4BKRQKhWIm0BuQQqFQKGYCvQEpFAqFYiY4tBpQo9ac9gFxq33PklxnBh5OVJpDGvWxBl++dm5O9uv4jc705zSGGGdX8rELTI/Z7UnNB33kmpHpY9o4uy32pS1ZZx+tSo2IG01VED+OPUNBYD63hR5Ue5LD5/5Qbga6B1rVRzDHjtE6CuB6LYgNz5nW5Njo1XVw9IFTyL+NajXUoaTWZLHYdget9V35u1HbzPlwIHu4+l2I5Gb9U+3moti3B7EJc8vSw61kkdajBGKQoedmb2d3+vPqXEfsI4gzyHNzrCG8Twrnchm8EnkUOMZWBJnUGPORWV9WKa+7Zkt+7s7A9C4FtYM1KyKiRl2u+TGLxwgq+dr7v/KA2L7kUuOrWBWgV8D1kKZm3mzog6sFoAuC9hQusigHX47JgQj7Pos+4fNARNRYkGsmzYwetgfR2C5EyU9At/rahonoPn5URozPQ/R6jWlCdfCCc6FfLWEatR/K6863Qe8qwMeP9Zk5THNzrH1ZEo8KfQJSKBQKxUygNyCFQqFQzASHloJLhgkV5x/dh8wu3AU6pRFI2uA0sw9ZOXJE7OvDI+/eriyZrJN5VLVt+b4ZlOd2WXllCqXhIdisZ4z9Kguwl2/Ix+MJJLxyG43hQD4OQzUleb75eyKeSIqhTCVlNUkM/VIHSgQqhMmFx/Ams6uZQExCApYmu9uGWjp5/FKx79xZSV1GrNw1hhLbPJNzXMF+ly9lSNHMIV11wErFcZ/jo2WRed80k/RDuyMpXAyerFiJvQ3RDSXMW8GOZ68raZxjq0vyteznFCItUkgU9ZuyBNcmllzakxRiBpZLOYsKsS0s6xeb1JgzVJMfYMmzfHFRyDn3WLIvuvJc+tSTckx83mCh+lDq7rBx4Hm2HTn/WNbvMdqwgijfEtI+AxYDkW7J1/qepNW+dv+p6c9LK5eIfcME6C44npxRsSurktqrAfXdZOs4gdJwBybZZUmmaU9S9TZYOWEyscMkAo+1AOTQOnAQ9AlIoVAoFDOB3oAUCoVCMRNc8A3ozJkz9KM/+qO0sLBAURTRM5/5TPrsZz873V9VFb3pTW+iI0eOUBRFdN1119F99933uA5aoVAoFN/5uCANaG9vj6699lr6vu/7PvrLv/xLWlpaovvuu4/m5oxFxK//+q/T29/+dnr3u99NJ0+epDe+8Y300pe+lO655x4KQ4ybPhhVblF1nuNdYOXSk4nkSXcgAvfY6onpzwMoM90bSR3k5NOeIrZTxvUO+5IfP3mJ1JOcoXmvbAT3cUuO0WfcaEWSP3bhbwAL7IPmmqa01wWLnDSTJcTc+mWUSC7Xc+XvuowytiHbwANuPWZlpkREo5HRdfqxnFMfbPqbNaNxdZk+R0RUg/LWydhodIO+5K3bbRlh7TsQyc2OZzwGiyJPrruYjTkDbaCC2AeXCW0BWLJYYCmTjOW55bJJCFplPpGf22nwCGg5/jHMseuZNy5tqSGWYDu0AxZSi2vmWrKh7BckICpycw5gSZMFa7HZ6Ux/Hgyh1QA0HwuiKXjagQvl0lDBTTmzgXLAFmY4huuB/XJZov2PnLewKddXVjHLIl+utUncFdujCYu4gHkq5amjS1cvNp+RyYPrQDRLacN1ycq/W4Tl0XK7YHq3DxoWgR7sMPFyoSnHgG0KFra98GgH3lZRPDYrngu6Af3ar/0aHT9+nN71rndN/+/kSSMSVlVFt912G/3yL/8yvfzlLycioj/8wz+klZUV+uAHP0ivetWr9r1nkiSUMLGzD188CoVCoXhi4oIouA996EN09dVX0w//8A/T8vIyPec5z6Hf+73fm+5/4IEHaH19na677rrp/7XbbXr+859Pd9xxx6O+56233krtdnv67/jx49/koSgUCoXiOwkXdAO6//776Z3vfCdddtll9Fd/9Vf00z/90/RzP/dz9O53v5uIiNbXH6FYVlZk9/XKysp0H+KWW26hXq83/Xfq1KlHfZ1CoVAonli4IAquLEu6+uqr6a1vfSsRET3nOc+hu+++m37nd36Hbrjhhm9qAEEQUABWEURE7XaTvPP16j1mBQ9pvtToSB47YTX6lidf3FlZFttD4E1HQ8N3tmBMG9sPi+06ozgbaIGfQx9KbHhrF+xCvIbUJ6pE7t/umwiJxRXZd1Iil85q78tc7huD1UgVMzsdsHIPfTn+Vlv2NU1YH83JE9ISZDCU9ijbWyaCYQH6ZioLrIWY9jE3Jzn5PJXHM4AYAh4/geuphL+zOJeO3LkDOmXCen+6EElQh+iMwJNaQRIz/hxiz9Gtvix4BLR8bWzJc+ezXp8mWDkFgbTIyWAdTFi0/MLKqtg3jKVm2mV0eAgxzhmspwGL2ihAUyzLg+OuiYhspjmWGI0NtlCusHuRel0Qye+CjDUr2STf1/ek1kGenLdubF4fuPJzsli+l8vG//ADXxP7Qk+u+Sgw66sJlksVRH0HHvYBmfVUg9680URqNQU7P1Eg1wikZYj5d0ie5xrYDu07d+warlifUgX2UQfhgp6Ajhw5Qk9/+tPF/11xxRX08MOPfDmvrj6yoDc2ZFbFxsbGdJ9CoVAoFEQXeAO69tpr6d577xX/95WvfIUuuugRg8CTJ0/S6uoq3X777dP9/X6fPvWpT9E111zzOAxXoVAoFE8UXBAF97rXvY5e+MIX0lvf+lb6kR/5Efr0pz9Nv/u7v0u/+7u/S0RElmXRzTffTL/6q79Kl1122bQMe21tjV7xildc0MCqKqHqfE2j7xsaYZLIstKF+Y7YjlmZYB9LEyv5+DjoS8sTiz3+N8FNurUmH6V7W+Z34xw/R2zSmFlhrC3K8W5typTWY0fnxHbIrDz6A1khaLlgnUKMKpCHSg6Ut3osOTZPwU4HkhoHQ6glZW+1Dk+7jnuwa3IPKhznFqRNDHcdzqF014/k+UBXdG6X4gHl0BtKCyZO4yZQqt/fkyXEtZahAnn5M9H+9Eu3IWkch1GkHpyQCtyCJ0NzLh2wsmkDHekzijGDNT4Z7IptiiU1U2O2N7uVPPZOU1KtXUbFZiNMqJVzMeJ0XQMSUDM5xpKQjmTJsUABWeCKbjMbmSQG5/UcU4uZnQ68T1jKNeLZcrvP1sU4kXM4D+v29P1mzSwuyuu3P5THvrxgzmU6AmrVw/J0OechkxTSkaSDfXyUYOX4DrjzR+CgnjPKNIY0YQvq79HGJ2OJuw6nKqvHRsFd0A3ouc99Ln3gAx+gW265hd785jfTyZMn6bbbbqNXv/rV09f8wi/8Ao1GI3rNa15D3W6XXvSiF9FHPvKRC+oBUigUCsUTHxdsRvpDP/RD9EM/9EMH7rcsi9785jfTm9/85m9pYAqFQqF4YkO94BQKhUIxExzaOAbHLsg5f3sMfM6Jy3vmCDjvKjRcrgVlvg6ULV90RJZlbzx0evozEoYT4MD7zL2hBlb1QOFTjVm4NBqyVPTM7hmxPRpL/nxu3vC1A9Ac2ouylJRrG2kGfvlQv25lZh6x9DIDjh7LUFMWEVHBCvKgJJ0q8zlLS2ti12Akj4e7ELlgRQ+nkpJMno+K/fJgIvd5UGbOy7+xxNyL5LEmTGNxPKmZ1DryfR2IiKjVzLne3ZNaHyaK5iz90gN9ZQzZBw4r/3agvLvdhjJs0Ijysble5hvS0t8H8XK13Zn+vFd0xb4USoZzFiWQQomwG8g5tSC3gifwWqDTEiSkVmzbc+RVWoCuxjUJF2yUKtAQ07E8t/OrRufB75GdbakTlkx/zKBs3APLomRiLL5CX44/hzJ4FyyxeKREBbEbNtgS8fSPqpJjQGsnh2nsNujKPmS+FKB3Oy4vw07Yz/D9cwD0CUihUCgUM4HegBQKhUIxE+gNSKFQKBQzwaHVgPZ6W9P4gZDZi4wmklv0wcomt8z+eChfG4D+UuWyP2R+2ew/vSG1GbRkLxkvnIP1ywrw8NXQcKMlSZ49mpc9BV3k+5m1DUZ954Xk1mMWV9xpyL6lZFv2MlDO7DfAwj+sST55a0fqF2HdfK4FFjkB9LvYjvkbZ29rU+zzoGfIY5x9CRbydiD/VoJ0bIpYtMAklz0SDtiJnGO+hCn0PfihnFOPHSv2e2WuPFdNsPjvbhmNy4V9eQ69Mkw7S+HYq7H83NNbxi+xzXQaIqIV6FUKPTlvdTbHo75cE9GydCtZWD1mxmuBLQxEpE9icy2lsIZDB6IDIJ7cYT06BUoHkG/g1owOkttSM7FhPfF4eLTwqkPPkxXIdcsjrnPoIxuPpO6csvWWW1Ibc8BOJ7WM/gKnhkro+yGIPqiYzpODLpXBBRExa6o8l5pPCD2OGesDLOB6TqGPzHXkoH2mr6ZMB8TUhoOgT0AKhUKhmAn0BqRQKBSKmeDQUnCv/P/9CEXnqa0PfODPp//faUnKCm+h48Q8lnea8pE8g8f5eDiE/eaxNmhKys0HasNjNMmoLx/R79uVZZpRaaa5BiXB3cGe2G7W5efUXTMOqOikCpyO+UP5uCsfuyOgUArmhp1A2XUFdhtIP+bMaXffXzAR0CvEy2blsbk2jInZ7+xLzQRrHgeogJiX0UKq7GAg56LV7JjPYYmzRESTVNIgDkvVRFohS+R/jKD0dJkl+SIHFMDnDlm6rSfZOYogObbDfrdWl7Qythpgiu5oYuYiaMgy/gmU7nP7I68OFHQMZf2M2rMt5F8kXVRCabhbsWsCHK7RyVykpwKHlcM65p+LDtBJBrSULz93NDDl0hZcD3ki6cdevzv9OQWJwHOASmbvVZTyfYpCbluwxh1WTm1BibYNbuUlsyWyCK21JIXoM+uqHFoJfJgXpOBS9jl8DLYDnOcB0CcghUKhUMwEegNSKBQKxUygNyCFQqFQzASHVgPKq5jy84Tvc1/wnOn/f+7z94jXbe50xbbDOG9M73McsAQBfpanao5j+bvxRPKzAXP3zsE+ZA4s2a3UcNNYyjt3TJa+epAe6TJroXQirWvyVL6XFxkO1gJ+HCQUcljpOHLatgXW9aHk/xP2uUUsx1ClkvtNWSmsB5ZFEPxJYc2UxvbGUhsrgVIu4ICY9Ee1BqSpwvlJmV0K10SIiBwXykxtM2bPlusnz9F6R5b2jpnWEUB6agxWQj6zq7FhXXpQGs71JMuG+AKYJ9cFKxV2DXBrFyKi0Vhqol7djNmvQwsAlFIHbL9VwtdKLkt5swTmrW40rTHoah6kzFpMy0xh/RelLMuOQqOPeYEsPY4isFGCKfaZdU8GcRI5xktwCyNY0yNI7uXtAgXUnLuOXKcBWDJtb5t2iGZHaoio06Zszef4nYKl4Uz788AeyILS9kkqrxcuRRXsczL47j0I+gSkUCgUiplAb0AKhUKhmAn0BqRQKBSKmeDQakABZRSc55mPLhm+c+5FzxOv+9CH/x+xzaWBRl3ypH3gY0PghS1mcY4xwhbw/zs7Ro9B95AE+PEBi89tRrJvA+04UtBj0i3DuZ7oSG0jHsv+oxY7XgvsXDDiumJNLQ5EH0Dbyb54XZf39kBfU+CCxQnjmwc9qQVgSi6Ti8gL5TxNErSql/x4o86tR+Sxoi7C+6ccG+3mwUqFnUsbzmsDoo13d2TEu8saenyIJCihVyYtzNps1mV/TgGaUMj6NtJCcu2+BWuaMNaCzQ2cZ9eTx7e9beLWbYjkQIt/Yv0tti3Hm2HMM8n36rGeNdeWx469Vvys+9hrUkBvEtNeQcqgdCLX00JH6rYlEyg3zp2TvwwRGAnrQUtBE0Vthp9KDxcmrK8YLI0Wl0x8TEnyc1LoX+NvZUN0gwuTUQi9Rl53w5GcJ4xb4b0/k8ScxzRTDUihUCgUhxh6A1IoFArFTKA3IIVCoVDMBIdWA3LKhJzzOsZkzHyyYMjf98KrxHZ3YHjTs+dkjMB3PftZYjuqSS60z3qKutuy3r0/gn4RxpcPUqkbxEPwdGJNLDlEGSegzWQQ/T0fGU58BLx7UJOa0B7j0hsQLY10ecC4WwvHkMrx59ArUzDdoRZA78tY8tZxafQvL5C6TQpNEyXrz5kU0D8RQh/KSHLMXPMqQL9D3ziL8fvQ9kMt6HepWMSFjaJJJsffgH6pgpvHgX1+Cr9rs5iODD3ZQGdLCsPLBy7EUssRUg6fU5bmeBzQBsY9qWEVrMmjAH0lS+GT2Joooe/E8yDg3pbbE+5bBh5nDvSk2Uw3gY8hD85PwjRfD85NCXpLb0/qqbZnzsd4LNf/wnxHbH/+wYenP7ue1LCWLjkitq2cn1uIiwHPOVRRuN5XQBNdAmsmYNeaBXHd+1t0mPYNnnk2aN9JIueCv7PHet1ybNw7APoEpFAoFIqZQG9ACoVCoZgJDi0Fl1c55ecpJ9c1j5tYbrjUkY+8TZZ0+PSnXyb27XW7YttxZYln57ixzz+yKBNFCyhrvutLd5v3AYohh0fc42smWTKCSAJh2U9Ef/ln/7fYXmiZ8tDdUU/sQ/7IZ5+bQVQDuNxTwvgLD6IC7AriF6CMlldEwxTus7W3bEMNVC5YgtTkXJTMZqXuSCps/cyG2F5oSmqjYhSRjVb1sF2xEtYcLPB7PTnHdWZZZLkwL5BCWcGJ55RjlshjxxgFj5XCl4WkaYtE0iu2b94LE3YtKJPNgPKN2OekI1kWX6Zy/MPYnHdu2U/0KOXdzFbGgWiAFGNDYJ4cZjXkQURHCnRwxhafD5Quvm8Q8XJ1SGmFVNAKKKOClRG3GrKdYwQxLkeW16Y/JznW/ANVzKizWgCvRW4MLHM4O1bC/KNNFGdXKyDz8hyst1hxuw12QC7Q/hFEeOQ5L9M25xnjVA6CPgEpFAqFYibQG5BCoVAoZgK9ASkUCoViJji0GpDtWmR/vXyQRSxXUFaaFTKiwK0MFzraHsE+OvC1REQV0yBqgSx9raBs84VXXT792QItBm17JiymF7lat5JljWtL0t4lGRiONYLfBRcZ8lkcQ1VK7t+uSc1hodOZ/tzbkBHiBdjpdzpSbxmNDIccQ8kzIb/sMa3Jl3O415WfG7LYZ7THX1leFNs10IiCObOdgLYx2JURxAHj1tc3pM3KHNgdcdsYH2xVCogDaDTAxmTMrUnELrJBd4sZZx7WoNweysh97t8E1kET0AlrUH6ccese4OkH2/JamjCLmcyTliwBaCg203EKsAdyPLA7Aq3DYtdWCeXqlge2Pqz8ezKQpdOBJzWhBRZZUGC9PaCCvPUxW+M5lKAPIXY7YXEM8wtS08X4cd4SkMRwjcIFnUKdeWFnB77WqeS2zT4njuU16UK5d1WxMcKx7rPtScCWi+l9vH2gUisehUKhUBxm6A1IoVAoFDOB3oAUCoVCMRMcWg2IrJLIeoQD5X0cPDabiKi3K3WETrMz/blCrw6wgUfrfR7VTIHkbvsDyY/PzRutQETyElGWyvu6y3p/XBd6FeBPgGbTg/3MMmcix2v50ANimeOtQJrZ3pER1z3G988FYIFvQy8A9HUQ69UIwM4ozaSmNdcy2sxo2JXjhd6L0DG6gkVwrMDR57HsxRjtsvMDOqEDtvcW+7vrGU99ptiXwZoYs5jq9XOyF6ndlnpRCTHu49ToJnWIbphALLXH7VKgl8SGc1mw9y0IoxqkNkYQjTAaGd3Eg9yNBDStft/MRbMtNZ8Q+o3y3MyTC/EeJcx/ABHjvKdo0pc9Zx70XnksSnt+Uc5pGkudaqffNeNtSn0u2DdGee4K1r/jhfI7JwMbKIf1I+VghhTAdw530PEh/jqFtVcDnW3MeiBL6F8rSqlLWUzftqGXZ9CV106nw/VV6MOC2HnsqePrj+vkqJkfBH0CUigUCsVMoDcghUKhUMwEh5aCc8ikUXIbHLxjOvBImExMCa4F1IXvSqoJE1H5W1XwKD3fko/wNttfQHKhB26/FXObLgpJUVWhfJxvL8rPub97evrzAFypW5GkgLiLdQKu2kUp6ZZ2w5SLlpA6mcDjvB9A2SZ77E4LtM+VnzMYGloEDHwph1LNXm5otFoEKY7yV8kDy5CQUR1DoA1KWDS8tHQEJbUVUDEloxuX1o6KfSk4EI/2OZ1bfEOOF+ybNs4ZKjmpS1rTWwOKlB2fXwdaGdJrh7BmdreNQ7wD3F6OaapNs75SoItKT45xvLdu3hdOVgpu635NWtuUuXkv34Ky8RhKuPnP4JjuR1Cuzmg3dMiJgMoHVyUaMOq17kE670SW+fvsvSxw77bAcZwn8GK5eg5WQi7MW8nshGy8mICirtfNuet15Xjn55fFdsqufyTOHFeOP4PvOt4uwaWIqlArHoVCoVAcYugNSKFQKBQzgd6AFAqFQjETHFoNqD8aU3qeU/dcQzTuS0EEbtpm5ZMh8NTIsWaZLGX0XMPlThK5D2/V3GW9APuZ0gE7/cpMcxiBpX8pS0ePXLQitrsTs398WpYBJ6BTDUZGQ6nAwr/ZmBfb46Epx12K5sS+DI69ADv3gukvOXC9OdisZMy6Q6ZBEjVZyTwRUbdrxu+WaPUil+oulJU3mub4lo6tin07E1lCv7Nj7GrcUq4R1AVzxul3B7J8NQGtCRn0MmZaGdihbEFJ+jwbP9qqbJ2Wn2sFRvsofLggXIyikPNmM40rhB6ADPTU3aGxMIJEBSrv7Yrto8umJNq1IRE1kmMoLIiXsM11V8B6iuArqlY3elhuQ8k8lObzcu9WW1rk5CAKdeF82MJiRn6OY8t5S1lERATangPng3/n1BqyZN6B0vC9nrSQ6jC7KarkekpBm+mzEnSyIYkYjoeXTDuu1BRzWOMBlN97zNaKWwVhi8VB0CcghUKhUMwEegNSKBQKxUygNyCFQqFQzASHVgOy7YDs8/EDUWR433Ep+fygLu+hNoszRoscCyxBMrAe4THWDvS+NCCKdsCsedCSJYcGJMsyOsN4LHsX0lSO0Ybo44W1JTM+sMQ5t7kltocsGmG+KW1KQugt8Vl0tuCLicgHvSUFax6PxUlPetISHy3+fRYH4GZg9Q72+SsrJvahgJgHCyxnVo9K3YpbhnT7kjsfYkQEO/YylWMaD2TfTLNh5jGDXosJrK/uUP7u4pzRddJYcvRFIH83ZXYpw6Gc78iBviDGu1elPLYql/qLDXM8SkxPSAEWLbW27M/JUvNax5Xv0+6sie3d3e7054WO1DYw1rk/kee2IPM5NsSRL0hJhUq2Lrya1CsaNXntlEwwxuj1AjLqvUj2+pRM5M0yec1mqP2xdRD6EONSgobCYuiH0E+UpvLcuaBhT0YsjhyyZTAGfWvbXAMXX3Kp2Le3J/XTWs2s8XgsdTSH4Higd4m5f1HMospjzB85APoEpFAoFIqZQG9ACoVCoZgJDi0FF3nB1C5jzMpfsSSygDJmmz0ThlDWWIFNhgt2HLwc0Qfqog9JkzmjCiZ78lEa0yJL9lpgcciHMQIzQI2WeQQ+Gkra43Nf+pL8XWbzMYIy7MFYlnAfmTOlyg6UycZQ3v3w6TNie27R0IK2B3SLIymtCTtfYS5pgjHY4PAQ2skEaDOS2NrekfvZC5CascGxO2ZpsZgsiRYik3VjkYM0Rwz0CmZAbu2ZNdNpSXqrBu7MFrPtSbpyXrIxlDUzmjNx5Twh/WhB8mrISn8LC8pxd+Qa8QNDzVjgVYNu677dmf4cg30OuquPwUYpY2vGAfuZZguteMzv1j1w1YYycu6tNYEWgAJcwmMoobfJfG4KJdtlJddTs2m+K0pYBQ5YO1VsJSM96sH4LRhTlZvvOg/KoftQRr6yauhs/O5yQ3k1JQWzL/OgbJ8A8P2VMnugqMEo0AQc9A+APgEpFAqFYibQG5BCoVAoZgK9ASkUCoViJji0GpCVl2Sd54o95u+eppLzdsEu3GIcLOcniYgKsP8P65KH51Y9MZRL2xZMFeOt6zVZ8uyCrsBd+QuIL9jelLw7lsI6rAQ9jGQp+CWXnJTvtWG4Xp6aSURkg73LiJUmV+hFX8l5ai8uiu2U/d2Sx1JHaIH+ZdmGLy8LOabxEBJpWaIoGnnkMKYIymYdZrsSQ9n1zq7URR7YNNuuL+fbS+TfZC123ufbcv4LKHkejKGEu270FugAoCSW66t0uF0TpGiSPHc5i1GoPFl63GL6HBHRsCfLarPCfI7XkMduQ9lv4PNzJ+elhMiRnb2u+UywyJlfkZ8zAp0k+QauLcNYJh4//fKnT38exPJaqoPWVzGNy/XQDkh+DupHg4FZq/FIrtt6Q17vHVZ2jtcdlmUP+qZtAefUdaCMHHS2kNmOxYmcY8+Xa7MU2p88thySVysWd4OadBjK89zvST2Jpw3z70iMNTkI+gSkUCgUiplAb0AKhUKhmAn0BqRQKBSKmeDwakDkknV+eGXFtBzoM9kX/Mp0HOy5wQjcDHjUmFnzoN6CUbslmddiBEEF23wgviff58iyjF/AXqUx0xmSUvYb1bC/JTW6wngk9QjeH0VE5LP+osqTExXAGHLIwPACo78kYB9SpjD+2HDeAcRFo3bmsCaoYV9qF/OL0k6/ADv6CbOjr9AyHizmL7/s8unPD5yS+lAJYo3HNK0UdCgPNJOFtuTLY3bubIjhsMByhpgeUEB/i+NCzLNnxpGADX8Of1J689ImKmXihx1AXAksW49pB24gx9QbSq2DaxB721K3mVuSGqID14frGY1r/qh8LfbFDdnv1kPZg1ZAX5/L+tscX05MHbSOrfV1sV2wCHWukRARlbCOLcec9wgsr8Y9eR1aLB4D9aEctNhWR14f46Gx//Ig5qEopU5Yjs33E2pNIIWT6/I1IV+LfWUBrE0+i5OB0YcmasWjUCgUisMMvQEpFAqFYibQG5BCoVAoZoJDqwFVtkfV+TiG8choHz7UpaPNPefSMX7BdeThZhCFYJVmfwkZxDbEDEheGCKrwfrdsgw/m0M/kWtDzwfwzTXH8NqBJXnepUhy4JMl4+/2pe79Yl8JfQ5xyeYJ24BA13Es9Kgy/LIFPlk767KvaW3exCYstKSOM0mkjpBxbQC45tFEvtYB3WrQN/zzaCwjOwaJ1M7KmuHlbVfy7Ft9Of5O03hqBZFce5EDPRLb0ubeZVEUvYHs+8FJL5k+2YrkeQ6gr6xk8+RDJD2Bl10Gwk7B9THQQVxfzvlgZHQ4ayznMHRlH5bfMHrGwsJlYl88kb0jO4Ou2LZZP567K88dfAylEzOPwZLseXKgXyrwzfyX4G+405VjyKBXxmHfMxHoLQ6cu4LFbGM0NvYE2sy0ELUkF85HmkHUic3fG7zWMrhmWX9kCH1LRSX1yLwyvzsGD8kQ9FML9LuEnQ/iWjHoxgdBn4AUCoVCMRPoDUihUCgUM8GhpeAmSTy1hPEDTjOAP0WJ9BBPi8REVEhPBTuOihV1l+DVAewdBcyaP4eSWhds+8UvAwWHn1PBkysvUQ0bkm5ZW5OP0p5jKLkKqLENiC8Y9c2jdi2E9wGb+BTor4JZ3QS+pAFXjhyXr2U0zmQiaSgPUmfd0NA4CVAk4CS0z9IoY2WfKwurYp/XlxTQKZbiWl+WZb9La8tiu5eZ8XsQbVBBoq4PsRY2o0GwlD2BxNcWS9wtkRMFa5ucJXQmQ1mujlQrBfJcJqwG18drB71f2G4vgLgC+JxRZmjNEo61guyGOqQLVyxtNe7LNdIGipHTVrienFAe6+bm5vTnEOjTOthw7fVluTQxStQq0fZGfo/wtYj03D56nq8hoLZ9oPoGsG59tp7SBJJWoSWAf+cg1V3BuctZwmu71RH7solcXwGcD06JzjG6PQQq7yDoE5BCoVAoZgK9ASkUCoViJrigG1BRFPTGN76RTp48SVEU0aWXXkq/8iu/QhXr9K+qit70pjfRkSNHKIoiuu666+i+++573AeuUCgUiu9sXJAG9Gu/9mv0zne+k9797nfTlVdeSZ/97Gfpx3/8x6ndbtPP/dzPERHRr//6r9Pb3/52eve7300nT56kN77xjfTSl76U7rnnnn3W3t8QDk0rDX2mg3A7c6L9JcKcuN7PxqL9uSwxLBkHjvYtFfDaMeO8CazHG3PSfp7bn6ADSwr2G54DMb2embPhAMqJgcOvs6jdECxMIihrLliZeQKl7AnM3NqC1ElipgmhtfvWrrRhCVkZaliDUvBYltwutU2ZdqMJ8w82SrtQ8txgukIKpce1QJ6PVoNZ/oC+5dYh3oNx6VjKG9Xkes6g1HrCysFd4M5rIdiwsCgH15PzFIEly9y8mac0lvv6qRxD4cnj8dgfi+NEtgs88GVZun/FUy6e/pwAp99ekSXQHotbH3elbkAVlPbC98CYxa+jlLG7Id+LR6i4LlwrYHcUsW2MHBmA5gPV60LHxe+JxYU5sW3l5r3SCZSrg6XXZGK+Y2JYezG0b2A0gohXR3sm0L9Spus4PsR55FI/arBok53NrW84Box2cJh+zLta0n0eaY+OC7oB/f3f/z29/OUvp5e97GVERHTxxRfTH/3RH9GnP/1pInrk6ee2226jX/7lX6aXv/zlRET0h3/4h7SyskIf/OAH6VWvetW+90yShBJ2IfT7/X2vUSgUCsUTDxdEwb3whS+k22+/nb7yla8QEdHnPvc5+sQnPkE/8AM/QEREDzzwAK2vr9N11103/Z12u03Pf/7z6Y477njU97z11lup3W5P/x0/fvxRX6dQKBSKJxYu6AnoDW94A/X7fbr88svJcRwqioLe8pa30Ktf/WoiIlo/7yi7siIdnldWVqb7ELfccgu9/vWvn273+329CSkUCsWTABd0A/qTP/kTes973kPvfe976corr6S77rqLbr75ZlpbW6MbbrjhmxpAEAQUgC080SMW4V/nwitmg79P8wHy1mOcd4p9QGBfAW0d5DCbkqyQfH8IPKrDoqYrkhz3CGrnedRumUp9wi7B3hxq9IeMksSYW7Qailjs8MqS7M/pDiS12al3zD6IbY6gD2hrR/YQRTVzPIOJ1OQi6AsKWB/K5kD2NTRb8rxvdhn/DJYfHvDwHohcCVsHOfRIgHRD803D4VcQ353Biz12PPsiuIcQqw029yGztsmhb8kFscNmGqMNa9qGaPndc2YeW82O2NesSR1kDD04HtM2fODp60flH3511nzVbkvd454vfkVsL8+vmdc2ZQQERpAMx6DrMH2syKU2U0EfTcnmBnWbyVieyzBg+gT0XWHERQ7RLA77HsGvnPEI1gw7Hg9iT3Kw0+ER9Q7ova6DfYvQe1WxHkf48iogKsRxmOUP9Hc5kFNTMY0ugjh4tGdKIGahZKJ2ZZv3nYDOdBAu6Ab08z//8/SGN7xhquU885nPpIceeohuvfVWuuGGG2h19ZEGwI2NDTpyxHhobWxs0LOf/ewL+SiFQqFQPMFxQRrQeDwWZnpERI7jTM0RT548Saurq3T77bdP9/f7ffrUpz5F11xzzeMwXIVCoVA8UXBBT0D/4l/8C3rLW95CJ06coCuvvJL+8R//kX7jN36DfuInfoKIHnlkvPnmm+lXf/VX6bLLLpuWYa+trdErXvGKCxuYZZF7/hF0zMoVsRway3NLRqNhgXYOFFYFNFvF7scBPHqmqSyZJMs8ejrw2E0WjpHRHrZ87MZ4QrRDqdgjblCT9iFJJsdksTE1W3L8zZb83IfXDa3Wh/LhzbHcXmrPi+3CMmNMLEg5bUhqqWAJkIUF9BCcS4dZ2zSgVHd3T1IZmIZJzCm8Bomo3S1wqXYN5YDWKVu7MgHy6DFDLTXqspw7TiRddObsWbE91+5Mf16Yk3PYA9fnZt2U63bmoZwY6CJiNA4wVGQDNbM43xHbw4kZc2BBGTnQOOMuo3/7cl2utaTdUZmbOR/uyHnJSrlOmx1Zmry3253+HIGdUWNOvrZkdjRw6BQ25fVRslc4QG9lGbhWw9/iFaNbnUDS78lEUnC1gJd7Q1m/B6XU7HrAcug0l2Ny4PuKMVzCcoyIyMZvO3YuXWiryEv5ORkrUbcgcRrd1F0o6/dr5jrklj9oPXUQLugG9Fu/9Vv0xje+kX7mZ36GNjc3aW1tjf79v//39KY3vWn6ml/4hV+g0WhEr3nNa6jb7dKLXvQi+shHPnJhPUAKhUKheMLjgm5AzWaTbrvtNrrtttsOfI1lWfTmN7+Z3vzmN3+rY1MoFArFExjqBadQKBSKmeCQxzE8wiOWzCmhjCX/2mxJLaA/Mrx1ALSfVaHNPRZUGJ0EOVUs6ebxAAnY0dTAdsV1WMzABMp8gV/OgS/3Wa2sZUltxirl9sa6sZ/PkSAHbrq9YLj1PtisLNWl9c7ulrTncCdmnhywGtkZyVJrl3HKFliyLLXk77r8fMC5KeHcxWAfVAvNe22eg54ziE1I2VyUUA693JRajcPK5D//xX8U+8JCXj7NhixV7u2Z8zMeyDEtNaWFTsLWbWNF7kMtwG8ZzWE4gJJ/qH59+ItfFttcUwlgDAQluCtNU5btkJwntJjhJem7O3INXH7JM8X2Aw89KLafcsml05+9UF4Pu0Np7RQ0zdqrQRl/ClY2HtdxbfnaOJbXzmgsr0u7MjpWFMiyctuTWlPGdOgQWhjiDHVmpulG8jvGg/dN4LuOJ75WGKWBJdvstQUkHHtgy5WziJsc2hBsW65xbDXIMvO5oWfmqSo1jkGhUCgUhxh6A1IoFArFTKA3IIVCoVDMBIdWA3L8ipzgEd6f17/7kI1dFFLbCAPDbyZgTY8SUA147Jj1vzgQqx0EUtepLNZLArXzSSyJeCs0H+xBf5HvyL8BbOhryljk7c452aMyGsl+i2Qcs31yXjxX8v1NdnyXQoz2DsR32xhlzrQyzwUbJdRuWG9DzZevTaD/yGYxBFEkXxtAbPhoAFb2GdNQFjryc2CeUtbjwWOziYgKtBBhtj5XHHuq2OUT9kTI9UQsBnocS61mEfSvkGl/4FxDDgaLVEYbcMBqJ4Q5fuZTLxfbds3sH0FvmwfaZcUiO7JcjqEEPc/xzP5LnirXU5rLz1k9LqMcMqbd5JY8nvklqavlPtNboC8uiHD85rWtzoLYt7QixziJpfZRpOb49nY3xD4b9JZ2m9nrQN6KO5LnnX9X+IE80SlmtUAkTM7OgQU9W64NfYtMP7Jh/dgQY8H7mjC6gSzQgOBLlBsTFOy7qyhQhH506BOQQqFQKGYCvQEpFAqFYiY4tBQc2ZXh3hgHV8HjZDwBR2L2swN0nRvIx9ReD9yZmbOwBbRTPMEST/OzhU+bYIcSx8wxtpTvszeQFjMlOGknjLqJIWYwBoqRO4WvQGLlw6ekHU2NlRsP+zLF0QVqaQ6seBLGVmSxpA0sKF/nbrnJGCxBbHBqZjY+Q6BWU5j/lRVpBTNi1ioTKCsP5iWNEzFqYziSc9gA122HHU8B1KoHtjEpnLuUOTs3IdXUsuWiaTVNO0GeyDXhAVXjstJYH5JVU3Ds7idyTB1GbYZQYjvudsV24BvaM4vBispFF3fzXrYjx1CvS8rHAqqPVxtjyqzlgjWMa+atDOT481Kuxbl5cw2gm3fUkJTcU+ZlhMxV3/08M/6mHNP/+tv/R2x/6n/9tRlvItcTVFoL+q63I+2YShccraFcmrtYYypAnsljt5mrvoPpzliyzfYXWGcN17ML72UzCyzuVF6iD9oB0CcghUKhUMwEegNSKBQKxUygNyCFQqFQzASHVgMqipyK4hEiMfQNhzwZghYDnD2v/nNhXwn85uISaBus5BbLDUPg2gtWWlpCNKMDpdUxt1kHfQhLqQl0hkHfcMpWJG2HxlC1abFk1jZUU1508pjYPrdpXtupS457+Yjkw7/24ENyzKnRjNB1vYRyXV4e2pqTulQSSw58wlIpfShXb7Rkye259XNie33baFyrR0/IQVlQas1KWIeQ8JiAhUiDjSMEPcIiuRZBNqSI2fystOX4uzubYnurb9bTymJH7Ou0ZQwEt9+ZQOluBdZOLpTJJ5U5dzGUCBcTsOln+qQPSbdYsj1hia/DPiToOvJ3M9CposhoTQXYS1VYFc8tsRw5BgeuUTs03xtWXY4hBX2lsbostutHzOttiOz4Zy97qdz+/hdPf77jr28X++64/a/EtsWsbtqgQyWg1zmoozDNNMc4CbAA4l9fFdjr8NgEIqmzudCWgECtKWXnnbefxHBdHQR9AlIoFArFTKA3IIVCoVDMBHoDUigUCsVMcHg1oGpC+fl46pxlH9gYdw26jmMfbOdSweEm0NvAe3/KSn5OWUJkL+M+y0xqAYOe7OMYMWGq34e+jEBasiQTSXrz1AHblVx0qyU1rHRi+P0xWNVkiXxfuzL8uQ207+kzUvMBKY18No0YC25boLsxvn8IPVu1OsQxOGaeLIihzsHaY35RHnvI4hicQPL9GzuyB6pgB9xekPpKO5BrJhsancr3wWYIBLD5OdlrcvrcmenPZ3PZawVSDdWZntHdkJZLwy253Rua9wracg6rEC5pSIDn1lUhaGMuZHjUPTOPaDeV47XDevXaTTkm25Hva2GEPbP1KXBXBdcd/5sZ9FS0ueqPzXWYurLnrzEnNbmjF58U2xkTn1wbepHgO8ezzSS/8DqpD137vd8jtu/4a9NDdOcn/qfYl6cQ7xHKk1eQmfNJJteTB/Nks3lC6yALNGreZ2lD0HkOJ8SCNWOxXsugZsZbfWMpiX2eQqFQKBQzgN6AFAqFQjET6A1IoVAoFDPBodWAoiCi6Lx3WzkyHHGeQkztvh4PU3/e68keGx65TURUFsCNMp8jjLgtod8im5j3Hg3A0wn48gnTPlwf/ask51qA113UNBpFCrrUzo7UBtqsT2gykq/FPo6cccY58Mc+9Bv5LvpBmTH689LjrCqkhtLtm3mbgJ9bRVKXavGo6bHkuNsQXzAcSZ3NYTx8DjEDPsx5a8FEjo/G8twNU7lmPGb0Zzny2Eror+iO5PlwmWaU5hBRDJZbCbO97zBPQiIiG4y19rpm3s6ekZ/J4wqIiBZWpS7F4zFSGEMdrg+H9ZV5+6In5BqxWYyCY8Gx7uuTkx8cM/80kCopakiNzvOMVgYvpQo+ZzAwuk9pyfXzXd/1QrE9gu+KNuspQi8+lLAsHjsPepEdynm69mUvn/78vO+7Tuz7yt2fF9sf+tM/Fts8dj5sgBcfPEtUrF+qgDgMG6IbeHx2BTEPvos6FGizrO+PxzzAFB0IfQJSKBQKxUygNyCFQqFQzASHloKjKiCqHnlUzFg6oR9Iyocn/xERZaxsEFNMx0NJ69Rr8r0KVoaagK36uTNnxHbILCuaYBMDzB5NEjNGF2a825UUUCuCMTFKLgSr+rAmU0J3N7enP3caskw5g7pIOzC/W5VYwglUjCOpjcA24yiBM9nbk1TGoGvORwLnanVNWvP0u6ZEvd/fFvvSVNI6nY6kVHyXUTMQW7FYlzTOuU1jgzMHFGIA5d81XnoNpeBoQ4RRFIOxmYvlBWm7QqWct7M9Uyq+Des0GYHlD6Odj52U5cOnN0+L7d0ulOuyIbpA9/IyWiKihF0PFawBD0p5RamyDa0SkFcyAXrVYzSzW5M0J9JsHr+AgDouUvlqn6X+pkNJ2R5dkmvPBtp8c8PYCc3Pd+QYQvm5CbcWgvBazy1g24zJbcm1d8U114jty69+ntj+0B+/Z/rzg/feI/ZZYB1msYHkKZSrQ1uFyyyjUijvDvZZkMnjyQpzXUb+wVTkQdAnIIVCoVDMBHoDUigUCsVMoDcghUKhUMwEh1YDmkwy+nrVscNsSrieQkRUuWDJzkoBU+DZo6YsSQVKn/pDU7aZjEAviqTmMGZlwONtWQq70ZPWL8vHTTyAVcopDyKIkwD9YsK0jwL0Fh+shmxWoppk8rUOlNGWLBrcAvucVgjR5UN5PJFn/m6BylfKMskRN1ud6c8VyTnd2ZPb3GZpaUFGQgR1OW/n9uScN2ptMz5XanIORHQvzxs9JgPO2wcrm5iVZbtQplxvtsV2DuXSUWjKXyvg6CcTWRobsriAI0dk3Hi33xXb46H53VNb6/A+ckwV1Ay3uXaWyzWC0fI+0wk9iGJuNDpiO82MllmgcgMtDXW2JoiICra/O5BWVS5cdwVrf4hhrQUNWfLM3+tZ33Wp2JdNumI7hu15FoGxsS73+TAmm5XQ18DKybbkmvFqZh6DJvjV2PJ5wAHrp3/5f/yb6c+9HamRfuRPPyi277nzH6c/r3agFD+ROm2eMr0bWg26O1ILrzfktRUxnW0yYOdG4xgUCoVCcZihNyCFQqFQzAR6A1IoFArFTHBoNaAyI/o6pc65dyj9pwLswm2PW1BAP0Ihefch6DzDEd8v+dkokNxnzuKLQ+jdOdGW8b4xE0ryXPLhjVBqWN2+1DYiZsvitORr77v/q2L7qawnZONB2bfUAFt+m9nrVKWclxgiiJuR7DficeUu8NQLR+RcxLFZYi7odTs7W2Kbn8sCxKU+aAMT0C8KZtFk+5KjL6H/qGLWSc2a1Da2NjfEdmfOHE8FHP0ItKX+UI6x0TSaBEY3nDx5ifzcdaPlbGycFftiiLyomJXK/Oqi2DeJ5Zg8T3L6HK0l+bujCUR0s4YRtPsP63JBWZmZ8/UdufZqbXnesceO981h7MbckuzhsljkQtuXYxjEUndoNoweNgC7rF5fnmffk8f3tU0TSTI/J/XIEUSqWGxdbBdyXTYhdpvbablteW6OHDsqtrE3ickt1F6RfUyves3/KbZP3Wu+G/7yTz8g9nXX5fdek8WXVIW8VsJInruywCgK9n1bmWu9BEufg6BPQAqFQqGYCfQGpFAoFIqZ4NBScHZlk33+sb9iz+joYD0aycd5yzGP4TGUDxe5fK3ry0fgIaNqqkJOTQH+FVnBHv8TSddlQ/kYXrJn5xzKo8tKjilOJVXjJOZRNgdblRUoVd48xyxmOpK6GO1J2oByM08V+AOlQF0GkNq6vWWoswmUhq+sHBPbe7tmTE4lH+cjcKl2GLXkWHJfHUpJXZJ0yw4ri4+huhUDICNWZm5BqX4LXJ7rjH60fUlFjiaS7pqk8lxevHx8+vO4LxM597pdsV1jrsl7PUnDOq48IN5e0B3INR6EcozNhqRELeaSPBxLexp0947Y70YQm/vQmfvl+Jlbeb0t115hyXOFKadJYj7XgvjdPqxbh5XJWzgvmfxu4LZd83V5Xvc2Jc2JAZ5BYK7vvS2gDHP56nrd2F4lUH08gHaBWtuMKbQ7Yt/ZhyV1efTYETlGdp2WpaSoXaAQL7ryadOf//1Tf17s+8KnPim2//JP/3T6M7aJtKElgyD9OWVUuOWa71ML/cgOgD4BKRQKhWIm0BuQQqFQKGYCvQEpFAqFYiY4tBpQEadUnOc5d7rGdiLwpY6QQ+loHBteu4jlvgr4fgtSBVc6pny635NkbuRJbn1lwdilnFuX3K1Vyt9tMOuOfiLH5LtSh2ovS85+wsqLSygjR+K6waxIalCiugAaypCVwo4yySePIVJhPJJax/ycOXZw4qEULDjazC4ohhJhtKcpCvP3kO9J3QntUOymPB+2zT4HUnPnGvK1xMrvWzVZXl9BeuSY2YscOSYjLpJYvnZ5oSO2N9dNNMI8zH+SyzHGsdH6Wg1pp5MWUoNYZHpLD0q/a4E872WOZdlmzbtQ5utBUmbGdLUcyos78/L8JExTtKBcHc/712NWpuNg+hJIoJSO5e+G7G/mCSTfup78buiwhN06jKl7DkrFwdZqzFZ2o9YR+6pSjj8fmWMvQXB06nI7YzZKe0NZGr64LC2YHoQ2kU7brL8ja7KEPq8g3ZltWr78orjyBc8X28967nOnP3/mf31C7Pv4X/6F2PZsOU8+j7zhbRTlY7u16BOQQqFQKGYCvQEpFAqFYibQG5BCoVAoZoJDqwGNdgZUnK9759HBu3vnxOsaEM1MjKuuB5L7dx15uCUc/nDb9GpUGfQqQNzvuV7X7APNp1YHzYEMv7+8ILWAnU2wo4FekoBZ9ZSW5HL9SPL9/YEZ0yCTVhhf/tJ9YntxzfDNBfR4hBBl7tqwzWJ6v3yfjAa+8mmXyzGyY+9Vcp72gMOfXzR9D7Yrjy0s5Rhq81KPmaRGC0lhDlP4HGK64aSQY8K+oILFGWxtyLhrD2yUKuijKVhccQF9Sw7EPkTM8iQeS13nOMQzZKyXrAZrHE3wUaPjn+OAXoHHzrVMD2IGdrbluiXWkxM1Zc+NC+urN5DaRsg00gi0GDeUGh0xrcMBu6Ya9EDZTP/qb8rYig5EsY/25PFYZHSevC/nxXPk51ieOT67Js/reALriemcjXoHxitfW1mgHy2bdTwE/WhlTfYE1lvmHDgQh0EQO08sPvuq7/3nYtfVL/oesf3Z//lxsf2Jv/nr6c+1yLyvbasGpFAoFIpDDL0BKRQKhWImsCqshZ0x+v0+tdtt+qmnX0nBeQsb2zOPiEFLPqJXvny8HA4MjdaAUuQSaKntHWljMtc2zrVhIGmEMSRYenVDZfShFLwJViQWow2G4Opc5XL6HXjszllJsQOUD1qRDJnjrweP3e2GHNOYve84w/JooBHGB1sLWZ4cf57IeVpoGapsNJYEUVCXdGSPldxu7koLEw9ozTFYggiuqZJUWDKRdMU8WxdNcPfNJ5K+6yyYkmivAZZLUJrsg3NwxRzHN2CthQ1JHTcZleaCc3Z/W6Zfri4ZuqU9J53XLShFLqHU+v4HjYXOSeaeTkQUQ1KmY5u5ccF1vlWX1NgWt5yBzwwieS01IUl2a2dn+nMGsax7fTmmpUVj9ZQlcu0tL0rn6YVFM8ZaTX4XpFCqX9ny+HxWzu7A3+lISXvs+AqwlyrheiZGwQW+nMMAKPWoJWnCitkUBS1JQQdgNXTsIjNPzRbMP6Sautz2Cm4HJZSv23i7YOX3/zez9BmPx/QjP/l/Uq/Xo1ZLfveI9ztwj0KhUCgU30boDUihUCgUM4HegBQKhUIxExxaDegN33sNBV8vw2apezt7m+L1k1RyxLz8LwT7fB9sfAoowc0ys72wLBMHsXRUJptCWiRoAV+690vTn4+dkKmHEZSOJmDxT0yjSOBUuVDqyCh7yjMoLwbbnpxx0xhemMQ4LwenG7pg8zHuSY1rhWkU3SHoNmAbs8PmuDkn5z+FQZ7blOX43aHR/mxPzsvyvNQGQsZbz0EUhQtRFDYrVS58eW4GYKt09Nia2A6Y1jQYy2MvXGnnUjJ9rCrk56CtksdOJkYb2FBCH4NOFbL38mDdetCmYPFy3Uq+D8H6mrCoExdKqUtbakIh6EcZ0+wq0DWrQs6THRrtDO1nTizJNVNnNjijkdQBQyjzH026YnuhzVKYwQqpguuucI1W49akvmVboAuW5vgaNamN4LVEsJ1XZnv12EViX0pyLmxmq3TsqIx1qGM8Mks1Xjt2sdwHGpYDMRA8PJmnoPb7fZpbmFcNSKFQKBSHE3oDUigUCsVMoDcghUKhUMwEh1YD+slnP4388/0mUWA4V4yerRw5fL7lQdRBMpZcbgnRAXVWSw/0MvXHsr8lYrX/MdptkNQrHGbVsS/7FzjWfWeDaUAY3VCBNsO3sacjg96YgEU3jCZSnyhz+b4h9CuMJua9sxTs/sHSv5gwOxe0SgG9osvGMcI5BRFrkkFMMlsXC8vSqt6CnqjxlumrySAyeQX6K0LWe8VteYiIMrAWiqHfyGGvt8GOxqvLPiDHYZ8Dus0RsFkpmf5SYh8T2AHlELWxMmf0sLgvdU38KijZ1VRBHxAENVNqmTF70M9SgOXPBCx/5lbN+dqC6PIil787js3vnrxE9jHNR3JUNb7cbOxfkZuBK+exyMy5dCy5xi38XqnYtid7biwHbInYzFWFPLYO9P34Nejp4pH1oG+H7Y7YDiIW/Q3v4wZgYdSssX3yfaNAjmm+JT+n1WGaFzvP/X6f5uc7qgEpFAqF4nBCb0AKhUKhmAn0BqRQKBSKmeDQxjHUWo1pH1DJbOF3B5IjdgJ5CD7TWwrQcSyMIIY+lILVsY/A+y0Drr3D7OktEHYqsDuvMW53byB1AowVRoQsMjoeydeOepLDbzFdan1L9ktxTzMiorBuxr8FOgj2BVmgX1SMT2/Py/ft9+XxtRYM/+vA+wzAN67TNvwztL7Qw6dk3w96X+Ws1yROpDaztbsjtrOBiZq+ZFnqKw70h9z7tQemP2NUdmcF5rQFa5Ed7uY5iBFx5OfUOmaNcH2OiGgT1kzEoxFAL7Jc0LsgYvzM5sb057kWRH+DnsfjAGxLHtuoL3ugbLYm3FCOH/vKRuA92H3YzE0wJzWHFLTLlEUsBCDkhKAH57G5PjLoY3I8uPZjiELIzPH5cD0XEJtQsmNP4X0aDalLMRtFajY7Yt8IvttcjPdmb+VAj9AklmvE5usA9Gvqye3eGaO9Li7C9dCRXoN3s+uBiMhhum5n0Wh5Q4iKPwj6BKRQKBSKmUBvQAqFQqGYCQ4tBZenCTnnyzVjRtVYjnz0tBwo+2U/94B6aYMtPCQhUGjxtEj5OZi2KEtWgSYAm5KdM93pz34NEh7BNgaYPkFBuDU5hnmw9C9ZyfPTni6TSRMoW05iU0rdAdrDh7Lr7u5AbPOyZp4GSUQiXZFIUqYh0ENhU45/j5XgBr4s3VxekOXRBfztlDhm4h586GH5WijDXp5nZdpwnnd7ksrosOTVCOb/1Iak1RZWZfn33tDMmwPH2liSr42ZPZAHVKUFdBEnsDKwDoqHsvzeAjv9ilE3mz0ZEVFCGbbHyuQbkEyawvtaLEJisgs0uS/Llgmu2cnIjLmXyTEtQMTC8pwZUwixFV4G1Fhl1jy4DFGaoeUSvICtizrQvec25PdK1DDrwsE2CogNKVmKazeRKawO0LIJUJUFT6h15XdMWUg6PmX0YwDWQU2QLQJ2fcTnZHJsH85lBOXeg66Z4wmTLUYjOZ6DoE9ACoVCoZgJ9AakUCgUipng0FFwX6e2UkYtpKwLu7DkM66D7sXslpoCn5XCawkely3r0T8T9xEROY553EcaJKm+wefCawuwXEAKjlN96EldQXd3yapzLNiXwOfmJT9WeGdbUhkZzAUfU5p/49fmpXlvB6bfhtfy3923D+iWAk5ezpI0S5j/knCOzXvl+8YLg2RvtW8M8Dn738vsr2AMOE9iG86VBdSYzd7rG77Po/yuI+YY1g/acLAKu3/qPHMKrgTKs4RKPTAhEO+Vw5gy+NyUXf8xOJnEDh6P+d1yH7UNY0TujLk1eFBVGadQecjGUez7TpE0Gi9sQydzB8cQg4M6Y2ZzB9Y4UKL8cwqgdO1Sfu1n7Hx5cHZSXCOepE9HKZMImKYxHj9Cq/5TRjuHzorn9OnTdPz48VkPQ6FQKBTfIk6dOkXHjh07cP+huwGVZUlnz56lqqroxIkTdOrUqW/oJfRkR7/fp+PHj+s8/RPQeXps0Hl6bNB5+saoqooGgwGtra2RbR+s9Bw6Cs62bTp27Ni0obHVaukJfgzQeXps0Hl6bNB5emzQeToY7Xb7n3yNFiEoFAqFYibQG5BCoVAoZoJDewMKgoD+43/8jxQEwT/94icxdJ4eG3SeHht0nh4bdJ4eHxy6IgSFQqFQPDlwaJ+AFAqFQvHEht6AFAqFQjET6A1IoVAoFDOB3oAUCoVCMRPoDUihUCgUM8GhvQG94x3voIsvvpjCMKTnP//59OlPf3rWQ5oZbr31Vnruc59LzWaTlpeX6RWveAXde++94jVxHNONN95ICwsL1Gg06Prrr6eNjY0D3vHJgbe97W1kWRbdfPPN0//TeXoEZ86coR/90R+lhYUFiqKInvnMZ9JnP/vZ6f6qquhNb3oTHTlyhKIoouuuu47uu+++GY74fz+KoqA3vvGNdPLkSYqiiC699FL6lV/5FWGwqfP0LaI6hHjf+95X+b5f/bf/9t+qL37xi9W/+3f/rup0OtXGxsashzYTvPSlL63e9a53VXfffXd11113VT/4gz9YnThxohoOh9PX/NRP/VR1/Pjx6vbbb68++9nPVi94wQuqF77whTMc9Wzx6U9/urr44ourZz3rWdVrX/va6f/rPFXV7u5uddFFF1U/9mM/Vn3qU5+q7r///uqv/uqvqq9+9avT17ztbW+r2u129cEPfrD63Oc+V/3Lf/kvq5MnT1aTyWSGI//fi7e85S3VwsJC9eEPf7h64IEHqve///1Vo9Go/st/+S/T1+g8fWs4lDeg5z3vedWNN9443S6KolpbW6tuvfXWGY7q8GBzc7MiourjH/94VVVV1e12K8/zqve///3T13zpS1+qiKi64447ZjXMmWEwGFSXXXZZ9dGPfrT6nu/5nukNSOfpEfziL/5i9aIXvejA/WVZVqurq9V//s//efp/3W63CoKg+qM/+qP/HUM8FHjZy15W/cRP/IT4v1e+8pXVq1/96qqqdJ4eDxw6Ci5NU7rzzjvpuuuum/6fbdt03XXX0R133DHDkR0e9HqPxOTOn4+LvvPOOynLMjFnl19+OZ04ceJJOWc33ngjvexlLxPzQaTz9HV86EMfoquvvpp++Id/mJaXl+k5z3kO/d7v/d50/wMPPEDr6+tintrtNj3/+c9/Us3TC1/4Qrr99tvpK1/5ChERfe5zn6NPfOIT9AM/8ANEpPP0eODQuWFvb29TURS0srIi/n9lZYW+/OUvz2hUhwdlWdLNN99M1157LT3jGc8gIqL19XXyfZ86nY547crKCq2vrz/Kuzxx8b73vY/+4R/+gT7zmc/s26fz9Ajuv/9+euc730mvf/3r6Zd+6ZfoM5/5DP3cz/0c+b5PN9xww3QuHu0afDLN0xve8Abq9/t0+eWXk+M4VBQFveUtb6FXv/rVREQ6T48DDt0NSPGNceONN9Ldd99Nn/jEJ2Y9lEOHU6dO0Wtf+1r66Ec/SmEYzno4hxZlWdLVV19Nb33rW4mI6DnPeQ7dfffd9Du/8zt0ww03zHh0hwd/8id/Qu95z3vove99L1155ZV011130c0330xra2s6T48TDh0Ft7i4SI7j7KtM2tjYoNXV1RmN6nDgpptuog9/+MP0N3/zNyJlcHV1ldI0pW63K17/ZJuzO++8kzY3N+m7v/u7yXVdcl2XPv7xj9Pb3/52cl2XVlZWdJ6I6MiRI/T0pz9d/N8VV1xBDz/8MBHRdC6e7Nfgz//8z9Mb3vAGetWrXkXPfOYz6d/8m39Dr3vd6+jWW28lIp2nxwOH7gbk+z5dddVVdPvtt0//ryxLuv322+maa66Z4chmh6qq6KabbqIPfOAD9LGPfYxOnjwp9l911VXkeZ6Ys3vvvZcefvjhJ9WcveQlL6EvfOELdNddd03/XX311fTqV796+rPOE9G11167r4z/K1/5Cl100UVERHTy5ElaXV0V89Tv9+lTn/rUk2qexuPxvjRPx3GoLEsi0nl6XDDrKohHw/ve974qCILqD/7gD6p77rmnes1rXlN1Op1qfX191kObCX76p3+6arfb1d/+7d9W586dm/4bj8fT1/zUT/1UdeLEiepjH/tY9dnPfra65pprqmuuuWaGoz4c4FVwVaXzVFWPlKi7rlu95S1vqe67777qPe95T1Wr1ar//t//+/Q1b3vb26pOp1P92Z/9WfX5z3++evnLX/6kKy++4YYbqqNHj07LsP/H//gf1eLiYvULv/AL09foPH1rOJQ3oKqqqt/6rd+qTpw4Ufm+Xz3vec+rPvnJT856SDMDET3qv3e9613T10wmk+pnfuZnqrm5uapWq1X/6l/9q+rcuXOzG/QhAd6AdJ4ewZ//+Z9Xz3jGM6ogCKrLL7+8+t3f/V2xvyzL6o1vfGO1srJSBUFQveQlL6nuvffeGY12Nuj3+9VrX/va6sSJE1UYhtUll1xS/Yf/8B+qJEmmr9F5+tageUAKhUKhmAkOnQakUCgUiicH9AakUCgUiplAb0AKhUKhmAn0BqRQKBSKmUBvQAqFQqGYCfQGpFAoFIqZQG9ACoVCoZgJ9AakUCgUiplAb0AKhUKhmAn0BqRQKBSKmUBvQAqFQqGYCf5fu9Uh3TKly5EAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_scaled_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are these two the same image (or) Comparing Images\n",
    "\n",
    "Finding if two images are equal with `Opencv`, is a quite simple operation.\n",
    "\n",
    "There are 2 fundamental elements to consider:\n",
    "\n",
    "1. The images have both the `same size` and `channels`\n",
    "2. Each `pixel` has the `same value`\n",
    "\n",
    "- If they have the `same sizes and channels`, then `SUBTRACT`. \n",
    "- The operation cv2.subtract(image1, image2) simply subtract from each pixel of the first image, the value of the corresponding pixel in the second image.\n",
    "- A `colored image` has `3 channels (blue, green, and red)`\n",
    "- The `cv2.subtract()` operation makes the subtraction `for every single channel` and we need to check if all the three channels are black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same size and channels = True ; scaled_down = (100, 100, 3) ; resize = (100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "im_resz_shape = img_resized.numpy().shape\n",
    "im_scale_shape = img_scaled_down.numpy().shape\n",
    "print(f'Same size and channels = { im_scale_shape == im_resz_shape} ; scaled_down = { im_scale_shape } ; resize = {im_resz_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images are different\n"
     ]
    }
   ],
   "source": [
    "difference = cv2.subtract(img_resized.numpy() , img_scaled_down.numpy())\n",
    "b, g, r = cv2.split(difference)\n",
    "if cv2.countNonZero(b) == 0 and cv2.countNonZero(g) == 0 and cv2.countNonZero(r) == 0:\n",
    "    print(\"The images are completely Equal\")\n",
    "else:\n",
    "    print(\"The images are different\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"> => So they are indeed Different ; Learnt something about processing images in cv2 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What exactly is the difference ?\n",
    "\n",
    "# print(b, g, r)\n",
    "# plt.imshow(difference.astype(np.uint8)  ) # <----- this is insane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Create Labelled Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Build Train and Test Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Build Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Build Distance Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Make Siamese Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Setup Loss and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Establish Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Build Train Step Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Build Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Real Time Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
